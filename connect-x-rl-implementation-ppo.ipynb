{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":253623,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":216886,"modelId":238602},{"sourceId":254134,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":217285,"modelId":239002},{"sourceId":254194,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":217336,"modelId":239052},{"sourceId":254214,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":217353,"modelId":239069}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport gym\nfrom kaggle_environments import make, evaluate\nfrom gym import spaces\n\nimport torch\nimport torch as th\nimport torch.nn as nn\n\n#!pip install \"stable-baselines3\"\nfrom stable_baselines3 import PPO \nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T04:50:37.253291Z","iopub.execute_input":"2025-02-10T04:50:37.253477Z","iopub.status.idle":"2025-02-10T04:50:37.259215Z","shell.execute_reply.started":"2025-02-10T04:50:37.253459Z","shell.execute_reply":"2025-02-10T04:50:37.258377Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#https://medium.com/@chen-yu/building-a-customized-residual-cnn-with-pytorch-471810e894ed\n\n# âœ… Define Residual Block\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int) -> None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        # ğŸ”¥ Downsample if input channels â‰  output channels\n        self.downsample = None\n        if in_channels != out_channels:\n            self.downsample = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        identity = self.downsample(x) if self.downsample else x\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x += identity  # ğŸ”¥ Residual Connection\n        return self.relu(x)\n\n\n# âœ… Define Custom Feature Extractor with Residual Blocks\nclass CustomResNetCNN(BaseFeaturesExtractor):\n    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n        super().__init__(observation_space, features_dim)\n        \n        n_input_channels = observation_space.shape[0]  # 1 channel for Connect 4\n\n        self.cnn = nn.Sequential(\n            ResidualBlock(n_input_channels, 64),  # Expand to 64\n            ResidualBlock(64, 64),  # Keep 64\n            ResidualBlock(64, 128),  # Expand to 128\n            ResidualBlock(128, 128),  # Keep 128\n            ResidualBlock(128, 256),  # Expand to 256\n            ResidualBlock(256, 256),  # Keep 256\n            nn.Flatten(),\n        )\n\n        # Compute output shape dynamically\n        with th.no_grad():\n            n_flatten = self.cnn(th.as_tensor(observation_space.sample()[None]).float()).shape[1]\n\n        self.linear = nn.Sequential(\n            nn.Linear(n_flatten, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, features_dim),\n            nn.ReLU()\n        )\n\n    def forward(self, observations: th.Tensor) -> th.Tensor:\n        return self.linear(self.cnn(observations))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T04:50:41.435986Z","iopub.execute_input":"2025-02-10T04:50:41.436288Z","iopub.status.idle":"2025-02-10T04:50:41.446971Z","shell.execute_reply.started":"2025-02-10T04:50:41.436265Z","shell.execute_reply":"2025-02-10T04:50:41.446117Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"The tip to train RL is to increase the difficulty of opponents gradually. The RL just is like a boy. He needs to learn for sometimes wins or sometimes loses. If he loses every time with a strong opponent like Minmax, he cannot learn anything since he lost in his first step.","metadata":{}},{"cell_type":"code","source":"class CustomCNN(BaseFeaturesExtractor):\n    \n    def __init__(self, observation_space: gym.spaces.Box, features_dim: int=256):\n        super(CustomCNN, self).__init__(observation_space, features_dim)\n        n_input_channels = observation_space.shape[0]\n        \n        self.cnn = nn.Sequential(\n            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # âœ… Deeper network\n            nn.ReLU(),\n            nn.Flatten(),\n        )\n\n        with th.no_grad():\n            n_flatten = self.cnn(\n                th.as_tensor(observation_space.sample()[None]).float()\n            ).shape[1]\n\n        self.linear = nn.Sequential(\n            nn.Linear(n_flatten, features_dim),\n            nn.ReLU(),\n        )\n\n    def forward(self, observations: th.Tensor) -> th.Tensor:\n        return self.linear(self.cnn(observations))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T04:50:45.134162Z","iopub.execute_input":"2025-02-10T04:50:45.134465Z","iopub.status.idle":"2025-02-10T04:50:45.140951Z","shell.execute_reply.started":"2025-02-10T04:50:45.134441Z","shell.execute_reply":"2025-02-10T04:50:45.140212Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Neural network for predicting action values\nclass CustomCNN1(BaseFeaturesExtractor):\n    \n    def __init__(self, observation_space: gym.spaces.Box, features_dim: int=128):\n        super(CustomCNN1, self).__init__(observation_space, features_dim)\n        # CxHxW images (channels first)\n        n_input_channels = observation_space.shape[0]\n        self.cnn = nn.Sequential(\n            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=0),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),\n            nn.ReLU(),\n            nn.Flatten(),\n        )\n\n        # Compute shape by doing one forward pass\n        with th.no_grad():\n            n_flatten = self.cnn(\n                th.as_tensor(observation_space.sample()[None]).float()\n            ).shape[1]\n\n        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n\n    def forward(self, observations: th.Tensor) -> th.Tensor:\n        return self.linear(self.cnn(observations))\n   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T04:50:46.844674Z","iopub.execute_input":"2025-02-10T04:50:46.844947Z","iopub.status.idle":"2025-02-10T04:50:46.850874Z","shell.execute_reply.started":"2025-02-10T04:50:46.844926Z","shell.execute_reply":"2025-02-10T04:50:46.849855Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"## New Structure\n\nimport torch as th\nimport torch.nn as nn\nimport gym\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n\nclass CustomCNN2(BaseFeaturesExtractor):\n    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 128):\n        super(CustomCNN2, self).__init__(observation_space, features_dim)\n        \n        n_input_channels = observation_space.shape[0]\n\n        self.cnn = nn.Sequential(\n            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Flatten(),\n        )\n        \n        \"\"\"        \n        self.cnn = nn.Sequential(\n            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Flatten(),\n        )\n        \"\"\"\n\n        # Compute shape by doing a forward pass with a dummy tensor\n        with th.no_grad():\n            n_flatten = self.cnn(th.as_tensor(observation_space.sample()[None]).float()).shape[1]\n\n        # Multi-layer MLP head for better feature extraction\n        self.linear = nn.Sequential(\n            nn.Linear(n_flatten, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),  # Dropout for regularization\n            nn.Linear(256, features_dim),\n            nn.ReLU()\n        )\n\n    def forward(self, observations: th.Tensor) -> th.Tensor:\n        return self.linear(self.cnn(observations))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T04:50:48.790759Z","iopub.execute_input":"2025-02-10T04:50:48.791060Z","iopub.status.idle":"2025-02-10T04:50:48.797851Z","shell.execute_reply.started":"2025-02-10T04:50:48.791013Z","shell.execute_reply":"2025-02-10T04:50:48.796858Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def MinMaxAB_agent(obs, config): #MinMax with alpha-beta pruning #Very strong opponent.\n    ################################\n    # Imports and helper functions #\n    ################################\n    \n    import numpy as np\n    import random\n    # Helper function for score_move: gets board at next step if agent drops piece in selected column\n    def drop_piece(grid, col, mark, config):\n        next_grid = grid.copy()\n        for row in range(config.rows-1, -1, -1):\n            if next_grid[row][col] == 0:\n                break\n        next_grid[row][col] = mark\n        return next_grid\n            \n    # Helper function for get_heuristic: checks if window satisfies heuristic conditions\n    def check_window(window, num_discs, piece, config):\n        return (window.count(piece) == num_discs and window.count(0) == config.inarow-num_discs)\n        \n    # Helper function for get_heuristic: counts number of windows satisfying specified heuristic conditions\n    def count_windows(grid, num_discs, piece, config):\n        num_windows = 0\n        # horizontal\n        for row in range(config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[row, col:col+config.inarow])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # vertical\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns):\n                window = list(grid[row:row+config.inarow, col])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # positive diagonal\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # negative diagonal\n        for row in range(config.inarow-1, config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        return num_windows\n    \n    # Helper function for minimax: calculates value of heuristic for grid\n    def get_heuristic(grid, mark, config):\n        num_threes = count_windows(grid, 3, mark, config)\n        num_fours = count_windows(grid, 4, mark, config)\n        num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n        num_fours_opp = count_windows(grid, 4, mark%2+1, config)\n        score = num_threes - 1e2*num_threes_opp - 1e4*num_fours_opp + 1e6*num_fours\n        return score\n\n    # Uses minimax to calculate value of dropping piece in selected column\n    def score_move(grid, col, mark, config, nsteps):\n        next_grid = drop_piece(grid, col, mark, config)\n        score = minimax(next_grid, nsteps-1, False, mark, config)\n        return score\n    \n    # Helper function for minimax: checks if agent or opponent has four in a row in the window\n    def is_terminal_window(window, config):\n        return window.count(1) == config.inarow or window.count(2) == config.inarow\n    \n    # Helper function for minimax: checks if game has ended\n    def is_terminal_node(grid, config):\n        # Check for draw \n        if list(grid[0, :]).count(0) == 0:\n            return True\n        # Check for win: horizontal, vertical, or diagonal\n        # horizontal \n        for row in range(config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[row, col:col+config.inarow])\n                if is_terminal_window(window, config):\n                    return True\n        # vertical\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns):\n                window = list(grid[row:row+config.inarow, col])\n                if is_terminal_window(window, config):\n                    return True\n        # positive diagonal\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n                if is_terminal_window(window, config):\n                    return True\n        # negative diagonal\n        for row in range(config.inarow-1, config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n                if is_terminal_window(window, config):\n                    return True\n        return False\n    \n    # Minimax implementation\n    def minimax(node, depth, maximizingPlayer, mark, config, a=-np.Inf, b=np.Inf):\n        is_terminal = is_terminal_node(node, config)\n        valid_moves = [c for c in range(config.columns) if node[0][c] == 0]\n        if depth == 0 or is_terminal:\n            return get_heuristic(node, mark, config)\n        if maximizingPlayer:\n            value = -np.Inf\n            for col in valid_moves:\n                child = drop_piece(node, col, mark, config)\n                value = max(value, minimax(child, depth-1, False, mark, config, a, b))\n                if value > b:\n                    break\n            a = max(a, value)\n            return value\n        else:\n            value = np.Inf\n            for col in valid_moves:\n                child = drop_piece(node, col, mark%2+1, config)\n                value = min(value, minimax(child, depth-1, True, mark, config, a, b))\n                if value < a:\n                    break\n            b = min(b, value)\n            return value\n\n    \n    #########################\n    # Agent makes selection #\n    #########################\n\n\n    # How deep to make the game tree: higher values take longer to run!\n    N_STEPS = 1\n    # Get list of valid moves\n    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n    # Convert the board to a 2D grid\n    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n    # Use the heuristic to assign a score to each possible board in the next step\n    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config, N_STEPS) for col in valid_moves]))\n    # Get a list of columns (moves) that maximize the heuristic\n    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n    # Select at random from the maximizing columns\n    return random.choice(max_cols)\n\n    #valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n    #for col in valid_moves:\n        #if check_winning_move(obs, config, col, obs.mark):\n            #return col\n            \n    #return random.choice(valid_moves)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T05:43:08.410325Z","iopub.execute_input":"2025-02-10T05:43:08.410628Z","iopub.status.idle":"2025-02-10T05:43:08.428179Z","shell.execute_reply.started":"2025-02-10T05:43:08.410602Z","shell.execute_reply":"2025-02-10T05:43:08.427283Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"def OneStep_agent(obs, config): #One-Step Look #Medium hard opponent. #Change score of heuristic to get strong or get weak.\n    ################################\n    # Imports and helper functions #\n    ################################\n    \n    import numpy as np\n    import random\n    \n    # Calculates score if agent drops piece in selected column\n    def score_move(grid, col, mark, config):\n        next_grid = drop_piece(grid, col, mark, config)\n        score = get_heuristic(next_grid, mark, config)\n        return score\n    \n    # Helper function for score_move: gets board at next step if agent drops piece in selected column\n    def drop_piece(grid, col, mark, config):\n        next_grid = grid.copy()\n        for row in range(config.rows-1, -1, -1):\n            if next_grid[row][col] == 0:\n                break\n        next_grid[row][col] = mark\n        return next_grid\n    \n    # Helper function for score_move: calculates value of heuristic for grid\n    A = 1*10**9\n    B = 1*10**6\n    C = 1*10**2\n    D = -1*10**6\n    E = -1*10**9\n    def get_heuristic(grid, mark, config):\n        num_fours = count_windows(grid, 4, mark, config)\n        num_threes = count_windows(grid, 3, mark, config)\n        num_two = count_windows(grid, 2, mark, config)\n        num_two_opp = count_windows(grid, 2, mark%2+1, config)\n        num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n        score = A*num_fours+B*num_threes+C*num_two+D*num_two_opp+E*num_threes_opp\n        return score\n    \n    # Helper function for get_heuristic: checks if window satisfies heuristic conditions\n    def check_window(window, num_discs, piece, config):\n        return (window.count(piece) == num_discs and window.count(0) == config.inarow-num_discs)\n        \n    # Helper function for get_heuristic: counts number of windows satisfying specified heuristic conditions\n    def count_windows(grid, num_discs, piece, config):\n        num_windows = 0\n        # horizontal\n        for row in range(config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[row, col:col+config.inarow])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # vertical\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns):\n                window = list(grid[row:row+config.inarow, col])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # positive diagonal\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # negative diagonal\n        for row in range(config.inarow-1, config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        return num_windows\n        \n    #########################\n    # Agent makes selection #\n    #########################\n\n    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config) for col in valid_moves]))\n    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n    \n    return random.choice(max_cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T04:51:00.078308Z","iopub.execute_input":"2025-02-10T04:51:00.078617Z","iopub.status.idle":"2025-02-10T04:51:00.089994Z","shell.execute_reply.started":"2025-02-10T04:51:00.078592Z","shell.execute_reply":"2025-02-10T04:51:00.089198Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def Simple_agent(obs, config): #Simple agent with basic rulers block and winning move! #Novie opponent.\n    # Your code here: Amend the agent!\n    import numpy as np\n    import random\n\n    \"\"\"\n    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n    return random.choice(valid_moves)\n    \"\"\"\n    # Gets board at next step if agent drops piece in selected column\n    def drop_piece(grid, col, piece, config):\n        next_grid = grid.copy()\n        for row in range(config.rows-1, -1, -1):\n            if next_grid[row][col] == 0:\n                break\n        next_grid[row][col] = piece\n        return next_grid\n    \n    # Returns True if dropping piece in column results in game win\n    def check_winning_move(obs, config, col, piece):\n        # Convert the board to a 2D grid\n        grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n        next_grid = drop_piece(grid, col, piece, config)\n        # horizontal\n        for row in range(config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(next_grid[row,col:col+config.inarow])\n                if window.count(piece) == config.inarow:\n                    return True\n        # vertical\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns):\n                window = list(next_grid[row:row+config.inarow,col])\n                if window.count(piece) == config.inarow:\n                    return True\n        # positive diagonal\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(next_grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n                if window.count(piece) == config.inarow:\n                    return True\n        # negative diagonal\n        for row in range(config.inarow-1, config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(next_grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n                if window.count(piece) == config.inarow:\n                    return True\n        return False\n    \n    OpnPlayer = 1\n    if obs.mark == 1:\n        OpnPlayer = 2\n    \n    Blocking = []\n    empty = []\n\n    for col in range(config.columns): #Winning\n        if check_winning_move(obs, config, col, obs.mark):\n            return col\n        if check_winning_move(obs, config, col, OpnPlayer):\n            Blocking.append(col)\n        if obs.board[col] == 0:\n            empty.append(col)\n\n    return Blocking[0] if Blocking else random.choice(empty)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T04:51:04.695998Z","iopub.execute_input":"2025-02-10T04:51:04.696316Z","iopub.status.idle":"2025-02-10T04:51:04.704993Z","shell.execute_reply.started":"2025-02-10T04:51:04.696294Z","shell.execute_reply":"2025-02-10T04:51:04.704294Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import numpy as np\nimport random\n\ndef one_step_lookahead(obs, config): #Medium level opponent\n    \"\"\" \n    Agent chooses the move with the highest immediate heuristic score.\n    \"\"\"\n    def score_move(grid, col, mark, config):\n        \"\"\"Simulate dropping a piece and evaluate score.\"\"\"\n        next_grid = drop_piece(grid, col, mark, config)\n        return get_heuristic(next_grid, mark, config)\n\n    def drop_piece(grid, col, mark, config):\n        \"\"\"Drop a piece in the specified column.\"\"\"\n        next_grid = grid.copy()\n        for row in range(config.rows - 1, -1, -1):\n            if next_grid[row][col] == 0:\n                next_grid[row][col] = mark\n                break\n        return next_grid\n\n    def get_heuristic(grid, mark, config):\n        \"\"\"Evaluate grid based on potential threats & opportunities.\"\"\"\n        num_threes = count_windows(grid, 3, mark, config)\n        num_threes_opp = count_windows(grid, 3, mark % 2 + 1, config)\n        return num_threes - num_threes_opp  # Favor our own 3-in-a-row\n\n    def count_windows(grid, num_discs, piece, config):\n        \"\"\"Count windows (sequences) where a move could win.\"\"\"\n        num_windows = 0\n        for row in range(config.rows):\n            for col in range(config.columns - (config.inarow - 1)):\n                window = list(grid[row, col : col + config.inarow])\n                if window.count(piece) == num_discs and window.count(0) == config.inarow - num_discs:\n                    num_windows += 1\n        return num_windows\n\n    # Select valid moves and compute scores\n    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n    scores = {col: score_move(grid, col, obs.mark, config) for col in valid_moves}\n\n    # Pick the best-scoring move\n    best_moves = [key for key in scores.keys() if scores[key] == max(scores.values())]\n    return random.choice(best_moves)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T04:51:15.024077Z","iopub.execute_input":"2025-02-10T04:51:15.024388Z","iopub.status.idle":"2025-02-10T04:51:15.032172Z","shell.execute_reply.started":"2025-02-10T04:51:15.024364Z","shell.execute_reply":"2025-02-10T04:51:15.031234Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class ConnectFourGym(gym.Env):\n    def __init__(self, agent2=\"random\"):\n        ks_env = make(\"connectx\", debug=True)\n        self.env = ks_env.train([None, agent2])\n        self.rows = ks_env.configuration.rows\n        self.columns = ks_env.configuration.columns\n        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n        self.action_space = spaces.Discrete(self.columns)\n        self.observation_space = spaces.Box(low=0, high=2, \n                                            shape=(1,self.rows,self.columns), dtype=int)\n\n        # Tuple corresponding to the min and max possible rewards\n        self.reward_range = (-10, 1)\n        # StableBaselines throws error if these are not defined\n        self.spec = None\n        self.metadata = None\n\n        self.history = []  # Store (episode, wins, losses, draws)\n        self.wins = 0\n        self.losses = 0\n        self.draws = 0\n        self.timestep = 0  # âœ… Track total timesteps\n        self.winR = 5       # Reduce win reward slightly\n        self.lostR = -3      # Less severe punishment for losing\n        self.drawR = -10      # Small penalty for drawing\n        self.good_moveR = 0.2 # Encourage good moves slightly more\n\n\n    def print_board(self, board):\n        symbols = {0: \".\", 1: \"X\", 2: \"O\"}  # X for agent, O for opponent\n        for r in range(self.rows):\n            row = [symbols[board[r * self.columns + c]] for c in range(self.columns)]\n            print(\" \".join(row))\n        print(\"-\" * 10)  # Separator\n\n    \n    def reset(self):\n        self.episode_reward = 0  # Track total reward. Reset for every new gamble.\n        \n        self.obs = self.env.reset()\n        return np.array(self.obs['board']).reshape(1,self.rows,self.columns) \n\n  \n    def is_good_move(self, action):\n        \"\"\" Define logic to check if a move is strategically good. \"\"\"\n        board = np.array(self.obs['board']).reshape(self.rows, self.columns)\n        \n        # Example: Prioritize center moves (more strategic)\n        if action == self.columns // 2:\n            return True\n        return False\n\n    \n    def change_reward(self, old_reward, done, action):\n        if old_reward == 1:  # Model wins\n            return self.winR\n        elif done:  # The opponent won the game\n            return self.lostR\n        else:\n            #return 0.05  # Small positive reward\n            return 1/(self.rows*self.columns)\n    \"\"\"   \n    def change_reward(self, old_reward, done, action):\n        if old_reward == 1:  # Model wins\n            return self.winR\n        elif done:\n            if old_reward == 0:  # Detect draws\n                return self.drawR\n            return self.lostR  # Losing penalty\n        #elif self.is_good_move(action):  # Encourage good moves\n            #return self.good_moveR\n        else:\n            return 1/(self.rows*self.columns)\n    \"\"\"\n    \n            \n    def step(self, action):\n        # Check if agent's move is valid\n        is_valid = (self.obs['board'][int(action)] == 0)\n        if is_valid: # Play the move\n            self.obs, old_reward, done, _ = self.env.step(int(action))\n            #reward = self.change_reward(old_reward, done)\n            reward = self.change_reward(old_reward, done, action)\n        else: # End the game and penalize agent\n            reward, done, _ = self.drawR, True, {}\n\n\n        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")  # Debug print\n        \n        self.episode_reward += reward  # Track total episode reward\n        \n        if done: #Print traning condition.\n            self.timestep += 1  # âœ… Increment timestep counter\n            #print(f\"Episode Finished! Total Reward: {self.episode_reward}\", done)\n            \n            if reward == self.winR:\n                self.wins += 1\n                #print(f\"Episode Finished! Model Won! Total Reward: {self.episode_reward}\")\n            elif reward == self.lostR:\n                self.losses += 1\n                #print(f\"Episode Finished! Model Lost! Total Reward: {self.episode_reward}\")\n            else:\n                self.draws += 1\n                #print(f\"Episode Finished! Draw! Total Reward: {self.episode_reward}\")\n            \n            # Store history\n            self.history.append((len(self.history) + 1, self.wins, self.losses, self.draws))\n\n            # âœ… Print every 50 timesteps\n            if self.timestep % 50 == 0:\n                total = self.wins +self.losses +self.draws\n                win_rate = self.wins / total\n                loss_rate = self.losses / total\n                draw_rate = self.draws / total\n            \n                print(f\"[Step {self.timestep}] Win rate: {win_rate:.2f}, Loss rate: {loss_rate:.2f}, Draw rate: {draw_rate:.2f}, Total Reward: {self.episode_reward}\")\n                #print(f\"[Step {self.timestep}] Total Wins: {self.wins}, Losses: {self.losses}, Draws: {self.draws}, Total Reward: {self.episode_reward}\")\n\n            #print(f\"Total Win:{ self.wins}; Total Lost:{self.losses}; Total draw:{self.draws}\")\n\n        #print(f\"Model chose action: {action}, Reward: {reward}\")\n        #self.print_board(self.obs['board'])  # ğŸ”¥ Visualize board after each move\n        \n        return np.array(self.obs['board']).reshape(1,self.rows,self.columns), reward, done, _","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T04:51:18.375159Z","iopub.execute_input":"2025-02-10T04:51:18.375452Z","iopub.status.idle":"2025-02-10T04:51:18.386957Z","shell.execute_reply.started":"2025-02-10T04:51:18.375430Z","shell.execute_reply":"2025-02-10T04:51:18.386143Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class ConnectFourGym2(gym.Env):\n    def __init__(self, agent2=\"random\"):\n        ks_env = make(\"connectx\", debug=True)\n        self.env = ks_env.train([None, agent2])\n        self.rows = ks_env.configuration.rows\n        self.columns = ks_env.configuration.columns\n\n        self.action_space = spaces.Discrete(self.columns)\n        self.observation_space = spaces.Box(low=0, high=2, shape=(1, self.rows, self.columns), dtype=int)\n\n        self.reward_range = (-1, 1)  # ğŸ”¥ Stable reward scaling\n        self.spec = None\n        self.metadata = None\n\n        self.history = []  \n        self.wins = 0\n        self.losses = 0\n        self.draws = 0\n        self.timestep = 0  \n\n        # ğŸ”¥ More stable reward values\n        self.winR = 5.0        # ğŸ”¥ Winning = 1\n        self.lostR = -3.0      # ğŸ”¥ Losing = -1\n        self.drawR = -0.2      # ğŸ”¥ Draw = Small penalty to encourage winning\n        self.illegal_moveR = -8  # ğŸ”¥ Less severe than -1, allows recovery\n        self.good_moveR = 0.05  # ğŸ”¥ Small reward for strong moves\n\n    def reset(self):\n        self.episode_reward = 0  \n        self.obs = self.env.reset()\n        return np.array(self.obs['board']).reshape(1, self.rows, self.columns)\n\n    def is_good_move(self, action):\n        \"\"\"Check if a move is good based on board state.\"\"\"\n        board = np.array(self.obs['board']).reshape(self.rows, self.columns)\n\n        if action == self.columns // 2:  # ğŸ”¥ Center is usually a good move\n            return 1  \n\n        # âœ… Check if the column is full before proceeding\n        available_rows = np.where(board[:, action] == 0)[0]\n        if available_rows.size == 0:  \n            return 0  # Invalid move\n\n        row = np.max(available_rows)  # Lowest available row\n        board[row, action] = 1  \n\n        if self.check_victory(board, 1):  \n            return 2  # High reward for winning moves\n\n        return 0  \n\n    def check_victory(self, board, player):\n        \"\"\"Check if a given board state results in a win.\"\"\"\n        inarow = 4  \n        for r in range(self.rows):\n            for c in range(self.columns):\n                if c <= self.columns - inarow and np.all(board[r, c:c+inarow] == player):\n                    return True\n                if r <= self.rows - inarow and np.all(board[r:r+inarow, c] == player):\n                    return True\n                if r <= self.rows - inarow and c <= self.columns - inarow and np.all(np.diag(board[r:r+inarow, c:c+inarow]) == player):\n                    return True\n                if r >= inarow - 1 and c <= self.columns - inarow and np.all(np.diag(np.fliplr(board[r-3:r+1, c:c+inarow])) == player):\n                    return True\n        return False\n\n    def change_reward(self, old_reward, done, action, move_quality):\n        \"\"\"Modify reward based on game state.\"\"\"\n        if old_reward == 1:  \n            return self.winR\n        elif done:\n            return self.lostR if old_reward == 0 else self.drawR  \n        elif move_quality == 2:  \n            return 0.5\n        elif move_quality == 1:  \n            return self.good_moveR\n        else:\n            return 0  \n\n    def step(self, action):\n        is_valid = (self.obs['board'][int(action)] == 0)\n\n        move_quality = self.is_good_move(action)  # ğŸ”¥ Compute move quality only once\n\n        if is_valid:  \n            self.obs, old_reward, done, _ = self.env.step(int(action))\n            reward = self.change_reward(old_reward, done, action, move_quality)\n        else:  \n            reward, done, _ = self.illegal_moveR, True, {}  # ğŸ”¥ Let agent retry\n\n        self.episode_reward += reward  \n        \n        if done:  \n            self.timestep += 1  \n            \n            if reward == self.winR:\n                self.wins += 1\n            elif reward == self.lostR:\n                self.losses += 1\n            else:\n                self.draws += 1\n\n            self.history.append((self.timestep, self.wins, self.losses, self.draws))\n\n            if self.timestep % 50 == 0:\n                total = self.wins + self.losses + self.draws\n                win_rate = self.wins / total\n                loss_rate = self.losses / total\n                draw_rate = self.draws / total\n            \n                print(f\"[Step {self.timestep}] Win rate: {win_rate:.2f}, Loss rate: {loss_rate:.2f}, Draw rate: {draw_rate:.2f}, Total Reward: {self.episode_reward}\")\n\n        return np.array(self.obs['board']).reshape(1, self.rows, self.columns), reward, done, {}\n\n\nfrom stable_baselines3.common.callbacks import BaseCallback\nimport os\nimport time\n\nclass StopTrainingCallback(BaseCallback):\n    def __init__(self, save_freq=1000, threshold=0.05, verbose=1):\n        super().__init__(verbose)\n        self.threshold = threshold\n        self.save_freq = save_freq\n        self.bestWinRate = 0\n        #self.save_path = save_path\n\n    def _on_step(self) -> bool:\n        \"\"\"Called at every step during training.\"\"\"\n        total_games = self.training_env.envs[0].wins + self.training_env.envs[0].losses + self.training_env.envs[0].draws\n        if total_games > 25:\n            win_rate = self.training_env.envs[0].wins / total_games\n        else:\n            win_rate = -1\n        #print(win_rate)\n        if win_rate > self.bestWinRate: #self.training_env.envs[0].bestWinRate:\n            print(self.bestWinRate)\n            self.bestWinRate = win_rate\n            model.save(f\"ppo_connect4_DynReward_WR_{win_rate:.2f}.pkl\")  # Save updated model\n        #print(self.bestWinRate)\n        \n        \"\"\"\n        if self.n_calls % self.save_freq == 0:  # Every 50 steps\n            timestamp = time.strftime(\"%Y-%m-%d_%H-%M-%S\")  # Format: YYYY-MM-DD_HH-MM-SS\n            model_path = f\"ppo_{timestamp}_step_{self.n_calls}.zip\"\n            self.model.save(model_path)\n            if self.verbose:\n                print(f\"âœ… Model saved at: {model_path}\")     \n        \"\"\"\n                \n        total_games = self.training_env.envs[0].wins + self.training_env.envs[0].losses + self.training_env.envs[0].draws\n        if total_games > 100:  # Avoid division by zero\n            loss_rate = self.training_env.envs[0].losses / total_games\n            #if self.verbose > 0:\n                #print(f\"ğŸ” Checking Stop Condition: Loss Rate = {loss_rate:.4f}\")\n\n            if loss_rate < self.threshold:  # âœ… Stop training if loss rate is too low\n                print(f\"ğŸš€ Stopping Training! Loss Rate = {loss_rate:.4f} < {self.threshold}\")\n                return False  # Returning False stops training\n        return True  # Continue training\n\n# âœ… Initialize the callback\nstop_callback = StopTrainingCallback(threshold=0.06, verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T05:16:27.533691Z","iopub.execute_input":"2025-02-10T05:16:27.533964Z","iopub.status.idle":"2025-02-10T05:16:27.551691Z","shell.execute_reply.started":"2025-02-10T05:16:27.533942Z","shell.execute_reply":"2025-02-10T05:16:27.550844Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"class DymaicRewardConnectFour(gym.Env):\n    def __init__(self, agent2=\"random\"):\n        ks_env = make(\"connectx\", debug=True)\n        self.env = ks_env.train([None, agent2])\n        self.config = ks_env.configuration\n        self.rows = ks_env.configuration.rows\n        self.columns = ks_env.configuration.columns\n        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n        self.action_space = spaces.Discrete(self.columns)\n        self.observation_space = spaces.Box(low=0, high=2, \n                                            shape=(1,self.rows,self.columns), dtype=int)\n        # Tuple corresponding to the min and max possible rewards\n        self.reward_range = (-10, 10)\n        # StableBaselines throws error if these are not defined\n        self.spec = None\n        self.metadata = None\n        self.winR = 5\n        self.lostR = -3\n\n        self.wins = 0\n        self.losses = 0\n        self.draws = 0\n        \n        self.timestep = 0\n        self.history = []\n\n        self.bestWinRate = 0\n        \n        \n        #####Dynaimc reward\n    # Helper function for get_heuristic: checks if window satisfies heuristic conditions\n    def check_window(self, window, num_discs, piece, config):\n        return (window.count(piece) == num_discs and window.count(0) == config.inarow-num_discs)\n        \n    # Helper function for get_heuristic: counts number of windows satisfying specified heuristic conditions\n    def count_windows(self, grid, num_discs, piece, config):\n        num_windows = 0\n        # horizontal\n        for row in range(config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[row, col:col+config.inarow])\n                if self.check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # vertical\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns):\n                window = list(grid[row:row+config.inarow, col])\n                if self.check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # positive diagonal\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n                if self.check_window(window, num_discs, piece, config):\n                    num_windows += 1\n    # negative diagonal\n        for row in range(config.inarow-1, config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n                if self.check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        return num_windows\n\n    \n    # Helper function for minimax: calculates value of heuristic for grid\n    def get_heuristic(self, grid, mark, config):\n        num_threes = self.count_windows(grid, 3, mark, config)\n        num_fours = self.count_windows(grid, 4, mark, config)\n        num_threes_opp = self.count_windows(grid, 3, mark%2+1, config)\n        num_fours_opp = self.count_windows(grid, 4, mark%2+1, config)\n        score = num_threes - 1e2*num_threes_opp - 1e4*num_fours_opp + 1e6*num_fours\n        return score\n        \n    def reset(self):\n        self.obs = self.env.reset()\n        self.episode_reward = 0\n        \n        return np.array(self.obs['board']).reshape(1,self.rows,self.columns)\n\n        \n    #def change_reward(self, old_reward, done, grid, mark, config):\n        #return self.get_heuristic(grid, mark, config)#/1e6*5\n        \"\"\"\n        if old_reward == 1: # The agent won the game\n            return self.winR\n        elif done: # The opponent won the game\n            return self.lostR\n        else: # Reward 1/42\n            return self.get_heuristic(grid, mark, config)\n            #return 1/(self.rows*self.columns)\n        \"\"\"\n    \n    \n    def change_reward(self, old_reward, done, grid, mark, config):\n        #Assigns a dynamic reward based on board state.\n       \n        heuristic_score = self.get_heuristic(grid, mark, config)  # Compute heuristic score\n    \n        if old_reward == 1:  # Agent won the game ğŸ‰\n            return self.winR  # âœ… Give a high reward for winning\n        \n        elif done:  # Game ended\n            return self.lostR  # âŒ Give a harsh penalty for losing\n\n        elif heuristic_score > 5000:  # If the move creates a strong advantage\n            return 2  # ğŸ”¥ Reward highly for good strategy\n        \n        elif heuristic_score < -5000:  # If the move helps opponent significantly\n            return -3  # âŒ Penalize bad moves that give opponent advantage\n        \n        elif heuristic_score > 1000:  # If the move is decent\n            return 0.5  # ğŸ‘ Small encouragement\n        \n        elif heuristic_score < -1000:  # If the move is weak\n            return -1  # âš ï¸ Small penalty\n            \n        else:\n            return 1/(self.rows*self.columns)*self.winR  # Neutral reward for normal moves\n           \n    def step(self, action):\n        # Check if agent's move is valid\n        is_valid = (self.obs['board'][int(action)] == 0)\n        grid = np.asarray(self.obs.board).reshape(self.config.rows, self.config.columns)\n        if is_valid: # Play the move\n            self.obs, old_reward, done, _ = self.env.step(int(action))\n            reward = self.change_reward(old_reward, done, grid, self.obs.mark, self.config)\n        else: # End the game and penalize agent\n            reward, done, _ = -5, True, {}\n\n        self.episode_reward += reward  # Track total episode reward\n        \n        if done: #Print traning condition.\n            self.timestep += 1  # âœ… Increment timestep counter\n            #print(f\"Episode Finished! Total Reward: {self.episode_reward}\", done)\n            \n            if reward == self.winR:\n                self.wins += 1\n                #print(f\"Episode Finished! Model Won! Total Reward: {self.episode_reward}\")\n            elif reward == self.lostR:\n                self.losses += 1\n                #print(f\"Episode Finished! Model Lost! Total Reward: {self.episode_reward}\")\n            else:\n                self.draws += 1\n                #print(f\"Episode Finished! Draw! Total Reward: {self.episode_reward}\")\n            \n            # Store history\n            self.history.append((len(self.history) + 1, self.wins, self.losses, self.draws))\n            #print(f\"[Step {self.timestep}] Win: {self.wins}, Loss: {self.losses}, Draw: {self.draws}, Total Reward: {self.episode_reward}\")\n\n           \n            # âœ… Print every 50 timesteps\n            if self.timestep % 50 == 0:\n                total = self.wins +self.losses +self.draws\n                win_rate = self.wins / total\n                loss_rate = self.losses / total\n                draw_rate = self.draws / total\n                print(f\"[Step {self.timestep}] Win rate: {win_rate:.2f}, Loss rate: {loss_rate:.2f}, Draw rate: {draw_rate:.2f}, Total Reward: {self.episode_reward}\")\n                #Recompute\n                self.wins = 0\n                self.losses = 0\n                self.draws = 0\n\n        #print(f\"Total Reward: {self.episode_reward}\")\n        \n        return np.array(self.obs['board']).reshape(1,self.rows,self.columns), reward, done, _\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T05:16:35.335578Z","iopub.execute_input":"2025-02-10T05:16:35.335874Z","iopub.status.idle":"2025-02-10T05:16:35.353057Z","shell.execute_reply.started":"2025-02-10T05:16:35.335851Z","shell.execute_reply":"2025-02-10T05:16:35.352275Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n    print(\"âœ… GPU is available: Using\", torch.cuda.get_device_name(0))\nelse:\n    print(\"âŒ GPU not available: Using CPU\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T05:16:39.762161Z","iopub.execute_input":"2025-02-10T05:16:39.762488Z","iopub.status.idle":"2025-02-10T05:16:39.767582Z","shell.execute_reply.started":"2025-02-10T05:16:39.762464Z","shell.execute_reply":"2025-02-10T05:16:39.766684Z"}},"outputs":[{"name":"stdout","text":"âœ… GPU is available: Using Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom stable_baselines3 import PPO\n\n# âœ… Set device explicitly\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"âœ… Using device: {device}\")\n\n\nmodel = PPO.load(f\"/kaggle/input/ppo_connect4_selfplay_v8/pytorch/default/1/ppo_connect4_selfplay_v8.pkl\", device=device)\n\ndef self_play_opponent(obs, config):\n    obs_array = np.array(obs['board']).reshape(1, config.rows, config.columns)\n    action, _states = model.predict(obs_array)\n    \n    valid_moves = [c for c in range(config.columns) if obs_array[0, 0, c] == 0]  # Only pick from non-full columns\n    if action not in valid_moves:  # If chosen action is invalid, pick a valid one\n        action = random.choice(valid_moves)\n        \n    return int(action)\n\n# âœ… Train against previous self-play version\nenvSelfPlay = DymaicRewardConnectFour(agent2=\"random\")\n\npolicy_kwargs = dict(features_extractor_class=CustomResNetCNN)\n\n#new_model = PPO(\"CnnPolicy\", envSelfPlay, policy_kwargs=policy_kwargs, \n                #learning_rate=0.0003, gamma=0.995, clip_range=0.2, \n                #ent_coef=0.01, n_steps=2048, batch_size=32, verbose=1, device=device)\n#new_model = PPO(\"CnnPolicy\", envSelfPlay, policy_kwargs=policy_kwargs, \n               # learning_rate=0.0003, n_steps=2048, verbose=0, device=device)\nnew_model = PPO(\"CnnPolicy\", envSelfPlay, policy_kwargs=policy_kwargs, \n                learning_rate=0.0003, verbose=0, device=device)\n#new_model.policy.load_state_dict(PPO.load(\"/kaggle/input/ppo_connect4_selfplay_v10_0209/pytorch/default/1/ppo_connect4_selfplay_v10_0209.pkl\", device=device).policy.state_dict())\n\n# âœ… Debug GPU Training with Small Steps First\n#print(f\"\\nğŸš€ Starting Training...Iteration {i}\")\nnew_model.learn(total_timesteps=60000, callback=stop_callback)  # ğŸ”¥ Start with a small number of steps\nnew_model.save(f\"ppo_connect4_DynReward_v{1}.pkl\")  # Save updated model\n\n#selfPlay()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T05:16:53.046881Z","iopub.execute_input":"2025-02-10T05:16:53.047208Z","iopub.status.idle":"2025-02-10T05:16:54.238816Z","shell.execute_reply.started":"2025-02-10T05:16:53.047182Z","shell.execute_reply":"2025-02-10T05:16:54.237682Z"}},"outputs":[{"name":"stdout","text":"âœ… Using device: cuda\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-52-baaecbbf6343>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#new_model = PPO(\"CnnPolicy\", envSelfPlay, policy_kwargs=policy_kwargs,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                \u001b[0;31m# learning_rate=0.0003, n_steps=2048, verbose=0, device=device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m new_model = PPO(\"CnnPolicy\", envSelfPlay, policy_kwargs=policy_kwargs, \n\u001b[0m\u001b[1;32m     33\u001b[0m                 learning_rate=0.0003, verbose=0, device=device)\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#new_model.policy.load_state_dict(PPO.load(\"/kaggle/input/ppo_connect4_selfplay_v10_0209/pytorch/default/1/ppo_connect4_selfplay_v10_0209.pkl\", device=device).policy.state_dict())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, target_kl, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_init_setup_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36m_setup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# Initialize schedules for policy/value clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36m_setup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m         )\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# pytype:disable=not-instantiable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         self.policy = self.policy_class(  # type: ignore[assignment]\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_schedule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_sde\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_sde\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, observation_space, action_space, lr_schedule, net_arch, activation_fn, ortho_init, use_sde, log_std_init, full_std, use_expln, squash_output, features_extractor_class, features_extractor_kwargs, share_features_extractor, normalize_images, optimizer_class, optimizer_kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0moptimizer_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m     ):\n\u001b[0;32m--> 780\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    781\u001b[0m             \u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, observation_space, action_space, lr_schedule, net_arch, activation_fn, ortho_init, use_sde, log_std_init, full_std, use_expln, squash_output, features_extractor_class, features_extractor_kwargs, share_features_extractor, normalize_images, optimizer_class, optimizer_kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_proba_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_sde\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_sde\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdist_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_schedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_constructor_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self, lr_schedule)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgain\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule_gains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;31m# Setup optimizer with initial learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \"\"\"\n\u001b[1;32m   1028\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \"\"\"\n\u001b[1;32m   1028\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \"\"\"\n\u001b[1;32m   1028\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36minit_weights\u001b[0;34m(module, gain)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \"\"\"\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morthogonal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/init.py\u001b[0m in \u001b[0;36morthogonal_\u001b[0;34m(tensor, gain, generator)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0;31m# Compute the qr factorization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflattened\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;31m# Make Q uniform according to https://arxiv.org/pdf/math-ph/0609050.pdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":52},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom stable_baselines3 import PPO\n\n# âœ… Set device explicitly\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"âœ… Using device: {device}\")\n\ndef selfPlay(iteration=20):\n    for i in range(11, iteration):\n        if i == 11:\n            #model = PPO.load(f\"/kaggle/input/ppo_connect4_selfplay_v8/pytorch/default/1/ppo_connect4_selfplay_v8.pkl\", device=device)\n            model = PPO.load(f\"/kaggle/working/ppo_connect4_selfplay_v11.pkl\", device=device)\n        else:\n            model = PPO.load(f\"/kaggle/working/ppo_connect4_selfplay_v{i}.pkl\", device=device)\n            \n        def self_play_opponent(obs, config):\n            obs_array = np.array(obs['board']).reshape(1, config.rows, config.columns)\n            action, _states = model.predict(obs_array)\n            \n            valid_moves = [c for c in range(config.columns) if obs_array[0, 0, c] == 0]  # Only pick from non-full columns\n            if action not in valid_moves:  # If chosen action is invalid, pick a valid one\n                #action = random.choice(valid_moves)\n                action = MinMaxAB_agent(obs, config)\n                \n            return int(action)\n        \n        # âœ… Train against previous self-play version\n        #envSelfPlay = ConnectFourGym(agent2=self_play_opponent)\n        envSelfPlay = DymaicRewardConnectFour(agent2=self_play_opponent)\n\n        policy_kwargs = dict(features_extractor_class=CustomResNetCNN)\n        \n        \"\"\"\n        new_model = PPO(\"CnnPolicy\", envSelfPlay, policy_kwargs=policy_kwargs, \n                        learning_rate=0.0003, gamma=0.995, clip_range=0.2, \n                        ent_coef=0.01, n_steps=2048, batch_size=32, verbose=1, device=device)\n        \"\"\"\n        \n        new_model = PPO(\"CnnPolicy\", envSelfPlay, policy_kwargs=policy_kwargs, \n                        learning_rate=0.0003, verbose=1, device=device)\n                        \n        new_model.policy.load_state_dict(model.policy.state_dict())\n        \n        # âœ… Debug GPU Training with Small Steps First\n        print(f\"\\nğŸš€ Starting Training...Iteration {i}\")\n        new_model.learn(total_timesteps=20000, callback=stop_callback)  # ğŸ”¥ Start with a small number of steps\n        new_model.save(f\"ppo_connect4_selfplay_v{i+1}.pkl\")  # Save updated model\n\nselfPlay()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T05:56:44.322668Z","iopub.execute_input":"2025-02-10T05:56:44.323010Z","iopub.status.idle":"2025-02-10T06:07:26.037563Z","shell.execute_reply.started":"2025-02-10T05:56:44.322983Z","shell.execute_reply":"2025-02-10T06:07:26.036252Z"}},"outputs":[{"name":"stdout","text":"âœ… Using device: cuda\nUsing cuda device\nWrapping the env with a `Monitor` wrapper\nWrapping the env in a DummyVecEnv.\n\nğŸš€ Starting Training...Iteration 11\n[Step 50] Win rate: 0.44, Loss rate: 0.24, Draw rate: 0.32, Total Reward: -2.1666666666666665\n[Step 100] Win rate: 0.60, Loss rate: 0.22, Draw rate: 0.18, Total Reward: -1.6904761904761905\n[Step 150] Win rate: 0.46, Loss rate: 0.26, Draw rate: 0.28, Total Reward: -3.928571428571429\n[Step 200] Win rate: 0.54, Loss rate: 0.32, Draw rate: 0.14, Total Reward: 6.309523809523809\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 9.67     |\n|    ep_rew_mean     | 1.75     |\n| time/              |          |\n|    fps             | 76       |\n|    iterations      | 1        |\n|    time_elapsed    | 26       |\n|    total_timesteps | 2048     |\n---------------------------------\n[Step 250] Win rate: 0.46, Loss rate: 0.42, Draw rate: 0.12, Total Reward: 6.190476190476191\n[Step 300] Win rate: 0.48, Loss rate: 0.24, Draw rate: 0.28, Total Reward: 5.476190476190476\n[Step 350] Win rate: 0.48, Loss rate: 0.32, Draw rate: 0.20, Total Reward: -2.1666666666666665\n[Step 400] Win rate: 0.52, Loss rate: 0.26, Draw rate: 0.22, Total Reward: 5.714285714285714\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 9.62       |\n|    ep_rew_mean          | 1.25       |\n| time/                   |            |\n|    fps                  | 70         |\n|    iterations           | 2          |\n|    time_elapsed         | 58         |\n|    total_timesteps      | 4096       |\n| train/                  |            |\n|    approx_kl            | 0.18085884 |\n|    clip_fraction        | 0.53       |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -1.29      |\n|    explained_variance   | -0.318     |\n|    learning_rate        | 0.0003     |\n|    loss                 | 2.06       |\n|    n_updates            | 10         |\n|    policy_gradient_loss | 0.00771    |\n|    value_loss           | 5.58       |\n----------------------------------------\n[Step 450] Win rate: 0.38, Loss rate: 0.40, Draw rate: 0.22, Total Reward: -3.2142857142857144\n[Step 500] Win rate: 0.50, Loss rate: 0.38, Draw rate: 0.12, Total Reward: -3.928571428571429\n[Step 550] Win rate: 0.38, Loss rate: 0.42, Draw rate: 0.20, Total Reward: 5.9523809523809526\n[Step 600] Win rate: 0.60, Loss rate: 0.28, Draw rate: 0.12, Total Reward: 6.190476190476191\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 9.67       |\n|    ep_rew_mean          | 2.11       |\n| time/                   |            |\n|    fps                  | 68         |\n|    iterations           | 3          |\n|    time_elapsed         | 89         |\n|    total_timesteps      | 6144       |\n| train/                  |            |\n|    approx_kl            | 0.15538797 |\n|    clip_fraction        | 0.508      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -1.3       |\n|    explained_variance   | -0.162     |\n|    learning_rate        | 0.0003     |\n|    loss                 | 1.4        |\n|    n_updates            | 20         |\n|    policy_gradient_loss | -0.00508   |\n|    value_loss           | 5.3        |\n----------------------------------------\n[Step 650] Win rate: 0.48, Loss rate: 0.32, Draw rate: 0.20, Total Reward: -1.4523809523809523\n[Step 700] Win rate: 0.76, Loss rate: 0.18, Draw rate: 0.06, Total Reward: 5.9523809523809526\n[Step 750] Win rate: 0.52, Loss rate: 0.34, Draw rate: 0.14, Total Reward: 5.714285714285714\n[Step 800] Win rate: 0.46, Loss rate: 0.32, Draw rate: 0.22, Total Reward: -4.285714285714286\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 9.49       |\n|    ep_rew_mean          | 1.51       |\n| time/                   |            |\n|    fps                  | 67         |\n|    iterations           | 4          |\n|    time_elapsed         | 120        |\n|    total_timesteps      | 8192       |\n| train/                  |            |\n|    approx_kl            | 0.15301023 |\n|    clip_fraction        | 0.485      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -1.31      |\n|    explained_variance   | -0.224     |\n|    learning_rate        | 0.0003     |\n|    loss                 | 1.9        |\n|    n_updates            | 30         |\n|    policy_gradient_loss | -0.00239   |\n|    value_loss           | 5.25       |\n----------------------------------------\n[Step 850] Win rate: 0.44, Loss rate: 0.40, Draw rate: 0.16, Total Reward: 6.904761904761905\n[Step 900] Win rate: 0.56, Loss rate: 0.26, Draw rate: 0.18, Total Reward: -3.4523809523809526\n[Step 950] Win rate: 0.64, Loss rate: 0.18, Draw rate: 0.18, Total Reward: 5.595238095238095\n[Step 1000] Win rate: 0.46, Loss rate: 0.44, Draw rate: 0.10, Total Reward: -1.4523809523809523\n[Step 1050] Win rate: 0.50, Loss rate: 0.32, Draw rate: 0.18, Total Reward: 5.9523809523809526\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 9.47       |\n|    ep_rew_mean          | 1.51       |\n| time/                   |            |\n|    fps                  | 67         |\n|    iterations           | 5          |\n|    time_elapsed         | 152        |\n|    total_timesteps      | 10240      |\n| train/                  |            |\n|    approx_kl            | 0.18606427 |\n|    clip_fraction        | 0.501      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -1.27      |\n|    explained_variance   | -0.0837    |\n|    learning_rate        | 0.0003     |\n|    loss                 | 2.16       |\n|    n_updates            | 40         |\n|    policy_gradient_loss | 0.01       |\n|    value_loss           | 5.85       |\n----------------------------------------\n[Step 1100] Win rate: 0.54, Loss rate: 0.24, Draw rate: 0.22, Total Reward: -2.5238095238095237\n[Step 1150] Win rate: 0.54, Loss rate: 0.32, Draw rate: 0.14, Total Reward: -2.1666666666666665\n[Step 1200] Win rate: 0.42, Loss rate: 0.52, Draw rate: 0.06, Total Reward: 6.5476190476190474\n[Step 1250] Win rate: 0.60, Loss rate: 0.32, Draw rate: 0.08, Total Reward: 5.476190476190476\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 9.2        |\n|    ep_rew_mean          | 2.3        |\n| time/                   |            |\n|    fps                  | 66         |\n|    iterations           | 6          |\n|    time_elapsed         | 183        |\n|    total_timesteps      | 12288      |\n| train/                  |            |\n|    approx_kl            | 0.15980372 |\n|    clip_fraction        | 0.477      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -1.27      |\n|    explained_variance   | -0.0505    |\n|    learning_rate        | 0.0003     |\n|    loss                 | 1.58       |\n|    n_updates            | 50         |\n|    policy_gradient_loss | 0.0147     |\n|    value_loss           | 5.19       |\n----------------------------------------\n[Step 1300] Win rate: 0.60, Loss rate: 0.34, Draw rate: 0.06, Total Reward: 6.071428571428571\n[Step 1350] Win rate: 0.70, Loss rate: 0.28, Draw rate: 0.02, Total Reward: 6.785714285714286\n[Step 1400] Win rate: 0.46, Loss rate: 0.46, Draw rate: 0.08, Total Reward: -1.9285714285714286\n[Step 1450] Win rate: 0.56, Loss rate: 0.40, Draw rate: 0.04, Total Reward: -2.0476190476190474\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 9.77       |\n|    ep_rew_mean          | 2.24       |\n| time/                   |            |\n|    fps                  | 66         |\n|    iterations           | 7          |\n|    time_elapsed         | 214        |\n|    total_timesteps      | 14336      |\n| train/                  |            |\n|    approx_kl            | 0.13913605 |\n|    clip_fraction        | 0.455      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -1.24      |\n|    explained_variance   | -0.141     |\n|    learning_rate        | 0.0003     |\n|    loss                 | 2.1        |\n|    n_updates            | 60         |\n|    policy_gradient_loss | 0.000813   |\n|    value_loss           | 5.28       |\n----------------------------------------\n[Step 1500] Win rate: 0.52, Loss rate: 0.36, Draw rate: 0.12, Total Reward: 5.833333333333333\n[Step 1550] Win rate: 0.68, Loss rate: 0.22, Draw rate: 0.10, Total Reward: 5.9523809523809526\n[Step 1600] Win rate: 0.60, Loss rate: 0.30, Draw rate: 0.10, Total Reward: -2.0476190476190474\n[Step 1650] Win rate: 0.60, Loss rate: 0.34, Draw rate: 0.06, Total Reward: 5.714285714285714\n[Step 1700] Win rate: 0.44, Loss rate: 0.44, Draw rate: 0.12, Total Reward: -1.8095238095238095\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 9.32       |\n|    ep_rew_mean          | 2.47       |\n| time/                   |            |\n|    fps                  | 66         |\n|    iterations           | 8          |\n|    time_elapsed         | 246        |\n|    total_timesteps      | 16384      |\n| train/                  |            |\n|    approx_kl            | 0.14594033 |\n|    clip_fraction        | 0.439      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -1.25      |\n|    explained_variance   | -0.181     |\n|    learning_rate        | 0.0003     |\n|    loss                 | 1.83       |\n|    n_updates            | 70         |\n|    policy_gradient_loss | -0.0057    |\n|    value_loss           | 4.97       |\n----------------------------------------\n[Step 1750] Win rate: 0.60, Loss rate: 0.32, Draw rate: 0.08, Total Reward: 5.9523809523809526\n[Step 1800] Win rate: 0.62, Loss rate: 0.28, Draw rate: 0.10, Total Reward: 5.714285714285714\n[Step 1850] Win rate: 0.64, Loss rate: 0.30, Draw rate: 0.06, Total Reward: 5.9523809523809526\n[Step 1900] Win rate: 0.54, Loss rate: 0.34, Draw rate: 0.12, Total Reward: 5.833333333333333\n---------------------------------------\n| rollout/                |           |\n|    ep_len_mean          | 9.66      |\n|    ep_rew_mean          | 2.67      |\n| time/                   |           |\n|    fps                  | 66        |\n|    iterations           | 9         |\n|    time_elapsed         | 278       |\n|    total_timesteps      | 18432     |\n| train/                  |           |\n|    approx_kl            | 0.1487549 |\n|    clip_fraction        | 0.441     |\n|    clip_range           | 0.2       |\n|    entropy_loss         | -1.21     |\n|    explained_variance   | -0.0506   |\n|    learning_rate        | 0.0003    |\n|    loss                 | 2         |\n|    n_updates            | 80        |\n|    policy_gradient_loss | -0.000674 |\n|    value_loss           | 5.36      |\n---------------------------------------\n[Step 1950] Win rate: 0.70, Loss rate: 0.24, Draw rate: 0.06, Total Reward: -2.4047619047619047\n0.8484848484848485\n[Step 2000] Win rate: 0.58, Loss rate: 0.34, Draw rate: 0.08, Total Reward: -1.9285714285714286\n[Step 2050] Win rate: 0.48, Loss rate: 0.46, Draw rate: 0.06, Total Reward: -2.2857142857142856\n[Step 2100] Win rate: 0.72, Loss rate: 0.26, Draw rate: 0.02, Total Reward: 6.071428571428571\n[Step 2150] Win rate: 0.76, Loss rate: 0.24, Draw rate: 0.00, Total Reward: 5.476190476190476\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 9.03       |\n|    ep_rew_mean          | 4          |\n| time/                   |            |\n|    fps                  | 66         |\n|    iterations           | 10         |\n|    time_elapsed         | 310        |\n|    total_timesteps      | 20480      |\n| train/                  |            |\n|    approx_kl            | 0.15095186 |\n|    clip_fraction        | 0.452      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -1.21      |\n|    explained_variance   | -0.234     |\n|    learning_rate        | 0.0003     |\n|    loss                 | 2.33       |\n|    n_updates            | 90         |\n|    policy_gradient_loss | -0.000125  |\n|    value_loss           | 5.16       |\n----------------------------------------\nUsing cuda device\nWrapping the env with a `Monitor` wrapper\nWrapping the env in a DummyVecEnv.\n\nğŸš€ Starting Training...Iteration 12\n[Step 50] Win rate: 0.28, Loss rate: 0.26, Draw rate: 0.46, Total Reward: -1.4523809523809523\n[Step 100] Win rate: 0.38, Loss rate: 0.18, Draw rate: 0.44, Total Reward: -3.571428571428571\n[Step 150] Win rate: 0.22, Loss rate: 0.14, Draw rate: 0.64, Total Reward: 5.476190476190476\n[Step 200] Win rate: 0.32, Loss rate: 0.08, Draw rate: 0.60, Total Reward: -4.285714285714286\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 10.3     |\n|    ep_rew_mean     | -0.868   |\n| time/              |          |\n|    fps             | 75       |\n|    iterations      | 1        |\n|    time_elapsed    | 27       |\n|    total_timesteps | 2048     |\n---------------------------------\n[Step 250] Win rate: 0.32, Loss rate: 0.24, Draw rate: 0.44, Total Reward: -3.2142857142857144\n[Step 300] Win rate: 0.24, Loss rate: 0.16, Draw rate: 0.60, Total Reward: 5.833333333333333\n[Step 350] Win rate: 0.34, Loss rate: 0.22, Draw rate: 0.44, Total Reward: -4.166666666666667\n[Step 400] Win rate: 0.30, Loss rate: 0.18, Draw rate: 0.52, Total Reward: 5.595238095238095\n---------------------------------------\n| rollout/                |           |\n|    ep_len_mean          | 10.2      |\n|    ep_rew_mean          | -0.422    |\n| time/                   |           |\n|    fps                  | 70        |\n|    iterations           | 2         |\n|    time_elapsed         | 58        |\n|    total_timesteps      | 4096      |\n| train/                  |           |\n|    approx_kl            | 0.1844792 |\n|    clip_fraction        | 0.508     |\n|    clip_range           | 0.2       |\n|    entropy_loss         | -1.25     |\n|    explained_variance   | -0.111    |\n|    learning_rate        | 0.0003    |\n|    loss                 | 1.61      |\n|    n_updates            | 10        |\n|    policy_gradient_loss | -0.00282  |\n|    value_loss           | 5.36      |\n---------------------------------------\n[Step 450] Win rate: 0.26, Loss rate: 0.24, Draw rate: 0.50, Total Reward: -3.928571428571429\n[Step 500] Win rate: 0.36, Loss rate: 0.26, Draw rate: 0.38, Total Reward: -3.8095238095238093\n[Step 550] Win rate: 0.42, Loss rate: 0.18, Draw rate: 0.40, Total Reward: -3.571428571428571\n[Step 600] Win rate: 0.38, Loss rate: 0.24, Draw rate: 0.38, Total Reward: -3.6904761904761907\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 9.78       |\n|    ep_rew_mean          | 0.685      |\n| time/                   |            |\n|    fps                  | 68         |\n|    iterations           | 3          |\n|    time_elapsed         | 89         |\n|    total_timesteps      | 6144       |\n| train/                  |            |\n|    approx_kl            | 0.16073713 |\n|    clip_fraction        | 0.522      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -1.34      |\n|    explained_variance   | -0.207     |\n|    learning_rate        | 0.0003     |\n|    loss                 | 1.53       |\n|    n_updates            | 20         |\n|    policy_gradient_loss | 0.00715    |\n|    value_loss           | 4.94       |\n----------------------------------------\n[Step 650] Win rate: 0.44, Loss rate: 0.28, Draw rate: 0.28, Total Reward: -1.8095238095238095\n[Step 700] Win rate: 0.38, Loss rate: 0.36, Draw rate: 0.26, Total Reward: 6.309523809523809\n[Step 750] Win rate: 0.40, Loss rate: 0.36, Draw rate: 0.24, Total Reward: 5.9523809523809526\n[Step 800] Win rate: 0.46, Loss rate: 0.30, Draw rate: 0.24, Total Reward: 6.190476190476191\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 9.59       |\n|    ep_rew_mean          | 0.983      |\n| time/                   |            |\n|    fps                  | 67         |\n|    iterations           | 4          |\n|    time_elapsed         | 121        |\n|    total_timesteps      | 8192       |\n| train/                  |            |\n|    approx_kl            | 0.11464536 |\n|    clip_fraction        | 0.48       |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -1.35      |\n|    explained_variance   | 0.0554     |\n|    learning_rate        | 0.0003     |\n|    loss                 | 2.16       |\n|    n_updates            | 30         |\n|    policy_gradient_loss | -0.00546   |\n|    value_loss           | 5.2        |\n----------------------------------------\n[Step 850] Win rate: 0.42, Loss rate: 0.26, Draw rate: 0.32, Total Reward: -3.8095238095238093\n[Step 900] Win rate: 0.42, Loss rate: 0.20, Draw rate: 0.38, Total Reward: 5.9523809523809526\n[Step 950] Win rate: 0.50, Loss rate: 0.26, Draw rate: 0.24, Total Reward: 6.428571428571429\n[Step 1000] Win rate: 0.46, Loss rate: 0.16, Draw rate: 0.38, Total Reward: 5.595238095238095\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 10.1        |\n|    ep_rew_mean          | 1.42        |\n| time/                   |             |\n|    fps                  | 67          |\n|    iterations           | 5           |\n|    time_elapsed         | 152         |\n|    total_timesteps      | 10240       |\n| train/                  |             |\n|    approx_kl            | 0.120150805 |\n|    clip_fraction        | 0.466       |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.39       |\n|    explained_variance   | -0.0814     |\n|    learning_rate        | 0.0003      |\n|    loss                 | 1.74        |\n|    n_updates            | 40          |\n|    policy_gradient_loss | -0.00527    |\n|    value_loss           | 5.28        |\n-----------------------------------------\n[Step 1050] Win rate: 0.52, Loss rate: 0.22, Draw rate: 0.26, Total Reward: -1.6904761904761905\n[Step 1100] Win rate: 0.44, Loss rate: 0.36, Draw rate: 0.20, Total Reward: -2.2857142857142856\n[Step 1150] Win rate: 0.38, Loss rate: 0.40, Draw rate: 0.22, Total Reward: -1.8095238095238095\n[Step 1200] Win rate: 0.48, Loss rate: 0.36, Draw rate: 0.16, Total Reward: 5.833333333333333\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 9.72       |\n|    ep_rew_mean          | 1.52       |\n| time/                   |            |\n|    fps                  | 66         |\n|    iterations           | 6          |\n|    time_elapsed         | 184        |\n|    total_timesteps      | 12288      |\n| train/                  |            |\n|    approx_kl            | 0.12283001 |\n|    clip_fraction        | 0.475      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -1.39      |\n|    explained_variance   | -0.0287    |\n|    learning_rate        | 0.0003     |\n|    loss                 | 1.54       |\n|    n_updates            | 50         |\n|    policy_gradient_loss | -0.0108    |\n|    value_loss           | 5.41       |\n----------------------------------------\n[Step 1250] Win rate: 0.48, Loss rate: 0.28, Draw rate: 0.24, Total Reward: -2.5238095238095237\n[Step 1300] Win rate: 0.46, Loss rate: 0.24, Draw rate: 0.30, Total Reward: -3.571428571428571\n[Step 1350] Win rate: 0.36, Loss rate: 0.30, Draw rate: 0.34, Total Reward: 5.714285714285714\n[Step 1400] Win rate: 0.46, Loss rate: 0.34, Draw rate: 0.20, Total Reward: 6.071428571428571\n---------------------------------------\n| rollout/                |           |\n|    ep_len_mean          | 10.1      |\n|    ep_rew_mean          | 1.2       |\n| time/                   |           |\n|    fps                  | 66        |\n|    iterations           | 7         |\n|    time_elapsed         | 216       |\n|    total_timesteps      | 14336     |\n| train/                  |           |\n|    approx_kl            | 0.1218084 |\n|    clip_fraction        | 0.469     |\n|    clip_range           | 0.2       |\n|    entropy_loss         | -1.38     |\n|    explained_variance   | -0.237    |\n|    learning_rate        | 0.0003    |\n|    loss                 | 1.93      |\n|    n_updates            | 60        |\n|    policy_gradient_loss | 0.000153  |\n|    value_loss           | 5.31      |\n---------------------------------------\n[Step 1450] Win rate: 0.42, Loss rate: 0.32, Draw rate: 0.26, Total Reward: 5.9523809523809526\n[Step 1500] Win rate: 0.58, Loss rate: 0.32, Draw rate: 0.10, Total Reward: 6.309523809523809\n[Step 1550] Win rate: 0.58, Loss rate: 0.24, Draw rate: 0.18, Total Reward: 6.190476190476191\n[Step 1600] Win rate: 0.50, Loss rate: 0.24, Draw rate: 0.26, Total Reward: -2.0476190476190474\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 9.51       |\n|    ep_rew_mean          | 1.77       |\n| time/                   |            |\n|    fps                  | 66         |\n|    iterations           | 8          |\n|    time_elapsed         | 247        |\n|    total_timesteps      | 16384      |\n| train/                  |            |\n|    approx_kl            | 0.12612332 |\n|    clip_fraction        | 0.479      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -1.38      |\n|    explained_variance   | -0.0808    |\n|    learning_rate        | 0.0003     |\n|    loss                 | 1.31       |\n|    n_updates            | 70         |\n|    policy_gradient_loss | -0.00436   |\n|    value_loss           | 5.09       |\n----------------------------------------\n[Step 1650] Win rate: 0.56, Loss rate: 0.30, Draw rate: 0.14, Total Reward: 5.714285714285714\n[Step 1700] Win rate: 0.54, Loss rate: 0.32, Draw rate: 0.14, Total Reward: -1.9285714285714286\n[Step 1750] Win rate: 0.58, Loss rate: 0.30, Draw rate: 0.12, Total Reward: 6.309523809523809\n[Step 1800] Win rate: 0.56, Loss rate: 0.34, Draw rate: 0.10, Total Reward: 5.9523809523809526\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 9.57       |\n|    ep_rew_mean          | 2.54       |\n| time/                   |            |\n|    fps                  | 66         |\n|    iterations           | 9          |\n|    time_elapsed         | 279        |\n|    total_timesteps      | 18432      |\n| train/                  |            |\n|    approx_kl            | 0.13779756 |\n|    clip_fraction        | 0.465      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -1.32      |\n|    explained_variance   | -0.0347    |\n|    learning_rate        | 0.0003     |\n|    loss                 | 1.37       |\n|    n_updates            | 80         |\n|    policy_gradient_loss | 0.00868    |\n|    value_loss           | 5.49       |\n----------------------------------------\n[Step 1850] Win rate: 0.60, Loss rate: 0.28, Draw rate: 0.12, Total Reward: -3.2142857142857144\n[Step 1900] Win rate: 0.60, Loss rate: 0.32, Draw rate: 0.08, Total Reward: -2.1666666666666665\n[Step 1950] Win rate: 0.54, Loss rate: 0.34, Draw rate: 0.12, Total Reward: -1.5714285714285714\n[Step 2000] Win rate: 0.60, Loss rate: 0.34, Draw rate: 0.06, Total Reward: -1.9285714285714286\n[Step 2050] Win rate: 0.62, Loss rate: 0.24, Draw rate: 0.14, Total Reward: -2.0476190476190474\n---------------------------------------\n| rollout/                |           |\n|    ep_len_mean          | 9.85      |\n|    ep_rew_mean          | 2.83      |\n| time/                   |           |\n|    fps                  | 65        |\n|    iterations           | 10        |\n|    time_elapsed         | 310       |\n|    total_timesteps      | 20480     |\n| train/                  |           |\n|    approx_kl            | 0.1159106 |\n|    clip_fraction        | 0.425     |\n|    clip_range           | 0.2       |\n|    entropy_loss         | -1.32     |\n|    explained_variance   | -0.128    |\n|    learning_rate        | 0.0003    |\n|    loss                 | 2.03      |\n|    n_updates            | 90        |\n|    policy_gradient_loss | -0.00167  |\n|    value_loss           | 5.76      |\n---------------------------------------\nUsing cuda device\nWrapping the env with a `Monitor` wrapper\nWrapping the env in a DummyVecEnv.\n\nğŸš€ Starting Training...Iteration 13\n[Step 50] Win rate: 0.46, Loss rate: 0.34, Draw rate: 0.20, Total Reward: 5.595238095238095\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-109-c23a5665d205>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ppo_connect4_selfplay_v{i+1}.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Save updated model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mselfPlay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-109-c23a5665d205>\u001b[0m in \u001b[0;36mselfPlay\u001b[0;34m(iteration)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# âœ… Debug GPU Training with Small Steps First\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nğŸš€ Starting Training...Iteration {i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_callback\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ğŸ”¥ Start with a small number of steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ppo_connect4_selfplay_v{i+1}.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Save updated model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     ) -> SelfPPO:\n\u001b[0;32m--> 308\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontinue_training\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0mclipped_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mnew_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \"\"\"\n\u001b[1;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Avoid circular imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/monitor.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to step environment that needs reset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mterminated\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/shimmy/openai_gym_compatibility.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \"\"\"\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgym_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_mode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-50-9394c54a80dd>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_valid\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Play the move\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchange_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# End the game and penalize agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-50-9394c54a80dd>\u001b[0m in \u001b[0;36mchange_reward\u001b[0;34m(self, old_reward, done, grid, mark, config)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m#Assigns a dynamic reward based on board state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mheuristic_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_heuristic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute heuristic score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mold_reward\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Agent won the game ğŸ‰\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-50-9394c54a80dd>\u001b[0m in \u001b[0;36mget_heuristic\u001b[0;34m(self, grid, mark, config)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Helper function for minimax: calculates value of heuristic for grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_heuristic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mnum_threes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_windows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mnum_fours\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_windows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mnum_threes_opp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_windows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmark\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-50-9394c54a80dd>\u001b[0m in \u001b[0;36mcount_windows\u001b[0;34m(self, grid, num_discs, piece, config)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minarow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minarow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minarow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minarow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_discs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpiece\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                     \u001b[0mnum_windows\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":109},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom stable_baselines3 import PPO\n\n# âœ… Set device explicitly\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"âœ… Using device: {device}\")\n\n\nmodel = PPO.load(f\"/kaggle/input/ppo_connect4_selfplay_v8/pytorch/default/1/ppo_connect4_selfplay_v8.pkl\", device=device)\n\ndef self_play_opponent(obs, config):\n    obs_array = np.array(obs['board']).reshape(1, config.rows, config.columns)\n    action, _states = model.predict(obs_array)\n    \n    valid_moves = [c for c in range(config.columns) if obs_array[0, 0, c] == 0]  # Only pick from non-full columns\n    if action not in valid_moves:  # If chosen action is invalid, pick a valid one\n        action = random.choice(valid_moves)\n        \n    return int(action)\n\n# âœ… Train against previous self-play version\nenvSelfPlay = DymaicRewardConnectFour(agent2=\"random\")\n\npolicy_kwargs = dict(features_extractor_class=CustomResNetCNN)\n\n#new_model = PPO(\"CnnPolicy\", envSelfPlay, policy_kwargs=policy_kwargs, \n                #learning_rate=0.0003, gamma=0.995, clip_range=0.2, \n                #ent_coef=0.01, n_steps=2048, batch_size=32, verbose=1, device=device)\nnew_model = PPO(\"CnnPolicy\", envSelfPlay, policy_kwargs=policy_kwargs, \n                learning_rate=0.0003, verbose=0, device=device)\n\n#new_model.policy.load_state_dict(PPO.load(\"/kaggle/input/ppo_connect4_selfplay_v10_0209/pytorch/default/1/ppo_connect4_selfplay_v10_0209.pkl\", device=device).policy.state_dict())\n\n# âœ… Debug GPU Training with Small Steps First\n#print(f\"\\nğŸš€ Starting Training...Iteration {i}\")\nnew_model.learn(total_timesteps=60000)#, callback=stop_callback)  # ğŸ”¥ Start with a small number of steps\nnew_model.save(f\"ppo_connect4_DynReward_v{1}.pkl\")  # Save updated model\n\nselfPlay()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_environments import make, evaluate\n\n#new_model.save(f\"ppo_connect4_DynReward_v{1}.pkl\")  # Save updated model\n\nmodel = PPO.load(\"/kaggle/input/ppo_connect4_selfplay_v8/pytorch/default/1/ppo_connect4_selfplay_v8.pkl\", device=device)\ndef opponent(obs, config):\n    while True:\n        obs_array = np.array(obs['board']).reshape(1, config.rows, config.columns)\n        action, _states = model.predict(obs_array)\n        valid_moves = [c for c in range(config.columns) if obs_array[0, 0, c] == 0]  # Only pick from non-full columns\n        if action in valid_moves:\n            return int(action)\n\n\n#newmodel = PPO.load(\"/kaggle/working/ppo_connect4_DynReward_WR_0.85.pkl\")  # Load latest agent\n#newmodel = PPO.load(f\"ppo_connect4_DynReward_v{1}.pkl\")  # Load latest agent\ndef agent(obs, config):\n    newmodel = PPO.load(\"//kaggle/working/ppo_connect4_selfplay_v13.pkl\")  # Load latest agent\n    while True:\n        obs_array = np.array(obs['board']).reshape(1, config.rows, config.columns)\n        action, _states = newmodel.predict(obs_array)\n        valid_moves = [c for c in range(config.columns) if obs_array[0, 0, c] == 0]  # Only pick from non-full columns\n        if action in valid_moves:\n            return int(action)\n\n# Create the game environment\nenv = make(\"connectx\")\n\n# Two random agents play one game round negamax opponent\n#env.run([RL_agent, MinMaxAB_agent])  MinMaxAB_agent opponent\nenv.run([opponent, MinMaxAB_agent])\n\n# Show the game\nenv.render(mode=\"ipython\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T06:19:39.479163Z","iopub.execute_input":"2025-02-10T06:19:39.479444Z","iopub.status.idle":"2025-02-10T06:19:40.709265Z","shell.execute_reply.started":"2025-02-10T06:19:39.479423Z","shell.execute_reply":"2025-02-10T06:19:40.708441Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<iframe srcdoc=\"<!--\n  Copyright 2020 Kaggle Inc\n\n  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License.\n-->\n<!DOCTYPE html>\n<html lang=&quot;en&quot;>\n  <head>\n    <title>Kaggle Simulation Player</title>\n    <meta name=&quot;viewport&quot; content=&quot;width=device-width,initial-scale=1&quot; />\n    <link\n      rel=&quot;stylesheet&quot;\n      href=&quot;https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.css&quot;\n      crossorigin=&quot;anonymous&quot;\n    />\n    <style type=&quot;text/css&quot;>\n      html,\n      body {\n        height: 100%;\n        font-family: sans-serif;\n        margin: 0px;\n      }\n      canvas {\n        /* image-rendering: -moz-crisp-edges;\n        image-rendering: -webkit-crisp-edges;\n        image-rendering: pixelated;\n        image-rendering: crisp-edges; */\n      }\n    </style>\n    <script src=&quot;https://unpkg.com/preact@10.0.1/dist/preact.umd.js&quot;></script>\n    <script src=&quot;https://unpkg.com/preact@10.0.1/hooks/dist/hooks.umd.js&quot;></script>\n    <script src=&quot;https://unpkg.com/htm@2.2.1/dist/htm.umd.js&quot;></script>\n    <script src=&quot;https://unpkg.com/chess.js@0.12.0/chess.js&quot;></script>\n    <script>\n      // Polyfill for Styled Components\n      window.React = {\n        ...preact,\n        createElement: preact.h,\n        PropTypes: { func: {} },\n      };\n    </script>\n    <script src=&quot;https://unpkg.com/styled-components@3.5.0-0/dist/styled-components.min.js&quot;></script>\n  </head>\n  <body>\n    <script>\n      \nwindow.kaggle = {\n  &quot;debug&quot;: false,\n  &quot;playing&quot;: true,\n  &quot;step&quot;: 0,\n  &quot;controls&quot;: true,\n  &quot;environment&quot;: {\n    &quot;id&quot;: &quot;02c0ead4-e777-11ef-bd9c-0242ac130202&quot;,\n    &quot;name&quot;: &quot;connectx&quot;,\n    &quot;title&quot;: &quot;ConnectX&quot;,\n    &quot;description&quot;: &quot;Classic Connect in a row but configurable.&quot;,\n    &quot;version&quot;: &quot;1.0.1&quot;,\n    &quot;configuration&quot;: {\n      &quot;episodeSteps&quot;: 1000,\n      &quot;actTimeout&quot;: 2,\n      &quot;runTimeout&quot;: 1200,\n      &quot;columns&quot;: 7,\n      &quot;rows&quot;: 6,\n      &quot;inarow&quot;: 4,\n      &quot;agentTimeout&quot;: 60,\n      &quot;timeout&quot;: 2\n    },\n    &quot;specification&quot;: {\n      &quot;action&quot;: {\n        &quot;description&quot;: &quot;Column to drop a checker onto the board.&quot;,\n        &quot;type&quot;: &quot;integer&quot;,\n        &quot;minimum&quot;: 0,\n        &quot;default&quot;: 0\n      },\n      &quot;agents&quot;: [\n        2\n      ],\n      &quot;configuration&quot;: {\n        &quot;episodeSteps&quot;: {\n          &quot;description&quot;: &quot;Maximum number of steps in the episode.&quot;,\n          &quot;type&quot;: &quot;integer&quot;,\n          &quot;minimum&quot;: 1,\n          &quot;default&quot;: 1000\n        },\n        &quot;actTimeout&quot;: {\n          &quot;description&quot;: &quot;Maximum runtime (seconds) to obtain an action from an agent.&quot;,\n          &quot;type&quot;: &quot;number&quot;,\n          &quot;minimum&quot;: 0,\n          &quot;default&quot;: 2\n        },\n        &quot;runTimeout&quot;: {\n          &quot;description&quot;: &quot;Maximum runtime (seconds) of an episode (not necessarily DONE).&quot;,\n          &quot;type&quot;: &quot;number&quot;,\n          &quot;minimum&quot;: 0,\n          &quot;default&quot;: 1200\n        },\n        &quot;columns&quot;: {\n          &quot;description&quot;: &quot;The number of columns on the board&quot;,\n          &quot;type&quot;: &quot;integer&quot;,\n          &quot;default&quot;: 7,\n          &quot;minimum&quot;: 1\n        },\n        &quot;rows&quot;: {\n          &quot;description&quot;: &quot;The number of rows on the board&quot;,\n          &quot;type&quot;: &quot;integer&quot;,\n          &quot;default&quot;: 6,\n          &quot;minimum&quot;: 1\n        },\n        &quot;inarow&quot;: {\n          &quot;description&quot;: &quot;The number of checkers in a row required to win.&quot;,\n          &quot;type&quot;: &quot;integer&quot;,\n          &quot;default&quot;: 4,\n          &quot;minimum&quot;: 1\n        },\n        &quot;agentTimeout&quot;: {\n          &quot;description&quot;: &quot;Obsolete field kept for backwards compatibility, please use observation.remainingOverageTime.&quot;,\n          &quot;type&quot;: &quot;number&quot;,\n          &quot;minimum&quot;: 0,\n          &quot;default&quot;: 60\n        },\n        &quot;timeout&quot;: {\n          &quot;description&quot;: &quot;Obsolete copy of actTimeout maintained for backwards compatibility. May be removed in the future.&quot;,\n          &quot;type&quot;: &quot;integer&quot;,\n          &quot;default&quot;: 2,\n          &quot;minimum&quot;: 0\n        }\n      },\n      &quot;info&quot;: {},\n      &quot;observation&quot;: {\n        &quot;remainingOverageTime&quot;: {\n          &quot;description&quot;: &quot;Total remaining banked time (seconds) that can be used in excess of per-step actTimeouts -- agent is disqualified with TIMEOUT status when this drops below 0.&quot;,\n          &quot;shared&quot;: false,\n          &quot;type&quot;: &quot;number&quot;,\n          &quot;minimum&quot;: 0,\n          &quot;default&quot;: 60\n        },\n        &quot;step&quot;: {\n          &quot;description&quot;: &quot;Current step within the episode.&quot;,\n          &quot;type&quot;: &quot;integer&quot;,\n          &quot;shared&quot;: true,\n          &quot;minimum&quot;: 0,\n          &quot;default&quot;: 0\n        },\n        &quot;board&quot;: {\n          &quot;description&quot;: &quot;Serialized grid (rows x columns). 0 = Empty, 1 = P1, 2 = P2&quot;,\n          &quot;type&quot;: &quot;array&quot;,\n          &quot;shared&quot;: true,\n          &quot;default&quot;: []\n        },\n        &quot;mark&quot;: {\n          &quot;defaults&quot;: [\n            1,\n            2\n          ],\n          &quot;description&quot;: &quot;Which checkers are the agents.&quot;,\n          &quot;enum&quot;: [\n            1,\n            2\n          ]\n        }\n      },\n      &quot;reward&quot;: {\n        &quot;description&quot;: &quot;-1 = Lost, 0 = Draw/Ongoing, 1 = Won&quot;,\n        &quot;enum&quot;: [\n          -1,\n          0,\n          1\n        ],\n        &quot;default&quot;: 0,\n        &quot;type&quot;: [\n          &quot;number&quot;,\n          &quot;null&quot;\n        ]\n      }\n    },\n    &quot;steps&quot;: [\n      [\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 0,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 3,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 1,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              0,\n              0,\n              0\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 2,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              0,\n              0,\n              2\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 6,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 3,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 3,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              0,\n              0,\n              2\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 4,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              0,\n              0,\n              0,\n              2,\n              0,\n              0,\n              1,\n              0,\n              0,\n              2\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 3,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 5,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              0,\n              0,\n              0,\n              2,\n              0,\n              0,\n              1,\n              0,\n              0,\n              2\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 6,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              0,\n              0,\n              0,\n              2,\n              0,\n              0,\n              1,\n              0,\n              0,\n              2\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 3,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 2,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 7,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              0,\n              0,\n              0,\n              2,\n              0,\n              1,\n              1,\n              0,\n              0,\n              2\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 8,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              1,\n              0,\n              0,\n              0,\n              2,\n              0,\n              1,\n              1,\n              0,\n              0,\n              2\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 2,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 4,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 9,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              1,\n              0,\n              0,\n              0,\n              2,\n              0,\n              1,\n              1,\n              1,\n              0,\n              2\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 10,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              1,\n              0,\n              0,\n              0,\n              2,\n              2,\n              1,\n              1,\n              1,\n              0,\n              2\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 1,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 3,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 11,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              1,\n              0,\n              0,\n              0,\n              2,\n              2,\n              1,\n              1,\n              1,\n              0,\n              2\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 12,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              1,\n              0,\n              0,\n              0,\n              2,\n              2,\n              1,\n              1,\n              1,\n              2,\n              2\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 5,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 2,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 13,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              1,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              1,\n              0,\n              0,\n              0,\n              2,\n              2,\n              1,\n              1,\n              1,\n              2,\n              2\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 14,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              1,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              1,\n              0,\n              2,\n              0,\n              2,\n              2,\n              1,\n              1,\n              1,\n              2,\n              2\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 5,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 4,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 15,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              1,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              1,\n              1,\n              2,\n              0,\n              2,\n              2,\n              1,\n              1,\n              1,\n              2,\n              2\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;INACTIVE&quot;\n        },\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: 0,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;ACTIVE&quot;\n        }\n      ],\n      [\n        {\n          &quot;action&quot;: 0,\n          &quot;reward&quot;: -1,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;step&quot;: 16,\n            &quot;board&quot;: [\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              0,\n              0,\n              0,\n              0,\n              0,\n              0,\n              2,\n              0,\n              0,\n              0,\n              0,\n              0,\n              1,\n              1,\n              2,\n              0,\n              0,\n              0,\n              0,\n              2,\n              1,\n              1,\n              2,\n              0,\n              2,\n              2,\n              1,\n              1,\n              1,\n              2,\n              2\n            ],\n            &quot;mark&quot;: 1\n          },\n          &quot;status&quot;: &quot;DONE&quot;\n        },\n        {\n          &quot;action&quot;: 4,\n          &quot;reward&quot;: 1,\n          &quot;info&quot;: {},\n          &quot;observation&quot;: {\n            &quot;remainingOverageTime&quot;: 60,\n            &quot;mark&quot;: 2\n          },\n          &quot;status&quot;: &quot;DONE&quot;\n        }\n      ]\n    ],\n    &quot;rewards&quot;: [\n      -1,\n      1\n    ],\n    &quot;statuses&quot;: [\n      &quot;DONE&quot;,\n      &quot;DONE&quot;\n    ],\n    &quot;schema_version&quot;: 1,\n    &quot;info&quot;: {}\n  },\n  &quot;logs&quot;: [\n    [],\n    [],\n    [\n      {\n        &quot;duration&quot;: 0.004952,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      },\n      {}\n    ],\n    [\n      {},\n      {\n        &quot;duration&quot;: 0.010674,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      }\n    ],\n    [\n      {\n        &quot;duration&quot;: 0.003793,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      },\n      {}\n    ],\n    [\n      {},\n      {\n        &quot;duration&quot;: 0.009463,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      }\n    ],\n    [\n      {\n        &quot;duration&quot;: 0.00334,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      },\n      {}\n    ],\n    [\n      {},\n      {\n        &quot;duration&quot;: 0.009597,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      }\n    ],\n    [\n      {\n        &quot;duration&quot;: 0.003227,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      },\n      {}\n    ],\n    [\n      {},\n      {\n        &quot;duration&quot;: 0.009224,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      }\n    ],\n    [\n      {\n        &quot;duration&quot;: 0.003308,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      },\n      {}\n    ],\n    [\n      {},\n      {\n        &quot;duration&quot;: 0.010341,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      }\n    ],\n    [\n      {\n        &quot;duration&quot;: 0.003072,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      },\n      {}\n    ],\n    [\n      {},\n      {\n        &quot;duration&quot;: 0.009383,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      }\n    ],\n    [\n      {\n        &quot;duration&quot;: 0.003591,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      },\n      {}\n    ],\n    [\n      {},\n      {\n        &quot;duration&quot;: 0.009538,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      }\n    ],\n    [\n      {\n        &quot;duration&quot;: 0.002998,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      },\n      {}\n    ],\n    [\n      {},\n      {\n        &quot;duration&quot;: 0.009931,\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stderr&quot;: &quot;&quot;\n      }\n    ]\n  ],\n  &quot;mode&quot;: &quot;ipython&quot;\n};\n\n\nwindow.kaggle.renderer = // Copyright 2020 Kaggle Inc\n//\n// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\nfunction renderer({\n  act,\n  agents,\n  environment,\n  frame,\n  height = 400,\n  interactive,\n  isInteractive,\n  parent,\n  step,\n  update,\n  width = 400,\n}) {\n  // Configuration.\n  const { rows, columns, inarow } = environment.configuration;\n\n  // Common Dimensions.\n  const unit = 8;\n  const minCanvasSize = Math.min(height, width);\n  const minOffset = minCanvasSize > 400 ? 30 : unit / 2;\n  const cellSize = Math.min(\n    (width - minOffset * 2) / columns,\n    (height - minOffset * 2) / rows\n  );\n  const cellInset = 0.8;\n  const pieceScale = cellSize / 100;\n  const xOffset = Math.max(0, (width - cellSize * columns) / 2);\n  const yOffset = Math.max(0, (height - cellSize * rows) / 2);\n\n  // Canvas Setup.\n  let canvas = parent.querySelector(&quot;canvas&quot;);\n  if (!canvas) {\n    canvas = document.createElement(&quot;canvas&quot;);\n    parent.appendChild(canvas);\n\n    if (interactive) {\n      canvas.addEventListener(&quot;click&quot;, evt => {\n        if (!isInteractive()) return;\n        const rect = evt.target.getBoundingClientRect();\n        const col = Math.floor((evt.clientX - rect.left - xOffset) / cellSize);\n        if (col >= 0 && col < columns) act(col);\n      });\n    }\n  }\n  canvas.style.cursor = isInteractive() ? &quot;pointer&quot; : &quot;default&quot;;\n\n  // Character Paths (based on 100x100 tiles).\n  const kPath = new Path2D(\n    `M78.3,96.5c-0.1,0.4-0.5,0.6-1.1,0.6H64.9c-0.7,0-1.4-0.3-1.9-1l-20.3-26L37,75.5v20.1 c0,0.9-0.5,1.4-1.4,1.4H26c-0.9,0-1.4-0.5-1.4-1.4V3.9c0-0.9,0.5-1.4,1.4-1.4h9.5C36.5,2.5,37,3,37,3.9v56.5l24.3-24.7 c0.6-0.6,1.3-1,1.9-1H76c0.6,0,0.9,0.2,1.1,0.7c0.2,0.6,0.1,1-0.1,1.2l-25.7,25L78,95.1C78.4,95.5,78.5,95.9,78.3,96.5z`\n  );\n  const goose1Path = new Path2D(\n    `M8.8,92.7c-4-18.5,4.7-37.2,20.7-46.2c0,0,2.7-1.4,3.4-1.9c2.2-1.6,3-2.1,3-5c0-5-2.1-7.2-2.1-7.2 c-3.9-3.3-6.3-8.2-6.3-13.7c0-10,8.1-18.1,18.1-18.1s18.1,8.1,18.1,18.1c0,6-1.5,32.7-2.3,38.8l-0.1,1`\n  );\n  const goose2Path = new Path2D(\n    `M27.4,19L8.2,27.6c0,0-7.3,2.9,2.6,5c6.1,1.3,24,5.9,24,5.9l1,0.3`\n  );\n  const goose3Path = new Path2D(\n    `M63.7,99.6C52.3,99.6,43,90.3,43,78.9s9.3-20.7,20.7-20.7c10.6,0,34.4,0.1,35.8,9`\n  );\n\n  // Canvas setup and reset.\n  let c = canvas.getContext(&quot;2d&quot;);\n  canvas.width = width;\n  canvas.height = height;\n  c.fillStyle = &quot;#000B2A&quot;;\n  c.fillRect(0, 0, canvas.width, canvas.height);\n\n  const getRowCol = cell => [Math.floor(cell / columns), cell % columns];\n\n  const getColor = (mark, opacity = 1) => {\n    if (mark === 1) return `rgba(0,255,255,${opacity})`;\n    if (mark === 2) return `rgba(255,255,255,${opacity})`;\n    return &quot;#fff&quot;;\n  };\n\n  const drawCellCircle = (cell, xFrame = 1, yFrame = 1, radiusOffset = 0) => {\n    const [row, col] = getRowCol(cell);\n    c.arc(\n      xOffset + xFrame * (col * cellSize + cellSize / 2),\n      yOffset + yFrame * (row * cellSize + cellSize / 2),\n      (cellInset * cellSize) / 2 - radiusOffset,\n      2 * Math.PI,\n      false\n    );\n  };\n\n  // Render the pieces.\n  const board = environment.steps[step][0].observation.board;\n\n  const drawPiece = mark => {\n    // Base Styles.\n    const opacity = minCanvasSize < 300 ? 0.6 - minCanvasSize / 1000 : 0.1;\n    c.fillStyle = getColor(mark, opacity);\n    c.strokeStyle = getColor(mark);\n    c.shadowColor = getColor(mark);\n    c.shadowBlur = 8 / cellInset;\n    c.lineWidth = 1 / cellInset;\n\n    // Outer circle.\n    c.save();\n    c.beginPath();\n    c.arc(50, 50, 50, 2 * Math.PI, false);\n    c.closePath();\n    c.lineWidth *= 4;\n    c.stroke();\n    c.fill();\n    c.restore();\n\n    // Inner circle.\n    c.beginPath();\n    c.arc(50, 50, 40, 2 * Math.PI, false);\n    c.closePath();\n    c.stroke();\n\n    // Kaggle &quot;K&quot;.\n    if (mark === 1) {\n      const scale = 0.54;\n      c.save();\n      c.translate(23, 23);\n      c.scale(scale, scale);\n      c.lineWidth /= scale;\n      c.shadowBlur /= scale;\n      c.stroke(kPath);\n      c.restore();\n    }\n\n    // Kaggle &quot;Goose&quot;.\n    if (mark === 2) {\n      const scale = 0.6;\n      c.save();\n      c.translate(24, 28);\n      c.scale(scale, scale);\n      c.lineWidth /= scale;\n      c.shadowBlur /= scale;\n      c.stroke(goose1Path);\n      c.stroke(goose2Path);\n      c.stroke(goose3Path);\n      c.beginPath();\n      c.arc(38.5, 18.6, 2.7, 0, Math.PI * 2, false);\n      c.closePath();\n      c.fill();\n      c.restore();\n    }\n  };\n\n  for (let i = 0; i < board.length; i++) {\n    const [row, col] = getRowCol(i);\n    if (board[i] === 0) continue;\n    // Easing In.\n    let yFrame = Math.min(\n      (columns * Math.pow(frame, 3)) / Math.floor(i / columns),\n      1\n    );\n\n    if (\n      step > 1 &&\n      environment.steps[step - 1][0].observation.board[i] === board[i]\n    ) {\n      yFrame = 1;\n    }\n\n    c.save();\n    c.translate(\n      xOffset + cellSize * col + (cellSize - cellSize * cellInset) / 2,\n      yOffset +\n        yFrame * (cellSize * row) +\n        (cellSize - cellSize * cellInset) / 2\n    );\n    c.scale(pieceScale * cellInset, pieceScale * cellInset);\n    drawPiece(board[i]);\n    c.restore();\n  }\n\n  // Background Gradient.\n  const bgRadius = (Math.min(rows, columns) * cellSize) / 2;\n  const bgStyle = c.createRadialGradient(\n    xOffset + (cellSize * columns) / 2,\n    yOffset + (cellSize * rows) / 2,\n    0,\n    xOffset + (cellSize * columns) / 2,\n    yOffset + (cellSize * rows) / 2,\n    bgRadius\n  );\n  bgStyle.addColorStop(0, &quot;#000B49&quot;);\n  bgStyle.addColorStop(1, &quot;#000B2A&quot;);\n\n  // Render the board overlay.\n  c.beginPath();\n  c.rect(0, 0, canvas.width, canvas.height);\n  c.closePath();\n  c.shadowBlur = 0;\n  for (let i = 0; i < board.length; i++) {\n    drawCellCircle(i);\n    c.closePath();\n  }\n  c.fillStyle = bgStyle;\n  c.fill(&quot;evenodd&quot;);\n\n  // Render the board overlay cell outlines.\n  for (let i = 0; i < board.length; i++) {\n    c.beginPath();\n    drawCellCircle(i);\n    c.strokeStyle = &quot;#0361B2&quot;;\n    c.lineWidth = 1;\n    c.stroke();\n    c.closePath();\n  }\n\n  const drawLine = (fromCell, toCell) => {\n    if (frame < 0.5) return;\n    const lineFrame = (frame - 0.5) / 0.5;\n    const x1 = xOffset + (fromCell % columns) * cellSize + cellSize / 2;\n    const x2 =\n      x1 +\n      lineFrame *\n        (xOffset + ((toCell % columns) * cellSize + cellSize / 2) - x1);\n    const y1 =\n      yOffset + Math.floor(fromCell / columns) * cellSize + cellSize / 2;\n    const y2 =\n      y1 +\n      lineFrame *\n        (yOffset + Math.floor(toCell / columns) * cellSize + cellSize / 2 - y1);\n    c.beginPath();\n    c.lineCap = &quot;round&quot;;\n    c.lineWidth = 4;\n    c.strokeStyle = getColor(board[fromCell]);\n    c.shadowBlur = 8;\n    c.shadowColor = getColor(board[fromCell]);\n    c.moveTo(x1, y1);\n    c.lineTo(x2, y2);\n    c.stroke();\n  };\n\n  // Generate a graph of the board.\n  const getCell = (cell, rowOffset, columnOffset) => {\n    const row = Math.floor(cell / columns) + rowOffset;\n    const col = (cell % columns) + columnOffset;\n    if (row < 0 || row >= rows || col < 0 || col >= columns) return -1;\n    return col + row * columns;\n  };\n  const makeNode = cell => {\n    const node = { cell, directions: [], value: board[cell] };\n    for (let r = -1; r <= 1; r++) {\n      for (let c = -1; c <= 1; c++) {\n        if (r === 0 && c === 0) continue;\n        node.directions.push(getCell(cell, r, c));\n      }\n    }\n    return node;\n  };\n  const graph = board.map((_, i) => makeNode(i));\n\n  // Check for any wins!\n  const getSequence = (node, direction) => {\n    const sequence = [node.cell];\n    while (sequence.length < inarow) {\n      const next = graph[node.directions[direction]];\n      if (!next || node.value !== next.value || next.value === 0) return;\n      node = next;\n      sequence.push(node.cell);\n    }\n    return sequence;\n  };\n\n  // Check all nodes.\n  for (let i = 0; i < board.length; i++) {\n    // Check all directions (not the most efficient).\n    for (let d = 0; d < 8; d++) {\n      const seq = getSequence(graph[i], d);\n      if (seq) {\n        drawLine(seq[0], seq[inarow - 1]);\n        i = board.length;\n        break;\n      }\n    }\n  }\n\n  // Upgrade the legend.\n  if (agents.length && (!agents[0].color || !agents[0].image)) {\n    const getPieceImage = mark => {\n      const pieceCanvas = document.createElement(&quot;canvas&quot;);\n      parent.appendChild(pieceCanvas);\n      pieceCanvas.style.marginLeft = &quot;10000px&quot;;\n      pieceCanvas.width = 100;\n      pieceCanvas.height = 100;\n      c = pieceCanvas.getContext(&quot;2d&quot;);\n      c.translate(10, 10);\n      c.scale(0.8, 0.8);\n      drawPiece(mark);\n      const dataUrl = pieceCanvas.toDataURL();\n      parent.removeChild(pieceCanvas);\n      return dataUrl;\n    };\n\n    agents.forEach(agent => {\n      agent.color = getColor(agent.index + 1);\n      agent.image = getPieceImage(agent.index + 1);\n    });\n    update({ agents });\n  }\n};\n\n\n    \n    </script>\n    <script>\n      const h = htm.bind(preact.h);\n      const { useContext, useEffect, useRef, useState } = preactHooks;\n      const styled = window.styled.default;\n\n      const Context = preact.createContext({});\n\n      const Loading = styled.div`\n        animation: rotate360 1.1s infinite linear;\n        border: 8px solid rgba(255, 255, 255, 0.2);\n        border-left-color: #0cb1ed;\n        border-radius: 50%;\n        height: 40px;\n        position: relative;\n        transform: translateZ(0);\n        width: 40px;\n\n        @keyframes rotate360 {\n          0% {\n            transform: rotate(0deg);\n          }\n          100% {\n            transform: rotate(360deg);\n          }\n        }\n      `;\n\n      const Logo = styled(\n        (props) => h`\n        <a href=&quot;https://kaggle.com&quot; target=&quot;_blank&quot; className=${props.className}>\n          <svg width=&quot;62px&quot; height=&quot;20px&quot; viewBox=&quot;0 0 62 24&quot; version=&quot;1.1&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot;>\n            <g fill=&quot;#1EBEFF&quot; fill-rule=&quot;nonzero&quot;>\n              <path d=&quot;M10.2,17.8c0,0.1-0.1,0.1-0.2,0.1H7.7c-0.1,0-0.3-0.1-0.4-0.2l-3.8-4.9l-1.1,1v3.8 c0,0.2-0.1,0.3-0.3,0.3H0.3c-0.2,0-0.3-0.1-0.3-0.3V0.3C0.1,0.1,0.2,0,0.3,0h1.8c0.2,0,0.3,0.1,0.3,0.3V11L7,6.3 c0.1-0.1,0.2-0.2,0.4-0.2h2.4c0.1,0,0.2,0,0.2,0.1c0,0.1,0,0.2,0,0.2l-4.9,4.7l5.1,6.3C10.2,17.6,10.2,17.7,10.2,17.8z&quot;/>\n              <path d=&quot;M19.6,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3v-0.4c-0.8,0.6-1.8,0.9-3,0.9c-1.1,0-2-0.3-2.8-1 c-0.8-0.7-1.2-1.6-1.2-2.7c0-1.7,1.1-2.9,3.2-3.5c0.8-0.2,2.1-0.5,3.8-0.6c0.1-0.6-0.1-1.2-0.5-1.7c-0.4-0.5-1-0.7-1.7-0.7 c-1,0-2,0.4-3,1C12.2,9.1,12.1,9.1,12,9l-0.9-1.3C11,7.5,11,7.4,11.1,7.3c1.3-0.9,2.7-1.4,4.2-1.4c1.1,0,2.1,0.3,2.8,0.8 c1.1,0.8,1.7,2,1.7,3.7v7.3C19.9,17.8,19.8,17.9,19.6,17.9z M17.5,12.4c-1.7,0.2-2.9,0.4-3.5,0.7c-0.9,0.4-1.2,0.9-1.1,1.6 c0.1,0.4,0.2,0.7,0.6,0.9c0.3,0.2,0.7,0.4,1.1,0.4c1.2,0.1,2.2-0.2,2.9-1V12.4z&quot;/>\n              <path d=&quot;M30.6,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3v11.7C32,20,31.5,21.5,30.6,22.5z M29.7,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7V9.9z&quot;/>\n              <path d=&quot;M42.9,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3H44c0.2,0,0.3,0.1,0.3,0.3v11.7C44.3,20,43.8,21.5,42.9,22.5z M42,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7L42,9.9 L42,9.9z&quot;/>\n              <path d=&quot;M48.3,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3V0.3c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3 v17.3C48.5,17.8,48.5,17.9,48.3,17.9z&quot;/>\n              <path d=&quot;M61.4,12.6c0,0.2-0.1,0.3-0.3,0.3h-8.5c0.1,0.9,0.5,1.6,1.1,2.2c0.7,0.6,1.6,0.9,2.7,0.9 c1,0,1.8-0.3,2.6-0.8c0.2-0.1,0.3-0.1,0.4,0l1.2,1.3c0.1,0.1,0.1,0.3,0,0.4c-1.3,0.9-2.7,1.4-4.4,1.4c-1.8,0-3.3-0.6-4.4-1.8 c-1.1-1.2-1.7-2.7-1.7-4.5c0-1.7,0.6-3.2,1.7-4.4c1-1.1,2.4-1.6,4.1-1.6c1.6,0,2.9,0.6,4,1.7c1.1,1.2,1.6,2.6,1.5,4.4L61.4,12.6 z M58,8.7c-0.6-0.5-1.3-0.8-2.1-0.8c-0.8,0-1.5,0.3-2.1,0.8c-0.6,0.5-1,1.2-1.1,2H59C59,9.9,58.6,9.3,58,8.7z&quot;/>\n            </g>\n          </svg>\n        </a>\n      `\n      )`\n        display: inline-flex;\n      `;\n\n      const Header = styled((props) => {\n        const { environment } = useContext(Context);\n\n        return h`<div className=${props.className} >\n          <${Logo} />\n          <span><b>Left / Right Arrow:</b> Increase / Decrease Step</span><span><b>0-9 Row Keys:</b> Playback Speed</span><span><b>Space:</b> Pause / Play</span>\n          ${environment.title}\n        </div>`;\n      })`\n        align-items: center;\n        border-bottom: 4px solid #212121;\n        box-sizing: border-box;\n        color: #fff;\n        display: flex;\n        flex: 0 0 36px;\n        font-size: 14px;\n        justify-content: space-between;\n        padding: 0 8px;\n        width: 100%;\n      `;\n\n      const Renderer = styled((props) => {\n        const context = useContext(Context);\n        const { animate, debug, playing, renderer, speed } = context;\n        const ref = preact.createRef();\n\n        useEffect(async () => {\n          if (!ref.current) return;\n\n          const renderFrame = async (start, step, lastFrame) => {\n            if (step !== context.step) return;\n            if (lastFrame === 1) {\n              if (!animate) return;\n              start = Date.now();\n            }\n            const frame =\n              playing || animate\n                ? Math.min((Date.now() - start) / speed, 1)\n                : 1;\n            try {\n              if (debug) console.time(&quot;render&quot;);\n              await renderer({\n                ...context,\n                frame,\n                height: ref.current.clientHeight,\n                hooks: preactHooks,\n                parent: ref.current,\n                preact,\n                styled,\n                width: ref.current.clientWidth,\n              });\n            } catch (error) {\n              if (debug) console.error(error);\n              console.log({ ...context, frame, error });\n            } finally {\n              if (debug) console.timeEnd(&quot;render&quot;);\n            }\n            window.requestAnimationFrame(() => renderFrame(start, step, frame));\n          };\n\n          await renderFrame(Date.now(), context.step);\n        }, [ref.current, context.step, context.renderer]);\n\n        return h`<div className=${props.className} ref=${ref} />`;\n      })`\n        align-items: center;\n        box-sizing: border-box;\n        display: flex;\n        height: 100%;\n        left: 0;\n        justify-content: center;\n        position: absolute;\n        top: 0;\n        width: 100%;\n      `;\n\n      const Processing = styled((props) => {\n        const { processing } = useContext(Context);\n        const text = processing === true ? &quot;Processing...&quot; : processing;\n        return h`<div className=${props.className}>${text}</div>`;\n      })`\n        bottom: 0;\n        color: #fff;\n        font-size: 12px;\n        left: 0;\n        line-height: 24px;\n        position: absolute;\n        text-align: center;\n        width: 100%;\n      `;\n\n      const Viewer = styled((props) => {\n        const { processing } = useContext(Context);\n        return h`<div className=${props.className}>\n          <${Renderer} />\n          ${processing && h`<${Processing} />`}\n        </div>`;\n      })`\n        background-color: #000b2a;\n        background-image: radial-gradient(\n          circle closest-side,\n          #000b49,\n          #000b2a\n        );\n        display: flex;\n        flex: 1;\n        overflow: hidden;\n        position: relative;\n        width: 100%;\n      `;\n\n      // Partitions the elements of arr into subarrays of max length num.\n      const groupIntoSets = (arr, num) => {\n        const sets = [];\n        arr.forEach(a => {\n          if (sets.length === 0 || sets[sets.length - 1].length === num) {\n            sets.push([]);\n          }\n          sets[sets.length - 1].push(a);\n        });\n        return sets;\n      }\n\n      // Expects `width` input prop to set proper max-width for agent name span.\n      const Legend = styled((props) => {\n        const { agents, legend } = useContext(Context);\n\n        const agentPairs = groupIntoSets(agents.sort((a, b) => a.index - b.index), 2);\n\n        return h`<div className=${props.className}>\n          ${agentPairs.map(agentList =>\n            h`<ul>\n                ${agentList.map(a =>\n                  h`<li key=${a.id} title=&quot;id: ${a.id}&quot; style=&quot;color:${a.color || &quot;#FFF&quot;}&quot;>\n                      ${a.image && h`<img src=${a.image} />`}\n                      <span>${a.name}</span>\n                    </li>`\n                )}\n              </ul>`)}\n        </div>`;\n      })`\n        background-color: #000b2a;\n        font-family: sans-serif;\n        font-size: 14px;\n        height: 48px;\n        width: 100%;\n\n        ul {\n          align-items: center;\n          display: flex;\n          flex-direction: row;\n          justify-content: center;\n        }\n\n        li {\n          align-items: center;\n          display: inline-flex;\n          transition: color 1s;\n        }\n\n        span {\n          max-width: ${p => (p.width || 400) * 0.5 - 36}px;\n          overflow: hidden;\n          text-overflow: ellipsis;\n          white-space: nowrap;\n        }\n\n        img {\n          height: 24px;\n          margin-left: 4px;\n          margin-right: 4px;\n          width: 24px;\n        }\n      `;\n\n      const StepInput = styled.input.attrs({\n        type: &quot;range&quot;,\n      })`\n        appearance: none;\n        background: rgba(255, 255, 255, 0.15);\n        border-radius: 2px;\n        display: block;\n        flex: 1;\n        height: 4px;\n        opacity: 0.8;\n        outline: none;\n        transition: opacity 0.2s;\n        width: 100%;\n\n        &:hover {\n          opacity: 1;\n        }\n\n        &::-webkit-slider-thumb {\n          appearance: none;\n          background: #1ebeff;\n          border-radius: 100%;\n          cursor: pointer;\n          height: 12px;\n          margin: 0;\n          position: relative;\n          width: 12px;\n\n          &::after {\n            content: &quot;&quot;;\n            position: absolute;\n            top: 0px;\n            left: 0px;\n            width: 200px;\n            height: 8px;\n            background: green;\n          }\n        }\n      `;\n\n      const PlayButton = styled.button`\n        align-items: center;\n        background: none;\n        border: none;\n        color: white;\n        cursor: pointer;\n        display: flex;\n        flex: 0 0 56px;\n        font-size: 20px;\n        height: 40px;\n        justify-content: center;\n        opacity: 0.8;\n        outline: none;\n        transition: opacity 0.2s;\n\n        &:hover {\n          opacity: 1;\n        }\n      `;\n\n      const StepCount = styled.span`\n        align-items: center;\n        color: white;\n        display: flex;\n        font-size: 14px;\n        justify-content: center;\n        opacity: 0.8;\n        padding: 0 16px;\n        pointer-events: none;\n      `;\n\n      const Controls = styled((props) => {\n        const { environment, pause, play, playing, setStep, step } = useContext(\n          Context\n        );\n        const value = step + 1;\n        const onClick = () => (playing ? pause() : play());\n        const onInput = (e) => {\n          pause();\n          setStep(parseInt(e.target.value) - 1);\n        };\n\n        return h`\n          <div className=${props.className}>\n            <${PlayButton} onClick=${onClick}><svg xmlns=&quot;http://www.w3.org/2000/svg&quot; width=&quot;24px&quot; height=&quot;24px&quot; viewBox=&quot;0 0 24 24&quot; fill=&quot;#FFFFFF&quot;>${\n          playing\n            ? h`<path d=&quot;M6 19h4V5H6v14zm8-14v14h4V5h-4z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n            : h`<path d=&quot;M8 5v14l11-7z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n        }</svg><//>\n            <${StepInput} min=&quot;1&quot; max=${\n          environment.steps.length\n        } value=&quot;${value}&quot; onInput=${onInput} />\n            <${StepCount}>${value} / ${environment.steps.length}<//>\n          </div>\n        `;\n      })`\n        align-items: center;\n        border-top: 4px solid #212121;\n        display: flex;\n        flex: 0 0 44px;\n        width: 100%;\n      `;\n\n      const Info = styled((props) => {\n        const {\n          environment,\n          playing,\n          step,\n          speed,\n          animate,\n          header,\n          controls,\n          settings,\n        } = useContext(Context);\n\n        return h`\n          <div className=${props.className}>\n            info:\n            step(${step}),\n            playing(${playing ? &quot;T&quot; : &quot;F&quot;}),\n            speed(${speed}),\n            animate(${animate ? &quot;T&quot; : &quot;F&quot;})\n          </div>`;\n      })`\n        color: #888;\n        font-family: monospace;\n        font-size: 12px;\n      `;\n\n      const Settings = styled((props) => {\n        const { environment, pause, play, playing, setStep, step } = useContext(\n          Context\n        );\n\n        return h`\n          <div className=${props.className}>\n            <${Info} />\n          </div>\n        `;\n      })`\n        background: #fff;\n        border-top: 4px solid #212121;\n        box-sizing: border-box;\n        padding: 20px;\n        width: 100%;\n\n        h1 {\n          font-size: 20px;\n        }\n      `;\n\n      const Player = styled((props) => {\n        const context = useContext(Context);\n        const { agents, controls, header, legend, loading, settings, width } = context;\n        return h`\n          <div className=${props.className}>\n            ${loading && h`<${Loading} />`}\n            ${!loading && header && h`<${Header} />`}\n            ${!loading && h`<${Viewer} />`}\n            ${!loading && legend && h`<${Legend} width=${width}/>`}\n            ${!loading && controls && h`<${Controls} />`}\n            ${!loading && settings && h`<${Settings} />`}\n          </div>`;\n      })`\n        align-items: center;\n        background: #212121;\n        border: 4px solid #212121;\n        box-sizing: border-box;\n        display: flex;\n        flex-direction: column;\n        height: 100%;\n        justify-content: center;\n        position: relative;\n        width: 100%;\n      `;\n\n      const App = () => {\n        const renderCountRef = useRef(0);\n        const [_, setRenderCount] = useState(0);\n\n        // These are bindings to the 0-9 keys and are milliseconds of timeout per step\n        const speeds = [\n          0,\n          3000,\n          1000,\n          500,\n          333, // Default\n          200,\n          100,\n          50,\n          25,\n          10,\n        ];\n\n        const contextRef = useRef({\n          animate: false,\n          agents: [],\n          controls: false,\n          debug: false,\n          environment: { steps: [], info: {} },\n          header: window.innerHeight >= 600,\n          height: window.innerHeight,\n          interactive: false,\n          legend: true,\n          loading: false,\n          playing: false,\n          processing: false,\n          renderer: () => &quot;DNE&quot;,\n          settings: false,\n          speed: speeds[4],\n          step: 0,\n          width: window.innerWidth,\n        });\n\n        // Context helpers.\n        const rerender = (contextRef.current.rerender = () =>\n          setRenderCount((renderCountRef.current += 1)));\n        const setStep = (contextRef.current.setStep = (newStep) => {\n          contextRef.current.step = newStep;\n          rerender();\n        });\n        const setPlaying = (contextRef.current.setPlaying = (playing) => {\n          contextRef.current.playing = playing;\n          rerender();\n        });\n        const pause = (contextRef.current.pause = () => setPlaying(false));\n\n        const playNext = () => {\n          const context = contextRef.current;\n\n          if (\n            context.playing &&\n            context.step < context.environment.steps.length - 1\n          ) {\n            setStep(context.step + 1);\n            play(true);\n          } else {\n            pause();\n          }\n        };\n\n        const play = (contextRef.current.play = (continuing) => {\n          const context = contextRef.current;\n          if (context.playing && !continuing) return;\n          if (!context.playing) setPlaying(true);\n          if (\n            !continuing &&\n            context.step === context.environment.steps.length - 1\n          ) {\n            setStep(0);\n          }\n          setTimeout(playNext, context.speed);\n        });\n\n        const updateContext = (o) => {\n          const context = contextRef.current;\n          Object.assign(context, o, {\n            environment: { ...context.environment, ...(o.environment || {}) },\n          });\n          rerender();\n        };\n\n        // First time setup.\n        useEffect(() => {\n          // Timeout is used to ensure useEffect renders once.\n          setTimeout(() => {\n            // Initialize context with window.kaggle.\n            updateContext(window.kaggle || {});\n\n            if (window.kaggle.playing) {\n                play(true);\n            }\n\n            // Listen for messages received to update the context.\n            window.addEventListener(\n              &quot;message&quot;,\n              (event) => {\n                // Ensure the environment names match before updating.\n                try {\n                  if (\n                    event.data.environment.name ==\n                    contextRef.current.environment.name\n                  ) {\n                    updateContext(event.data);\n                  }\n                } catch {}\n              },\n              false\n            );\n            // Listen for keyboard commands.\n            window.addEventListener(\n              &quot;keydown&quot;,\n              (event) => {\n                const {\n                  interactive,\n                  isInteractive,\n                  playing,\n                  step,\n                  environment,\n                } = contextRef.current;\n                const key = event.keyCode;\n                const zero_key = 48\n                const nine_key = 57\n                if (\n                  interactive ||\n                  isInteractive() ||\n                  (key !== 32 && key !== 37 && key !== 39 && !(key >= zero_key && key <= nine_key))\n                )\n                  return;\n\n                if (key === 32) {\n                  playing ? pause() : play();\n                } else if (key === 39) {\n                  contextRef.current.playing = false;\n                  if (step < environment.steps.length - 1) setStep(step + 1);\n                  rerender();\n                } else if (key === 37) {\n                  contextRef.current.playing = false;\n                  if (step > 0) setStep(step - 1);\n                  rerender();\n                } else if (key >= zero_key && key <= nine_key) {\n                  contextRef.current.speed = speeds[key - zero_key];\n                }\n                event.preventDefault();\n                return false;\n              },\n              false\n            );\n          }, 1);\n        }, []);\n\n        if (contextRef.current.debug) {\n          console.log(&quot;context&quot;, contextRef.current);\n        }\n\n        // Ability to update context.\n        contextRef.current.update = updateContext;\n\n        // Ability to communicate with ipython.\n        const execute = (contextRef.current.execute = (source) =>\n          new Promise((resolve, reject) => {\n            try {\n              window.parent.IPython.notebook.kernel.execute(source, {\n                iopub: {\n                  output: (resp) => {\n                    const type = resp.msg_type;\n                    if (type === &quot;stream&quot;) return resolve(resp.content.text);\n                    if (type === &quot;error&quot;) return reject(new Error(resp.evalue));\n                    return reject(new Error(&quot;Unknown message type: &quot; + type));\n                  },\n                },\n              });\n            } catch (e) {\n              reject(new Error(&quot;IPython Unavailable: &quot; + e));\n            }\n          }));\n\n        // Ability to return an action from an interactive session.\n        contextRef.current.act = (action) => {\n          const id = contextRef.current.environment.id;\n          updateContext({ processing: true });\n          execute(`\n            import json\n            from kaggle_environments import interactives\n            if &quot;${id}&quot; in interactives:\n                action = json.loads('${JSON.stringify(action)}')\n                env, trainer = interactives[&quot;${id}&quot;]\n                trainer.step(action)\n                print(json.dumps(env.steps))`)\n            .then((resp) => {\n              try {\n                updateContext({\n                  processing: false,\n                  environment: { steps: JSON.parse(resp) },\n                });\n                play();\n              } catch (e) {\n                updateContext({ processing: resp.split(&quot;\\n&quot;)[0] });\n                console.error(resp, e);\n              }\n            })\n            .catch((e) => console.error(e));\n        };\n\n        // Check if currently interactive.\n        contextRef.current.isInteractive = () => {\n          const context = contextRef.current;\n          const steps = context.environment.steps;\n          return (\n            context.interactive &&\n            !context.processing &&\n            context.step === steps.length - 1 &&\n            steps[context.step].some((s) => s.status === &quot;ACTIVE&quot;)\n          );\n        };\n\n        return h`\n          <${Context.Provider} value=${contextRef.current}>\n            <${Player} />\n          <//>`;\n      };\n\n      preact.render(h`<${App} />`, document.body);\n    </script>\n  </body>\n</html>\n\" width=\"300\" height=\"300\" frameborder=\"0\"></iframe> "},"metadata":{}}],"execution_count":139},{"cell_type":"code","source":"def get_win_percentages(agent1, agent2, n_rounds=100):\n    # Use default Connect Four setup\n    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n    # Agent 1 goes first (roughly) half the time          \n    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n    # Agent 2 goes first (roughly) half the time      \n    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))\n\n\n#agent\nget_win_percentages(agent1=agent, agent2=opponent  , n_rounds=10)  #\"random\"  \"random\" opponent negamax","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T06:27:03.439927Z","iopub.execute_input":"2025-02-10T06:27:03.440287Z","iopub.status.idle":"2025-02-10T06:28:40.220988Z","shell.execute_reply.started":"2025-02-10T06:27:03.440261Z","shell.execute_reply":"2025-02-10T06:28:40.220108Z"}},"outputs":[{"name":"stdout","text":"Agent 1 Win Percentage: 0.4\nAgent 2 Win Percentage: 0.6\nNumber of Invalid Plays by Agent 1: 0\nNumber of Invalid Plays by Agent 2: 0\n","output_type":"stream"}],"execution_count":141},{"cell_type":"code","source":"policy_kwargs = dict(\n    features_extractor_class=CustomResNetCNN#CustomCNN2,\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.550Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nenvTrain_new = ConnectFourGym(agent2=\"random\")  # Change the opponent\n#envTrain_new = ConnectFourGym(agent2=MinMaxAB_agent)  # Change the opponent\n\nmodel = PPO.load(\"ppo_connect4.pkl\", env=envTrain_new, device=device)\n#model = PPO.load(\"/kaggle/input/ppo_connect4/pytorch/default/1/ppo_connect4.pkl\", env=envTrain_new, device=device)\n\n# Create a new model with modified hyperparameters, using the old model's parameters\nnew_model = PPO(\"CnnPolicy\", env=envTrain_new, policy_kwargs=model.policy_kwargs, \n                learning_rate=0.0003,  # Change learning rate here\n                gamma=0.99,            # Change gamma here\n                clip_range=0.1,       # Change clip range here\n                verbose=0, device=device)\n\n# Load the trained parameters from the old model into the new one\n#new_model.policy.load_state_dict(model.policy.state_dict())\n\n# Continue training\nnew_model.learn(total_timesteps=50000)\nnew_model.save(\"ppo_connect4.pkl\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.550Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\n# Start timer\nstart_time = time.time()\n\n\n# Use GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n#envTrain_new = ConnectFourGym(agent2=\"random\")  # Change the opponent\n#envTrain_new = ConnectFourGym2(agent2=\"random\")\n#envTrain_new = ConnectFourGym(agent2=MinMaxAB_agent)  # Change the opponent\n#envTrain_new = ConnectFourGym(agent2=OneStep_agent)  \n\n\n#model = PPO.load(\"/kaggle/input/ppo_connect4/pytorch/default/1/ppo_connect4.pkl\", env=envTrain_new, device=device)\n\n# Create a new model with modified hyperparameters, using the old model's parameters\n\"\"\"\nnew_model = PPO(\"CnnPolicy\", env=envTrain_new, policy_kwargs=model.policy_kwargs, \n                learning_rate=0.0003,  # Change learning rate here\n                gamma=0.99,            # Change gamma here\n                clip_range=0.1,       # Change clip range here\n                verbose=0, device=device)\n\"\"\"\nmodel = PPO.load(\"/kaggle/working/ppo_connect4_selfplay_v2.pkl\", device=device)#, env=envTrain_new, device=device) ppo_Res_connect4\ndef self_play_opponent_v2(obs, config):\n    #model = PPO.load(\"/kaggle/working/ppo_connect4_selfplay_v2.pkl\", device=device)\n    obs_array = np.array(obs['board']).reshape(1, config.rows, config.columns)\n    action, _states = model.predict(obs_array)\n    return int(action)\n    \nenvTrain_new = ConnectFourGym2(agent2=self_play_opponent_v2) #\"random\"\n\nnew_model = PPO(\"CnnPolicy\", envTrain_new, \n                policy_kwargs=policy_kwargs,  \n                learning_rate=0.0001,  # ğŸ”¥ Lower LR for stability\n                gamma=0.997,           # ğŸ”¥ More long-term planning\n                clip_range=0.2,        # ğŸ”¥ Keep stable updates\n                ent_coef=0.02,         # ğŸ”¥ Encourage some exploration\n                n_steps=4096,          # ğŸ”¥ More steps per update\n                batch_size=128,        # ğŸ”¥ Increase batch size for ResNet\n                verbose=0, device=device)\n\n\n# Load the trained parameters from the old model into the new one\nnew_model.policy.load_state_dict(model.policy.state_dict())\n\n\n# Train against random first\n#envTrain_new = ConnectFourGym(agent2=\"random\")\nnew_model.learn(total_timesteps=150000)\nnew_model.save(\"ppo_Res_connect4_01.pkl\")\nend_time = time.time()\n\n# Print total training time\nprint(f\"Training completed in {end_time - start_time:.2f} seconds\")\n\n# Then against Simple_agent\nenvTrain_new = ConnectFourGym(agent2=Simple_agent)\nnew_model.set_env(envTrain_new)\nnew_model.learn(total_timesteps=60000)\nnew_model.save(\"ppo_Res_connect4_02.pkl\")\nend_time = time.time()\n\n# Print total training time\nprint(f\"Training completed in {end_time - start_time:.2f} seconds\")\n\n# Finally, train against Negamax\nenvTrain_new = ConnectFourGym(agent2=\"negamax\")\nnew_model.set_env(envTrain_new)\nnew_model.learn(total_timesteps=60000)  # More training time for stronger opponents\nnew_model.save(\"ppo_Res_connect4_03.pkl\")\nend_time = time.time()\n\n# Print total training time\nprint(f\"Training completed in {end_time - start_time:.2f} seconds\")\n\n\n\"\"\"\n#new_model.learn(total_timesteps=50000)\n\n# Then, train against a smarter opponent\n#envTrain_new = ConnectFourGym(agent2=\"negamax\")\n#new_model.set_env(envTrain_new)\n#new_model.learn(total_timesteps=50000)\n\n\n# Then, train against a smarter opponent\nenvTrain_new = ConnectFourGym(agent2=MinMaxAB_agent)\nnew_model.set_env(envTrain_new)\nnew_model.learn(total_timesteps=50000)\n\n# Finally, train against the OneStep_agent\nenvTrain_new = ConnectFourGym(agent2=OneStep_agent)\nnew_model.set_env(envTrain_new)\nnew_model.learn(total_timesteps=50000)\n\"\"\"\n\nnew_model.save(\"ppo_Res_connect4.pkl\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_model.save(\"ppo_Res_connect4.pkl\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nenvTrain_new = ConnectFourGym(agent2=\"random\")  # Change the opponent\n#envTrain_new = ConnectFourGym(agent2=MinMaxAB_agent)  # Change the opponent\n\n# Create a new model with modified hyperparameters, using the old model's parameters\nnew_model = PPO(\"CnnPolicy\", env=envTrain_new, policy_kwargs=policy_kwargs, \n                learning_rate=0.0003,  # Change learning rate here\n                gamma=0.99,            # Change gamma here\n                clip_range=0.1,       # Change clip range here\n                verbose=0, device=device)\n\n# Load the trained parameters from the old model into the new one\n#new_model.policy.load_state_dict(model.policy.state_dict())\n\n# Continue training\nnew_model.learn(total_timesteps=50000)\nnew_model.save(\"ppo_connect4.pkl\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\"\"\"\nimport time\n\n# Start timer\nstart_time = time.time()\n\n# Train agent\nmodel.learn(total_timesteps=5000)\n\nend_time = time.time()\n\n# Print total training time\nprint(f\"Training completed in {end_time - start_time:.2f} seconds\")\n\"\"\"","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n#envTrain_new = ConnectFourGym(agent2=\"random\")  # Change the opponent\nenvTrain_new = ConnectFourGym(agent2=MinMaxAB_agent)  # Change the opponent\n\nmodel = PPO.load(\"ppo_connect4.pkl\", env=envTrain_new, device=device)\n#model = PPO.load(\"/kaggle/input/ppo_connect4/pytorch/default/1/ppo_connect4.pkl\", env=envTrain_new, device=device)\n\n# Create a new model with modified hyperparameters, using the old model's parameters\nnew_model = PPO(\"CnnPolicy\", env=envTrain_new, policy_kwargs=model.policy_kwargs, \n                learning_rate=0.0003,  # Change learning rate here\n                gamma=0.99,            # Change gamma here\n                clip_range=0.1,       # Change clip range here\n                verbose=1, device=device)\n\n# Load the trained parameters from the old model into the new one\nnew_model.policy.load_state_dict(model.policy.state_dict())\n\n# Continue training\nnew_model.learn(total_timesteps=50000)\nnew_model.save(\"ppo_connect4.pkl\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_model.save(\"ppo_connect42.pkl\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Extract data\nepisodes = [h[0] for h in envTrain_new.history]\nwins = [h[1] for h in envTrain_new.history]\nlosses = [h[2] for h in envTrain_new.history]\ndraws = [h[3] for h in envTrain_new.history]\n\n# Plot results\nplt.figure(figsize=(10, 5))\nplt.plot(episodes, wins, label=\"Wins\", linestyle=\"-\", marker=\"o\")\nplt.plot(episodes, losses, label=\"Losses\", linestyle=\"--\", marker=\"x\")\nplt.plot(episodes, draws, label=\"Draws\", linestyle=\":\", marker=\"s\")\n\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Count\")\nplt.title(\"Connect Four Training Progress\")\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#model.save(\"ppo_connect4.pkl\")\n#model = PPO.load(\"/kaggle/input/ppo_connect411.pkl/pytorch/default/1/ppo_connect4(11).pkl\")\nmodel = PPO.load(\"/kaggle/input/ppo_res_connect4__selfplay_v1/pytorch/default/1/ppo_Res_connect4__selfplay_v1.pkl\")\n#model = PPO.load(\"ppo_Res_connect4.pkl\") #ppo_Res_connect4 ppo_connect4\n\ndef agent1(obs, config):\n    # Use the best model to select a column\n    col, _ = model.predict(np.array(obs['board']).reshape(1, 6,7))\n    # Check if selected column is valid\n    is_valid = (obs['board'][int(col)] == 0)\n    # If not valid, select random move. \n    if is_valid:\n        return int(col)\n    else:\n        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])\n        \ndef get_win_percentages(agent1, agent2, n_rounds=100):\n    # Use default Connect Four setup\n    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n    # Agent 1 goes first (roughly) half the time          \n    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n    # Agent 2 goes first (roughly) half the time      \n    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))\n\n\n\nget_win_percentages(agent1=agent1, agent2=\"random\", n_rounds=100) \n\n#Simple_agent OneStep_agent agent1 \"random\" one_step_lookahead","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def agent1(obs, config):\n    # Use the best model to select a column\n    col, _ = model.predict(np.array(obs['board']).reshape(1, 6,7))\n    # Check if selected column is valid\n    is_valid = (obs['board'][int(col)] == 0)\n    # If not valid, select random move. \n    if is_valid:\n        return int(col)\n    else:\n        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the game environment\nenv = make(\"connectx\")\n\n# Two random agents play one game round \"random\"\nenv.run([agent1, OneStep_agent])\n\n# Show the game\nenv.render(mode=\"ipython\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nimport numpy as np\nimport pandas as pd\nimport gym\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom kaggle_environments import make, evaluate\nfrom gym import spaces\n\nimport torch\nimport torch as th\nimport torch.nn as nn\n\n#!pip install \"stable-baselines3\"\nfrom stable_baselines3 import PPO \nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n\n\n\nprint(\"GPU Available:\", torch.cuda.is_available())\nprint(\"Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_environments import make, evaluate\n\n# Create the game environment\nenv = make(\"connectx\")\n\n# Two random agents play one game round\nenv.run([OneStep_agent, \"random\"]) #my_agent_Nab my_agent3 OneStep_agent MinMaxAB_agent\n\n# Show the game\nenv.render(mode=\"ipython\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nimport numpy as np\nimport pandas as pd\nimport gym\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom kaggle_environments import make, evaluate\nfrom gym import spaces\n\nclass ConnectFourGym(gym.Env):\n    def __init__(self, agent2=\"random\"):\n        ks_env = make(\"connectx\", debug=True)\n        self.env = ks_env.train([None, agent2])\n        self.rows = ks_env.configuration.rows\n        self.columns = ks_env.configuration.columns\n        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n        self.action_space = spaces.Discrete(self.columns)\n        self.observation_space = spaces.Box(low=0, high=2, \n                                            shape=(1,self.rows,self.columns), dtype=int)\n        # Tuple corresponding to the min and max possible rewards\n        self.reward_range = (-10, 1)\n        # StableBaselines throws error if these are not defined\n        self.spec = None\n        self.metadata = None\n        \n    def reset(self):\n        self.obs = self.env.reset()\n        return np.array(self.obs['board']).reshape(1,self.rows,self.columns)\n        \n    def change_reward(self, old_reward, done):\n        if old_reward == 1: # The agent won the game\n            return 1\n        elif done: # The opponent won the game\n            return -1\n        else: # Reward 1/42\n            return 1/(self.rows*self.columns)\n            \n    def step(self, action):\n        # Check if agent's move is valid\n        is_valid = (self.obs['board'][int(action)] == 0)\n        if is_valid: # Play the move\n            self.obs, old_reward, done, _ = self.env.step(int(action))\n            reward = self.change_reward(old_reward, done)\n        else: # End the game and penalize agent\n            reward, done, _ = -10, True, {}\n        return np.array(self.obs['board']).reshape(1,self.rows,self.columns), reward, done, _\n    \n# Create ConnectFour environment \nenvTrain = ConnectFourGym(agent2=\"random\")\n#envTrain = ConnectFourGym(agent2=MinMaxAB_agent)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Old Structure\n\nimport torch as th\nimport torch.nn as nn\n\n#!pip install \"stable-baselines3\"\nfrom stable_baselines3 import PPO \nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n\n# Neural network for predicting action values\nclass CustomCNN(BaseFeaturesExtractor):\n    \n    def __init__(self, observation_space: gym.spaces.Box, features_dim: int=128):\n        super(CustomCNN, self).__init__(observation_space, features_dim)\n        # CxHxW images (channels first)\n        n_input_channels = observation_space.shape[0]\n        self.cnn = nn.Sequential(\n            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=0),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),\n            nn.ReLU(),\n            nn.Flatten(),\n        )\n\n        # Compute shape by doing one forward pass\n        with th.no_grad():\n            n_flatten = self.cnn(\n                th.as_tensor(observation_space.sample()[None]).float()\n            ).shape[1]\n\n        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n\n    def forward(self, observations: th.Tensor) -> th.Tensor:\n        return self.linear(self.cnn(observations))\n\npolicy_kwargs = dict(\n    features_extractor_class=CustomCNN,\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## New Structure\n\nimport torch as th\nimport torch.nn as nn\nimport gym\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n\nclass CustomCNN(BaseFeaturesExtractor):\n    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 128):\n        super(CustomCNN, self).__init__(observation_space, features_dim)\n        \n        n_input_channels = observation_space.shape[0]\n        \n        self.cnn = nn.Sequential(\n            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Flatten(),\n        )\n\n        # Compute shape by doing a forward pass with a dummy tensor\n        with th.no_grad():\n            n_flatten = self.cnn(th.as_tensor(observation_space.sample()[None]).float()).shape[1]\n\n        # Multi-layer MLP head for better feature extraction\n        self.linear = nn.Sequential(\n            nn.Linear(n_flatten, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),  # Dropout for regularization\n            nn.Linear(256, features_dim),\n            nn.ReLU()\n        )\n\n    def forward(self, observations: th.Tensor) -> th.Tensor:\n        return self.linear(self.cnn(observations))\n\npolicy_kwargs = dict(\n    features_extractor_class=CustomCNN,\n)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from stable_baselines3.common.callbacks import BaseCallback\nimport time\nimport numpy as np\n\nclass RewardTrackingCallback(BaseCallback):\n    def __init__(self, check_freq=1000, verbose=1):\n        super(RewardTrackingCallback, self).__init__(verbose)\n        self.check_freq = check_freq\n        self.episode_rewards = []\n        self.episode_wins = 0  # Count wins\n        self.episode_losses = 0  # Count losses\n        self.episode_count = 0\n        self.start_time = time.time()\n    \n    def _on_step(self) -> bool:\n        # Check if episode is done\n        if \"episode\" in self.locals:\n            episode_reward = self.locals[\"episode\"][\"r\"]\n            self.episode_rewards.append(episode_reward)\n            self.episode_count += 1\n            \n            # Assume reward of 1 means a win, -1 means a loss\n            if episode_reward == 1:\n                self.episode_wins += 1\n            elif episode_reward == -1:\n                self.episode_losses += 1\n            \n            # Print reward stats every check_freq steps\n            if self.num_timesteps % self.check_freq == 0:\n                avg_reward = np.mean(self.episode_rewards[-10:])  # Average last 10 episodes\n                win_rate = (self.episode_wins / self.episode_count) * 100 if self.episode_count > 0 else 0\n                elapsed_time = time.time() - self.start_time\n                \n                print(f\"Timestep: {self.num_timesteps} | Avg Reward: {avg_reward:.2f} | Wins: {self.episode_wins} | Losses: {self.episode_losses} | Win Rate: {win_rate:.2f}% | Time: {elapsed_time:.2f}s\")\n\n        return True  # Continue training\n\n# Attach the callback\ncallback = RewardTrackingCallback(check_freq=1)\n\n# Create PPO model with GPU support\nmodel = PPO(\"CnnPolicy\", envTrain, policy_kwargs=policy_kwargs, verbose=1, device=device)\n\n# Train the agent\nmodel.learn(total_timesteps=50000, callback=callback)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from stable_baselines3.common.callbacks import BaseCallback\nimport time\n\nclass StepRewardTrackingCallback(BaseCallback):\n    def __init__(self, verbose=1):\n        super(StepRewardTrackingCallback, self).__init__(verbose)\n        self.start_time = time.time()\n\n    def _on_step(self) -> bool:\n        # Retrieve reward from locals\n        reward = self.locals[\"rewards\"]\n        done = self.locals[\"dones\"]\n\n        # Print step-wise reward details\n        elapsed_time = time.time() - self.start_time\n        print(f\"Timestep: {self.num_timesteps} | Reward: {reward} | Done: {done} | Time: {elapsed_time:.2f}s\")\n\n        return True  # Continue training\n\n# Attach the callback\ncallback = StepRewardTrackingCallback()\n\n# Create PPO model with GPU support\n#model = PPO(\"CnnPolicy\", envTrain, policy_kwargs=policy_kwargs, verbose=1, device=device)\n\n# Train the agent\n#model.learn(total_timesteps=50000, callback=callback)\n\npolicy_kwargs = dict(\n    features_extractor_class=CustomCNN,\n)\n        \n# Initialize agent\nmodel = PPO(\"CnnPolicy\", envTrain, policy_kwargs=policy_kwargs, verbose=0)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train agent\nmodel.learn(total_timesteps=6)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\n# Start timer\nstart_time = time.time()\n\n# Use GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Create PPO model with GPU support\nmodel = PPO(\"CnnPolicy\", envTrain, policy_kwargs=policy_kwargs, verbose=1, device=device)\n\n# Train the agent\nmodel.learn(total_timesteps=50000, callback=callback)\n\n\n###################\n\n# Initialize agent\n#model = PPO(\"CnnPolicy\", env, policy_kwargs=policy_kwargs, verbose=0)\n\n# Train agent\n#model.learn(total_timesteps=1) #50000\n\n# Save the trained model\n#model.save(\"/kaggle/working/ppo_model\")\n\n# End timer\nend_time = time.time()\n\n# Print total training time\nprint(f\"Training completed in {end_time - start_time:.2f} seconds\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create PPO model with GPU support\nmodel = PPO(\"CnnPolicy\", envTrain, policy_kwargs=policy_kwargs, verbose=1, device=device)\n\n# Train the agent\nmodel.learn(total_timesteps=50000, callback=callback)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"/kaggle/working/ppo_model_winMM\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def RL_agent(obs, config):\n    from stable_baselines3 import PPO\n    import random\n    import numpy as np\n\n   # model = PPO.load(\"/kaggle/input/ppo_modelpkl/pytorch/default/1/ppo_model.pkl\")\n    model = PPO.load(\"/kaggle/working/ppo_model_winMM\")\n    \n    # Use the best model to select a column\n    col, _ = model.predict(np.array(obs['board']).reshape(1, 6,7))\n    # Check if selected column is valid\n    is_valid = (obs['board'][int(col)] == 0)\n    # If not valid, select random move. \n    if is_valid:\n        return int(col)\n    else:\n        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_environments import make, evaluate\n\n# Create the game environment\nenv = make(\"connectx\")\n\n# Two random agents play one game round\n#env.run([RL_agent, MinMaxAB_agent])\nenv.run([RL_agent, 'random'])\n\n# Show the game\nenv.render(mode=\"ipython\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndef get_win_percentages(agent1, agent2, n_rounds=100):\n    # Use default Connect Four setup\n    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n    # Agent 1 goes first (roughly) half the time          \n    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n    # Agent 2 goes first (roughly) half the time      \n    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))\n\nget_win_percentages(agent1=RL_agent, agent2='random', n_rounds=10)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import inspect\nimport os\n\ndef write_agent_to_file(function, file):\n    with open(file, \"a\" if os.path.exists(file) else \"w\") as f:\n        f.write(inspect.getsource(function))\n        print(function, \"written to\", file)\n\nwrite_agent_to_file(RL_agent, \"submission.py\") #my_agent my_agent_Nab","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nfrom kaggle_environments import agent\nfrom kaggle_environments import utils\n\nout = sys.stdout\nsubmission = utils.read_file(\"/kaggle/working/submission.py\")\nagent = agent.get_last_callable(submission, path=submission)\nsys.stdout = out\n\nenv = make(\"connectx\", debug=True)\nenv.run([agent, agent])\nprint(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -R /kaggle/working","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.555Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h1>Submisstion Version<h1/>","metadata":{}},{"cell_type":"code","source":"def RL_agent(obs, config):\n    from stable_baselines3 import PPO\n    import random\n    import numpy as np\n\n    model = PPO.load(\"./ppo_model.pkl\")\n    \n    # Use the best model to select a column\n    col, _ = model.predict(np.array(obs['board']).reshape(1, 6,7))\n    # Check if selected column is valid\n    is_valid = (obs['board'][int(col)] == 0)\n    # If not valid, select random move. \n    if is_valid:\n        return int(col)\n    else:\n        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])\n\n\nimport inspect\nimport os\n\ndef write_agent_to_file(function, file):\n    with open(file, \"a\" if os.path.exists(file) else \"w\") as f:\n        f.write(inspect.getsource(function))\n        print(function, \"written to\", file)\n\nwrite_agent_to_file(RL_agent, \"submission.py\") #my_agent my_agent_Nab","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.555Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nThe submission way was referenced by  \n[NickMacd's PPO using SB3 Notebook](https://www.kaggle.com/code/nickmacd/ppo-using-sb3/notebook)  \nand  \n[Connect X Exercise: Deep RL Submission V1](https://www.kaggle.com/code/svendaj/connect-x-exercise-deep-rl-submission-v1/notebook).\n","metadata":{"execution":{"iopub.status.busy":"2025-02-08T23:11:17.210949Z","iopub.execute_input":"2025-02-08T23:11:17.211323Z","iopub.status.idle":"2025-02-08T23:11:17.218534Z","shell.execute_reply.started":"2025-02-08T23:11:17.211256Z","shell.execute_reply":"2025-02-08T23:11:17.216994Z"}}},{"cell_type":"code","source":"#Load a previous agent\nmodel = PPO.load(\"/kaggle/input/ppo_modelpkl/pytorch/default/1/ppo_model.pkl\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#save agent\nmodel.save('ppo_connectx')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile main.py\nimport os, sys, random\nimport numpy as np\nfrom stable_baselines3 import PPO\nimport torch\n\ncwd = '/kaggle_simulations/agent/'\nif os.path.exists(cwd):\n    sys.path.append(cwd)\nelse:\n    cwd = ''\n\nmodel = None\n\ndef agent(obs, config):\n    global model\n    # load the trained model\n    if model == None:\n        model = PPO.load(cwd + \"ppo_connectx\")\n    \"\"\"\n    #reshape the board into the expected output    \n    board = torch.tensor(obs['board'], dtype=torch.float32)\n    mark = obs['mark']\n    board[(board !=mark) & (board != 0)] = 8\n    board[board==mark] = 4\n    board = board/8\n    board = torch.reshape(board, (6,7))\n    board = board.unsqueeze(dim=0)\n    \n    #predict the action\n    action, _ = model.predict(board, deterministic=True)\n    \n    #if valid, return the action, else choose a random action\n    if board[0][0][action] ==0:\n        return int(action)\n    return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])\n    \"\"\"\n    \n    # Use the best model to select a column\n    col, _ = model.predict(np.array(obs['board']).reshape(1, 6,7))\n    # Check if selected column is valid\n    is_valid = (obs['board'][int(col)] == 0)\n    # If not valid, select random move. \n    if is_valid:\n        return int(col)\n    else:\n        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])\n    ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pack files used for submission\n!tar cvfz submission.tar.gz main.py ppo_connectx.zip","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.556Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nfrom kaggle_environments import utils, agent\nfrom kaggle_environments import make, evaluate\n\nout = sys.stdout\nsubmission = utils.read_file(\"main.py\")\nagent = agent.get_last_callable(submission, path= submission)\nsys.stdout = out\n\nenv = make(\"connectx\", debug=True)\nenv.run([agent, MinMaxAB_agent])\nprint(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")\nenv.render(mode=\"ipython\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-10T04:37:33.556Z"}},"outputs":[],"execution_count":null}]}