{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport gym\nfrom kaggle_environments import make, evaluate\nfrom gym import spaces\n\nimport torch\nimport torch as th\nimport torch.nn as nn\n\n#!pip install \"stable-baselines3\"\nfrom stable_baselines3 import PPO \nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T02:26:22.450631Z","iopub.execute_input":"2025-02-11T02:26:22.450972Z","iopub.status.idle":"2025-02-11T02:26:49.027928Z","shell.execute_reply.started":"2025-02-11T02:26:22.450918Z","shell.execute_reply":"2025-02-11T02:26:49.027002Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  from pkg_resources import resource_stream, resource_exists\n/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(pkg)\n/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(pkg)\n/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(pkg)\n/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(pkg)\n/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n  from jax import xla_computation as _xla_computation\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#https://medium.com/@chen-yu/building-a-customized-residual-cnn-with-pytorch-471810e894ed\n\n# ‚úÖ Define Residual Block\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int) -> None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        # üî• Downsample if input channels ‚â† output channels\n        self.downsample = None\n        if in_channels != out_channels:\n            self.downsample = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        identity = self.downsample(x) if self.downsample else x\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x += identity  # üî• Residual Connection\n        return self.relu(x)\n\n\n# ‚úÖ Define Custom Feature Extractor with Residual Blocks\nclass CustomResNetCNN(BaseFeaturesExtractor):\n    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n        super().__init__(observation_space, features_dim)\n        \n        n_input_channels = observation_space.shape[0]  # 1 channel for Connect 4\n\n        self.cnn = nn.Sequential(\n            ResidualBlock(n_input_channels, 32),  # Expand to 64\n            ResidualBlock(32, 64),  # Keep 64\n            ResidualBlock(64, 64),  # Keep 64\n            ResidualBlock(64, 128),  # Expand to 128\n            ResidualBlock(128, 128),  # Keep 128\n            ResidualBlock(128, 256),  # Expand to 256\n            ResidualBlock(256, 256),  # Keep 256\n            nn.Flatten(),\n        )\n\n        # Compute output shape dynamically\n        with th.no_grad():\n            n_flatten = self.cnn(th.as_tensor(observation_space.sample()[None]).float()).shape[1]\n\n        self.linear = nn.Sequential(\n            nn.Linear(n_flatten, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, features_dim),\n            nn.ReLU()\n        )\n\n    def forward(self, observations: th.Tensor) -> th.Tensor:\n        return self.linear(self.cnn(observations))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T08:06:47.374688Z","iopub.execute_input":"2025-02-10T08:06:47.375083Z","iopub.status.idle":"2025-02-10T08:06:47.387275Z","shell.execute_reply.started":"2025-02-10T08:06:47.375053Z","shell.execute_reply":"2025-02-10T08:06:47.385937Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomCNN(BaseFeaturesExtractor):\n    \n    def __init__(self, observation_space: gym.spaces.Box, features_dim: int=256):\n        super(CustomCNN, self).__init__(observation_space, features_dim)\n        # CxHxW images (channels first)\n        n_input_channels = observation_space.shape[0]\n        self.cnn = nn.Sequential(\n            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=0),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),\n            nn.ReLU(),\n            nn.Flatten(),\n        )\n\n        # Compute shape by doing one forward pass\n        with th.no_grad():\n            n_flatten = self.cnn(\n                th.as_tensor(observation_space.sample()[None]).float()\n            ).shape[1]\n\n        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n\n    def forward(self, observations: th.Tensor) -> th.Tensor:\n        return self.linear(self.cnn(observations))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T08:06:52.111448Z","iopub.execute_input":"2025-02-10T08:06:52.111896Z","iopub.status.idle":"2025-02-10T08:06:52.119807Z","shell.execute_reply.started":"2025-02-10T08:06:52.111863Z","shell.execute_reply":"2025-02-10T08:06:52.118495Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def MinMaxAB_agent(obs, config): #MinMax with alpha-beta pruning\n    ################################\n    # Imports and helper functions #\n    ################################\n    \n    import numpy as np\n    import random\n    # Helper function for score_move: gets board at next step if agent drops piece in selected column\n    def drop_piece(grid, col, mark, config):\n        next_grid = grid.copy()\n        for row in range(config.rows-1, -1, -1):\n            if next_grid[row][col] == 0:\n                break\n        next_grid[row][col] = mark\n        return next_grid\n            \n    # Helper function for get_heuristic: checks if window satisfies heuristic conditions\n    def check_window(window, num_discs, piece, config):\n        return (window.count(piece) == num_discs and window.count(0) == config.inarow-num_discs)\n        \n    # Helper function for get_heuristic: counts number of windows satisfying specified heuristic conditions\n    def count_windows(grid, num_discs, piece, config):\n        num_windows = 0\n        # horizontal\n        for row in range(config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[row, col:col+config.inarow])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # vertical\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns):\n                window = list(grid[row:row+config.inarow, col])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # positive diagonal\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # negative diagonal\n        for row in range(config.inarow-1, config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        return num_windows\n    \n    # Helper function for minimax: calculates value of heuristic for grid\n    def get_heuristic(grid, mark, config):\n        num_threes = count_windows(grid, 3, mark, config)\n        num_fours = count_windows(grid, 4, mark, config)\n        num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n        num_fours_opp = count_windows(grid, 4, mark%2+1, config)\n        score = num_threes - 1e2*num_threes_opp - 1e4*num_fours_opp + 1e6*num_fours\n        return score\n\n    # Uses minimax to calculate value of dropping piece in selected column\n    def score_move(grid, col, mark, config, nsteps):\n        next_grid = drop_piece(grid, col, mark, config)\n        score = minimax(next_grid, nsteps-1, False, mark, config)\n        return score\n    \n    # Helper function for minimax: checks if agent or opponent has four in a row in the window\n    def is_terminal_window(window, config):\n        return window.count(1) == config.inarow or window.count(2) == config.inarow\n    \n    # Helper function for minimax: checks if game has ended\n    def is_terminal_node(grid, config):\n        # Check for draw \n        if list(grid[0, :]).count(0) == 0:\n            return True\n        # Check for win: horizontal, vertical, or diagonal\n        # horizontal \n        for row in range(config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[row, col:col+config.inarow])\n                if is_terminal_window(window, config):\n                    return True\n        # vertical\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns):\n                window = list(grid[row:row+config.inarow, col])\n                if is_terminal_window(window, config):\n                    return True\n        # positive diagonal\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n                if is_terminal_window(window, config):\n                    return True\n        # negative diagonal\n        for row in range(config.inarow-1, config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n                if is_terminal_window(window, config):\n                    return True\n        return False\n    \n    # Minimax implementation\n    def minimax(node, depth, maximizingPlayer, mark, config, a=-np.Inf, b=np.Inf):\n        is_terminal = is_terminal_node(node, config)\n        valid_moves = [c for c in range(config.columns) if node[0][c] == 0]\n        if depth == 0 or is_terminal:\n            return get_heuristic(node, mark, config)\n        if maximizingPlayer:\n            value = -np.Inf\n            for col in valid_moves:\n                child = drop_piece(node, col, mark, config)\n                value = max(value, minimax(child, depth-1, False, mark, config, a, b))\n                if value > b:\n                    break\n            a = max(a, value)\n            return value\n        else:\n            value = np.Inf\n            for col in valid_moves:\n                child = drop_piece(node, col, mark%2+1, config)\n                value = min(value, minimax(child, depth-1, True, mark, config, a, b))\n                if value < a:\n                    break\n            b = min(b, value)\n            return value\n\n    \n    #########################\n    # Agent makes selection #\n    #########################\n\n\n    # How deep to make the game tree: higher values take longer to run!\n    N_STEPS = 1\n    # Get list of valid moves\n    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n    # Convert the board to a 2D grid\n    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n    # Use the heuristic to assign a score to each possible board in the next step\n    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config, N_STEPS) for col in valid_moves]))\n    # Get a list of columns (moves) that maximize the heuristic\n    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n    # Select at random from the maximizing columns\n    return random.choice(max_cols)\n\n\n    \n    #valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n    #for col in valid_moves:\n        #if check_winning_move(obs, config, col, obs.mark):\n            #return col\n            \n    #return random.choice(valid_moves)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T08:06:54.273997Z","iopub.execute_input":"2025-02-10T08:06:54.274371Z","iopub.status.idle":"2025-02-10T08:06:54.297023Z","shell.execute_reply.started":"2025-02-10T08:06:54.274344Z","shell.execute_reply":"2025-02-10T08:06:54.295415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ConnectFourGym(gym.Env):\n    def __init__(self, agent2=\"random\"):\n        ks_env = make(\"connectx\", debug=True)\n        self.env = ks_env.train([None, agent2])\n        self.rows = ks_env.configuration.rows\n        self.columns = ks_env.configuration.columns\n        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n        self.action_space = spaces.Discrete(self.columns)\n\n        self.observation_space = spaces.Box(low=0, high=2, \n                                    shape=(1, self.rows, self.columns), \n                                    dtype=np.float32)  # ‚úÖ Change dtype for DQN\n\n        # Tuple corresponding to the min and max possible rewards\n        self.reward_range = (-10, 1)\n        # StableBaselines throws error if these are not defined\n        self.spec = None\n        self.metadata = None\n\n        self.history = []  # Store (episode, wins, losses, draws)\n        self.wins = 0\n        self.losses = 0\n        self.draws = 0\n        self.timestep = 0  # ‚úÖ Track total timesteps\n        self.winR = 2\n        self.lostR = -5\n        self.drawR = -0.5\n\n    def print_board(self, board):\n        symbols = {0: \".\", 1: \"X\", 2: \"O\"}  # X for agent, O for opponent\n        for r in range(self.rows):\n            row = [symbols[board[r * self.columns + c]] for c in range(self.columns)]\n            print(\" \".join(row))\n        print(\"-\" * 10)  # Separator\n\n    \n    def reset(self):\n        self.episode_reward = 0  # Track total reward. Reset for every new gamble.\n        \n        self.obs = self.env.reset()\n        return np.array(self.obs['board']).reshape(1, self.rows, self.columns).astype(np.float32)\n        \n    def change_reward(self, old_reward, done):\n        if old_reward == 1:  # Model wins\n            return self.winR\n        elif done:  # Model loses\n            return self.lostR\n        else:\n            #return 0.05  # Small positive reward\n            return 1/(self.rows*self.columns)\n\n            \n    def step(self, action):\n        # Check if agent's move is valid\n        is_valid = (self.obs['board'][int(action)] == 0)\n        if is_valid: # Play the move\n            self.obs, old_reward, done, _ = self.env.step(int(action))\n            reward = self.change_reward(old_reward, done)\n        else: # End the game and penalize agent\n            reward, done, _ = self.drawR, True, {}\n\n\n        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")  # Debug print\n        \n        self.episode_reward += reward  # Track total episode reward\n        \n        if done: #Print traning condition.\n            self.timestep += 1  # ‚úÖ Increment timestep counter\n            #print(f\"Episode Finished! Total Reward: {self.episode_reward}\", done)\n            \n            if reward == self.winR:\n                self.wins += 1\n                #print(f\"Episode Finished! Model Won! Total Reward: {self.episode_reward}\")\n            elif reward == self.lostR:\n                self.losses += 1\n                #print(f\"Episode Finished! Model Lost! Total Reward: {self.episode_reward}\")\n            else:\n                self.draws += 1\n                #print(f\"Episode Finished! Draw! Total Reward: {self.episode_reward}\")\n            \n            # Store history\n            self.history.append((len(self.history) + 1, self.wins, self.losses, self.draws))\n\n            # ‚úÖ Print every 50 timesteps\n            if self.timestep % 50 == 0:\n                print(f\"[Step {self.timestep}] Total Wins: {self.wins}, Losses: {self.losses}, Draws: {self.draws}, Total Reward: {self.episode_reward}\")\n\n\n            #print(f\"Total Win:{ self.wins}; Total Lost:{self.losses}; Total draw:{self.draws}\")\n\n        #print(f\"Model chose action: {action}, Reward: {reward}\")\n        #self.print_board(self.obs['board'])  # üî• Visualize board after each move\n        \n        return np.array(self.obs['board']).reshape(1,self.rows,self.columns), reward, done, _","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T08:06:58.772214Z","iopub.execute_input":"2025-02-10T08:06:58.772538Z","iopub.status.idle":"2025-02-10T08:06:58.786224Z","shell.execute_reply.started":"2025-02-10T08:06:58.772513Z","shell.execute_reply":"2025-02-10T08:06:58.784879Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from stable_baselines3.common.callbacks import BaseCallback\nimport os\nimport time\n\nclass StopTrainingCallback(BaseCallback):\n    def __init__(self, save_freq=1000, threshold=0.05, verbose=1):\n        super().__init__(verbose)\n        self.threshold = threshold\n        self.save_freq = save_freq\n        self.bestWinRate = 0\n        #self.save_path = save_path\n\n    def _on_step(self) -> bool:\n        \"\"\"Called at every step during training.\"\"\"\n        total_games = self.training_env.envs[0].wins + self.training_env.envs[0].losses + self.training_env.envs[0].draws\n        if total_games > 25:\n            win_rate = self.training_env.envs[0].wins / total_games\n        else:\n            win_rate = -1\n        #print(win_rate)\n        if win_rate > self.bestWinRate: #self.training_env.envs[0].bestWinRate:\n            print(self.bestWinRate)\n            self.bestWinRate = win_rate\n            self.model.save(f\"ppo_connect4_DynReward_WR_{win_rate:.2f}.pkl\")  # Save updated model\n        #print(self.bestWinRate)\n        \n        \"\"\"\n        if self.n_calls % self.save_freq == 0:  # Every 50 steps\n            timestamp = time.strftime(\"%Y-%m-%d_%H-%M-%S\")  # Format: YYYY-MM-DD_HH-MM-SS\n            model_path = f\"ppo_{timestamp}_step_{self.n_calls}.zip\"\n            self.model.save(model_path)\n            if self.verbose:\n                print(f\"‚úÖ Model saved at: {model_path}\")     \n        \"\"\"\n                \n        total_games = self.training_env.envs[0].wins + self.training_env.envs[0].losses + self.training_env.envs[0].draws\n        if total_games > 100:  # Avoid division by zero\n            loss_rate = self.training_env.envs[0].losses / total_games\n            #if self.verbose > 0:\n                #print(f\"üîé Checking Stop Condition: Loss Rate = {loss_rate:.4f}\")\n\n            if loss_rate < self.threshold:  # ‚úÖ Stop training if loss rate is too low\n                print(f\"üöÄ Stopping Training! Loss Rate = {loss_rate:.4f} < {self.threshold}\")\n                return False  # Returning False stops training\n        return True  # Continue training\n\n# ‚úÖ Initialize the callback\nstop_callback = StopTrainingCallback(threshold=0.06, verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T08:07:02.279664Z","iopub.execute_input":"2025-02-10T08:07:02.280086Z","iopub.status.idle":"2025-02-10T08:07:02.288950Z","shell.execute_reply.started":"2025-02-10T08:07:02.280055Z","shell.execute_reply":"2025-02-10T08:07:02.287610Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DymaicRewardConnectFour(gym.Env):\n    def __init__(self, agent2=\"random\"):\n        ks_env = make(\"connectx\", debug=True)\n        self.env = ks_env.train([None, agent2])\n        self.config = ks_env.configuration\n        #ks_env.configuration['inarow'] = 5\n        self.rows = ks_env.configuration.rows\n        self.columns = ks_env.configuration.columns\n        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n        self.action_space = spaces.Discrete(self.columns)\n        self.observation_space = spaces.Box(low=0, high=2, \n                                            shape=(1,self.rows,self.columns), dtype=int)\n        # Tuple corresponding to the min and max possible rewards\n        self.reward_range = (-10, 10)\n        # StableBaselines throws error if these are not defined\n        self.spec = None\n        self.metadata = None\n        self.N = 1\n        self.winR = 8/self.N\n        self.lostR = -5/self.N\n        self.illM = -8/self.N\n        self.draw = 2/self.N\n\n        self.wins = 0\n        self.losses = 0\n        self.draws = 0\n        \n        self.timestep = 0\n        self.history = []\n\n        self.bestWinRate = 0\n        \n        \n        #####Dynaimc reward\n    # Helper function for get_heuristic: checks if window satisfies heuristic conditions\n    def check_window(self, window, num_discs, piece, config):\n        return (window.count(piece) == num_discs and window.count(0) == config.inarow-num_discs)\n        \n    # Helper function for get_heuristic: counts number of windows satisfying specified heuristic conditions\n    def count_windows(self, grid, num_discs, piece, config):\n        num_windows = 0\n        # horizontal\n        for row in range(config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[row, col:col+config.inarow])\n                if self.check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # vertical\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns):\n                window = list(grid[row:row+config.inarow, col])\n                if self.check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # positive diagonal\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n                if self.check_window(window, num_discs, piece, config):\n                    num_windows += 1\n    # negative diagonal\n        for row in range(config.inarow-1, config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n                if self.check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        return num_windows\n\n    \n    # Helper function for minimax: calculates value of heuristic for grid\n    def get_heuristic(self, grid, mark, config):\n        num_threes = self.count_windows(grid, 3, mark, config)\n        num_fours = self.count_windows(grid, 4, mark, config)\n        num_threes_opp = self.count_windows(grid, 3, mark%2+1, config)\n        num_fours_opp = self.count_windows(grid, 4, mark%2+1, config)\n        score = num_threes - 1e2*num_threes_opp - 1e4*num_fours_opp + 1e6*num_fours\n        return score\n        \n    def reset(self):\n        self.obs = self.env.reset()\n        self.episode_reward = 0\n        \n        return np.array(self.obs['board']).reshape(1,self.rows,self.columns)\n\n        \n    #def change_reward(self, old_reward, done, grid, mark, config):\n        #return self.get_heuristic(grid, mark, config)#/1e6*5\n        \"\"\"\n        if old_reward == 1: # The agent won the game\n            return self.winR\n        elif done: # The opponent won the game\n            return self.lostR\n        else: # Reward 1/42\n            return self.get_heuristic(grid, mark, config)\n            #return 1/(self.rows*self.columns)\n        \"\"\"\n    \n    \n    def change_reward(self, old_reward, done, grid, mark, config):\n        #Assigns a dynamic reward based on board state.\n       \n        #heuristic_score = self.get_heuristic(grid, mark, config)  # Compute heuristic score\n    \n        if old_reward == 1:  # Agent won the game üéâ\n            return self.winR  # ‚úÖ Give a high reward for winning\n        \n        elif done:  # Game ended\n            return self.lostR  # ‚ùå Give a harsh penalty for losing\n        \n        else:\n            return self.draw #(1 / (self.rows * self.columns)) * self.winR  # Neutral reward for normal moves\n\n        \"\"\"\n        elif heuristic_score > 5000:  # If the move creates a strong advantage\n            return 2  # üî• Reward highly for good strategy\n        \n        elif heuristic_score < -5000:  # If the move helps opponent significantly\n            return -3  # ‚ùå Penalize bad moves that give opponent advantage\n        \n        elif heuristic_score > 1000:  # If the move is decent\n            return 0.5  # üëç Small encouragement\n        \n        elif heuristic_score < -1000:  # If the move is weak\n            return -1  # ‚ö†Ô∏è Small penalty\n        \"\"\"\n           \n    def step(self, action):\n        # Check if agent's move is valid\n        is_valid = (self.obs['board'][int(action)] == 0)\n        grid = np.asarray(self.obs.board).reshape(self.config.rows, self.config.columns)\n        if is_valid: # Play the move\n            self.obs, old_reward, done, _ = self.env.step(int(action))\n            reward = self.change_reward(old_reward, done, grid, self.obs.mark, self.config)\n        else: # End the game and penalize agent\n            reward, done, _ = self.illM, False, {} #True False\n\n        self.episode_reward += reward  # Track total episode reward\n        \n        if done: #Print traning condition.\n            self.timestep += 1  # ‚úÖ Increment timestep counter\n            #print(f\"Episode Finished! Total Reward: {self.episode_reward}\", done)\n            \n            if reward == self.winR:\n                self.wins += 1\n                #print(f\"Episode Finished! Model Won! Total Reward: {self.episode_reward}\")\n            elif reward == self.lostR:\n                self.losses += 1\n                #print(f\"Episode Finished! Model Lost! Total Reward: {self.episode_reward}\")\n            else:\n                self.draws += 1\n                #print(f\"Episode Finished! Draw! Total Reward: {self.episode_reward}\")\n            \n            # Store history\n            self.history.append((len(self.history) + 1, self.wins, self.losses, self.draws))\n            #print(f\"[Step {self.timestep}] Win: {self.wins}, Loss: {self.losses}, Draw: {self.draws}, Total Reward: {self.episode_reward}\")\n            #print(f\"Total Reward: {self.episode_reward}\")\n               \n            # ‚úÖ Print every 50 timesteps\n            if self.timestep % 50 == 0:\n                total = self.wins +self.losses +self.draws\n                win_rate = self.wins / total\n                loss_rate = self.losses / total\n                draw_rate = self.draws / total\n                print(f\"[Step {self.timestep}] Win rate: {win_rate:.2f}, Loss rate: {loss_rate:.2f}, Draw rate: {draw_rate:.2f}, Total Reward: {self.episode_reward}\")\n                #Recompute\n                self.wins = 0\n                self.losses = 0\n                self.draws = 0\n\n        #print(f\"Total Reward: {self.episode_reward}\")\n        \n        return np.array(self.obs['board']).reshape(1,self.rows,self.columns), reward, done, _\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T08:28:47.085082Z","iopub.execute_input":"2025-02-10T08:28:47.085451Z","iopub.status.idle":"2025-02-10T08:28:47.106473Z","shell.execute_reply.started":"2025-02-10T08:28:47.085418Z","shell.execute_reply":"2025-02-10T08:28:47.105327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n    print(\"‚úÖ GPU is available: Using\", torch.cuda.get_device_name(0))\nelse:\n    print(\"‚ùå GPU not available: Using CPU\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T08:07:09.490446Z","iopub.execute_input":"2025-02-10T08:07:09.490940Z","iopub.status.idle":"2025-02-10T08:07:09.503395Z","shell.execute_reply.started":"2025-02-10T08:07:09.490905Z","shell.execute_reply":"2025-02-10T08:07:09.502029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create ConnectFour environment \n#envTrain = ConnectFourGym(agent2=\"random\")\n#envTrain = DymaicRewardConnectFour(agent2=\"random\")\n#envTrain = ConnectFourGym(agent2=MinMaxAB_agent)\n#policy_kwargs = dict(\n   #features_extractor_class=CustomResNetCNN,\n#)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:31:02.808935Z","iopub.execute_input":"2025-02-09T05:31:02.809228Z","iopub.status.idle":"2025-02-09T05:31:02.876821Z","shell.execute_reply.started":"2025-02-09T05:31:02.809210Z","shell.execute_reply":"2025-02-09T05:31:02.875682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from stable_baselines3 import DQN\nfrom stable_baselines3.common.vec_env import DummyVecEnv\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Create the environment\nenvTrain_new = DummyVecEnv([lambda: DymaicRewardConnectFour(agent2=\"random\")])  # Start with a weak opponent\n\npolicy_kwargs = dict(\n    features_extractor_class=CustomResNetCNN, #CustomResNetCNN,  # Use custom CNN CustomCNN\n    features_extractor_kwargs=dict(features_dim=256)  # Output feature size\n)\n\n\n#model = DQN.load(\"/kaggle/working/dqn_connect4.pkl\", env=envTrain_new, device=device)\n\n# Define the DQN model\nnew_model = DQN(\"CnnPolicy\", envTrain_new, \n            policy_kwargs=policy_kwargs,\n            learning_rate=0.00015,  # ‚úÖ Reduce learning rate for smoother updates\n            buffer_size=200000,  # ‚úÖ Larger buffer for better experience replay\n            batch_size=128,  # ‚úÖ Larger batch size for better training stability\n            gamma=0.98,  # ‚úÖ Slightly reduce discount factor to balance short vs. long-term rewards\n            exploration_fraction=0.08,  # ‚úÖ Reduce exploration decay for better exploration\n            exploration_final_eps=0.05,  # ‚úÖ Ensure some exploration remains\n            target_update_interval=5000,  # ‚úÖ Less frequent target updates for stable learning\n            train_freq=8,  # ‚úÖ Train every 8 steps for more updates\n            verbose=0, \n            device=\"cuda\")  # ‚úÖ Force GPU usage\n\"\"\"\n\n# Define PPO Model\nnew_model = PPO(\"CnnPolicy\", envTrain_new, \n            policy_kwargs=policy_kwargs,\n            learning_rate=0.00015,  # ‚úÖ Smooth learning rate\n            n_steps=2048,  # ‚úÖ Increase rollout buffer size\n            batch_size=128,  # ‚úÖ Larger batch size for stable training\n            n_epochs=10,  # ‚úÖ More epochs for policy updates\n            gamma=0.98,  # ‚úÖ Reduce discount factor for balanced long-term rewards\n            gae_lambda=0.95,  # ‚úÖ Generalized Advantage Estimation\n            clip_range=0.2,  # ‚úÖ Standard PPO clip range\n            ent_coef=0.01,  # ‚úÖ Encourage exploration with entropy\n            vf_coef=0.5,  # ‚úÖ Value function coefficient\n            max_grad_norm=0.5,  # ‚úÖ Gradient clipping for stability\n            verbose=0,  \n            device=\"cuda\")  # ‚úÖ Force GPU usag\n\"\"\"\nprint(new_model.device)\n# Load the trained parameters from the old model into the new one\n#new_model.policy.load_state_dict(model.policy.state_dict())\n\n# Train the model\nnew_model.learn(total_timesteps=100000, callback=stop_callback)\nnew_model.save(\"dqn_connect4.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T08:48:20.033391Z","iopub.execute_input":"2025-02-10T08:48:20.033736Z","iopub.status.idle":"2025-02-10T11:34:44.235285Z","shell.execute_reply.started":"2025-02-10T08:48:20.033709Z","shell.execute_reply":"2025-02-10T11:34:44.233366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Javascript\n\n# ‚úÖ Save the model first\n#new_model.save(\"/kaggle/working/dqn_connect4.pkl\")\n\n# ‚úÖ Auto-download (triggers without clicking)\n#auto_download(\"dqn_connect4.pkl\")\n\nfrom IPython.display import FileLink\n\n#model.save(\"/kaggle/working/dqn_connect4.pkl\")  # ‚úÖ Save model\nFileLink(\"/kaggle/working/dqn_connect4.pkl\")  # ‚úÖ Clickable download link\n\n\"\"\"\nimport time\n\n# Start timer\nstart_time = time.time()\n\n\nend_time = time.time()\n\n# Print total training time\nprint(f\"Training completed in {end_time - start_time:.2f} seconds\")\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T07:38:50.461311Z","iopub.execute_input":"2025-02-10T07:38:50.461691Z","iopub.status.idle":"2025-02-10T07:38:50.469697Z","shell.execute_reply.started":"2025-02-10T07:38:50.461660Z","shell.execute_reply":"2025-02-10T07:38:50.468784Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n#envTrain_new = ConnectFourGym(agent2=\"random\")  # Change the opponent\nenvTrain_new = ConnectFourGym(agent2=MinMaxAB_agent)  # Change the opponent\n\nmodel = PPO.load(\"ppo_connect4.pkl\", env=envTrain_new, device=device)\n#model = PPO.load(\"/kaggle/input/ppo_connect4/pytorch/default/1/ppo_connect4.pkl\", env=envTrain_new, device=device)\n\n# Create a new model with modified hyperparameters, using the old model's parameters\nnew_model = PPO(\"CnnPolicy\", env=envTrain_new, policy_kwargs=model.policy_kwargs, \n                learning_rate=0.0003,  # Change learning rate here\n                gamma=0.99,            # Change gamma here\n                clip_range=0.1,       # Change clip range here\n                verbose=1, device=device)\n\n# Load the trained parameters from the old model into the new one\nnew_model.policy.load_state_dict(model.policy.state_dict())\n\n# Continue training\nnew_model.learn(total_timesteps=50000)\nnew_model.save(\"ppo_connect4.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:08:52.505514Z","iopub.execute_input":"2025-02-09T05:08:52.505978Z","iopub.status.idle":"2025-02-09T05:22:39.243101Z","shell.execute_reply.started":"2025-02-09T05:08:52.505937Z","shell.execute_reply":"2025-02-09T05:22:39.241585Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_model.save(\"ppo_connect42.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:23:22.102930Z","iopub.execute_input":"2025-02-09T05:23:22.103238Z","iopub.status.idle":"2025-02-09T05:23:22.143040Z","shell.execute_reply.started":"2025-02-09T05:23:22.103216Z","shell.execute_reply":"2025-02-09T05:23:22.142172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Extract data\nepisodes = [h[0] for h in envTrain_new.history]\nwins = [h[1] for h in envTrain_new.history]\nlosses = [h[2] for h in envTrain_new.history]\ndraws = [h[3] for h in envTrain_new.history]\n\n# Plot results\nplt.figure(figsize=(10, 5))\nplt.plot(episodes, wins, label=\"Wins\", linestyle=\"-\", marker=\"o\")\nplt.plot(episodes, losses, label=\"Losses\", linestyle=\"--\", marker=\"x\")\nplt.plot(episodes, draws, label=\"Draws\", linestyle=\":\", marker=\"s\")\n\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Count\")\nplt.title(\"Connect Four Training Progress\")\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:05:37.321482Z","iopub.execute_input":"2025-02-09T05:05:37.321794Z","iopub.status.idle":"2025-02-09T05:05:37.651769Z","shell.execute_reply.started":"2025-02-09T05:05:37.321774Z","shell.execute_reply":"2025-02-09T05:05:37.650948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#model.save(\"ppo_connect4.pkl\")\n#model = PPO.load(\"/kaggle/input/ppo_connect4/pytorch/default/1/ppo_connect4.pkl\")\nmodel = PPO.load(\"ppo_connect4.pkl\")\n\ndef agent1(obs, config):\n    # Use the best model to select a column\n    col, _ = model.predict(np.array(obs['board']).reshape(1, 6,7))\n    # Check if selected column is valid\n    is_valid = (obs['board'][int(col)] == 0)\n    # If not valid, select random move. \n    if is_valid:\n        return int(col)\n    else:\n        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])\n        \ndef get_win_percentages(agent1, agent2, n_rounds=100):\n    # Use default Connect Four setup\n    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n    # Agent 1 goes first (roughly) half the time          \n    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n    # Agent 2 goes first (roughly) half the time      \n    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))\n\n\n\nget_win_percentages(agent1=agent1, agent2=\"random\", n_rounds=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T05:06:03.430500Z","iopub.execute_input":"2025-02-09T05:06:03.430793Z","iopub.status.idle":"2025-02-09T05:06:04.796116Z","shell.execute_reply.started":"2025-02-09T05:06:03.430772Z","shell.execute_reply":"2025-02-09T05:06:04.795362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def agent1(obs, config):\n    # Use the best model to select a column\n    col, _ = model.predict(np.array(obs['board']).reshape(1, 6,7))\n    # Check if selected column is valid\n    is_valid = (obs['board'][int(col)] == 0)\n    # If not valid, select random move. \n    if is_valid:\n        return int(col)\n    else:\n        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the game environment\nenv = make(\"connectx\")\n\n# Two random agents play one game round\nenv.run([agent1, \"random\"])\n\n# Show the game\nenv.render(mode=\"ipython\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nimport numpy as np\nimport pandas as pd\nimport gym\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom kaggle_environments import make, evaluate\nfrom gym import spaces\n\nimport torch\nimport torch as th\nimport torch.nn as nn\n\n#!pip install \"stable-baselines3\"\nfrom stable_baselines3 import PPO \nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n\n\n\nprint(\"GPU Available:\", torch.cuda.is_available())\nprint(\"Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T01:56:33.301914Z","iopub.execute_input":"2025-02-09T01:56:33.302120Z","iopub.status.idle":"2025-02-09T01:57:02.948029Z","shell.execute_reply.started":"2025-02-09T01:56:33.302090Z","shell.execute_reply":"2025-02-09T01:57:02.946677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def MinMaxAB_agent(obs, config): #MinMax with alpha-beta pruning\n    ################################\n    # Imports and helper functions #\n    ################################\n    \n    import numpy as np\n    import random\n    # Helper function for score_move: gets board at next step if agent drops piece in selected column\n    def drop_piece(grid, col, mark, config):\n        next_grid = grid.copy()\n        for row in range(config.rows-1, -1, -1):\n            if next_grid[row][col] == 0:\n                break\n        next_grid[row][col] = mark\n        return next_grid\n            \n    # Helper function for get_heuristic: checks if window satisfies heuristic conditions\n    def check_window(window, num_discs, piece, config):\n        return (window.count(piece) == num_discs and window.count(0) == config.inarow-num_discs)\n        \n    # Helper function for get_heuristic: counts number of windows satisfying specified heuristic conditions\n    def count_windows(grid, num_discs, piece, config):\n        num_windows = 0\n        # horizontal\n        for row in range(config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[row, col:col+config.inarow])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # vertical\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns):\n                window = list(grid[row:row+config.inarow, col])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # positive diagonal\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # negative diagonal\n        for row in range(config.inarow-1, config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        return num_windows\n    \n    # Helper function for minimax: calculates value of heuristic for grid\n    def get_heuristic(grid, mark, config):\n        num_threes = count_windows(grid, 3, mark, config)\n        num_fours = count_windows(grid, 4, mark, config)\n        num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n        num_fours_opp = count_windows(grid, 4, mark%2+1, config)\n        score = num_threes - 1e2*num_threes_opp - 1e4*num_fours_opp + 1e6*num_fours\n        return score\n\n    # Uses minimax to calculate value of dropping piece in selected column\n    def score_move(grid, col, mark, config, nsteps):\n        next_grid = drop_piece(grid, col, mark, config)\n        score = minimax(next_grid, nsteps-1, False, mark, config)\n        return score\n    \n    # Helper function for minimax: checks if agent or opponent has four in a row in the window\n    def is_terminal_window(window, config):\n        return window.count(1) == config.inarow or window.count(2) == config.inarow\n    \n    # Helper function for minimax: checks if game has ended\n    def is_terminal_node(grid, config):\n        # Check for draw \n        if list(grid[0, :]).count(0) == 0:\n            return True\n        # Check for win: horizontal, vertical, or diagonal\n        # horizontal \n        for row in range(config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[row, col:col+config.inarow])\n                if is_terminal_window(window, config):\n                    return True\n        # vertical\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns):\n                window = list(grid[row:row+config.inarow, col])\n                if is_terminal_window(window, config):\n                    return True\n        # positive diagonal\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n                if is_terminal_window(window, config):\n                    return True\n        # negative diagonal\n        for row in range(config.inarow-1, config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n                if is_terminal_window(window, config):\n                    return True\n        return False\n    \n    # Minimax implementation\n    def minimax(node, depth, maximizingPlayer, mark, config, a=-np.Inf, b=np.Inf):\n        is_terminal = is_terminal_node(node, config)\n        valid_moves = [c for c in range(config.columns) if node[0][c] == 0]\n        if depth == 0 or is_terminal:\n            return get_heuristic(node, mark, config)\n        if maximizingPlayer:\n            value = -np.Inf\n            for col in valid_moves:\n                child = drop_piece(node, col, mark, config)\n                value = max(value, minimax(child, depth-1, False, mark, config, a, b))\n                if value > b:\n                    break\n            a = max(a, value)\n            return value\n        else:\n            value = np.Inf\n            for col in valid_moves:\n                child = drop_piece(node, col, mark%2+1, config)\n                value = min(value, minimax(child, depth-1, True, mark, config, a, b))\n                if value < a:\n                    break\n            b = min(b, value)\n            return value\n\n    \n    #########################\n    # Agent makes selection #\n    #########################\n\n\n    # How deep to make the game tree: higher values take longer to run!\n    N_STEPS = 3\n    # Get list of valid moves\n    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n    # Convert the board to a 2D grid\n    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n    # Use the heuristic to assign a score to each possible board in the next step\n    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config, N_STEPS) for col in valid_moves]))\n    # Get a list of columns (moves) that maximize the heuristic\n    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n    # Select at random from the maximizing columns\n    return random.choice(max_cols)\n\n\n    \n    #valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n    #for col in valid_moves:\n        #if check_winning_move(obs, config, col, obs.mark):\n            #return col\n            \n    #return random.choice(valid_moves)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T01:57:12.287929Z","iopub.execute_input":"2025-02-09T01:57:12.288282Z","iopub.status.idle":"2025-02-09T01:57:12.305944Z","shell.execute_reply.started":"2025-02-09T01:57:12.288261Z","shell.execute_reply":"2025-02-09T01:57:12.305022Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def OneStep_agent(obs, config): #One-Step Look\n    ################################\n    # Imports and helper functions #\n    ################################\n    \n    import numpy as np\n    import random\n    \n    # Calculates score if agent drops piece in selected column\n    def score_move(grid, col, mark, config):\n        next_grid = drop_piece(grid, col, mark, config)\n        score = get_heuristic(next_grid, mark, config)\n        return score\n    \n    # Helper function for score_move: gets board at next step if agent drops piece in selected column\n    def drop_piece(grid, col, mark, config):\n        next_grid = grid.copy()\n        for row in range(config.rows-1, -1, -1):\n            if next_grid[row][col] == 0:\n                break\n        next_grid[row][col] = mark\n        return next_grid\n    \n    # Helper function for score_move: calculates value of heuristic for grid\n    A = 1*10**9\n    B = 1*10**6\n    C = 1*10**2\n    D = -1*10**6\n    E = -1*10**9\n    def get_heuristic(grid, mark, config):\n        num_fours = count_windows(grid, 4, mark, config)\n        num_threes = count_windows(grid, 3, mark, config)\n        num_two = count_windows(grid, 2, mark, config)\n        num_two_opp = count_windows(grid, 2, mark%2+1, config)\n        num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n        score = A*num_fours+B*num_threes+C*num_two+D*num_two_opp+E*num_threes_opp\n        return score\n    \n    # Helper function for get_heuristic: checks if window satisfies heuristic conditions\n    def check_window(window, num_discs, piece, config):\n        return (window.count(piece) == num_discs and window.count(0) == config.inarow-num_discs)\n        \n    # Helper function for get_heuristic: counts number of windows satisfying specified heuristic conditions\n    def count_windows(grid, num_discs, piece, config):\n        num_windows = 0\n        # horizontal\n        for row in range(config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[row, col:col+config.inarow])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # vertical\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns):\n                window = list(grid[row:row+config.inarow, col])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # positive diagonal\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # negative diagonal\n        for row in range(config.inarow-1, config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        return num_windows\n        \n    #########################\n    # Agent makes selection #\n    #########################\n\n    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config) for col in valid_moves]))\n    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n    \n    return random.choice(max_cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T01:58:08.353905Z","iopub.execute_input":"2025-02-09T01:58:08.354317Z","iopub.status.idle":"2025-02-09T01:58:08.366390Z","shell.execute_reply.started":"2025-02-09T01:58:08.354296Z","shell.execute_reply":"2025-02-09T01:58:08.364839Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def Simple_agent(obs, config): #Simple agent with basic rulers block and winning move!\n    # Your code here: Amend the agent!\n    import numpy as np\n    import random\n\n    \"\"\"\n    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n    return random.choice(valid_moves)\n    \"\"\"\n    # Gets board at next step if agent drops piece in selected column\n    def drop_piece(grid, col, piece, config):\n        next_grid = grid.copy()\n        for row in range(config.rows-1, -1, -1):\n            if next_grid[row][col] == 0:\n                break\n        next_grid[row][col] = piece\n        return next_grid\n    \n    # Returns True if dropping piece in column results in game win\n    def check_winning_move(obs, config, col, piece):\n        # Convert the board to a 2D grid\n        grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n        next_grid = drop_piece(grid, col, piece, config)\n        # horizontal\n        for row in range(config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(next_grid[row,col:col+config.inarow])\n                if window.count(piece) == config.inarow:\n                    return True\n        # vertical\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns):\n                window = list(next_grid[row:row+config.inarow,col])\n                if window.count(piece) == config.inarow:\n                    return True\n        # positive diagonal\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(next_grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n                if window.count(piece) == config.inarow:\n                    return True\n        # negative diagonal\n        for row in range(config.inarow-1, config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(next_grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n                if window.count(piece) == config.inarow:\n                    return True\n        return False\n    \n    OpnPlayer = 1\n    if obs.mark == 1:\n        OpnPlayer = 2\n    \n    Blocking = []\n    empty = []\n\n    for col in range(config.columns): #Winning\n        if check_winning_move(obs, config, col, obs.mark):\n            return col\n        if check_winning_move(obs, config, col, OpnPlayer):\n            Blocking.append(col)\n        if obs.board[col] == 0:\n            empty.append(col)\n\n    return Blocking[0] if Blocking else random.choice(empty)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T01:58:11.288090Z","iopub.execute_input":"2025-02-09T01:58:11.288487Z","iopub.status.idle":"2025-02-09T01:58:11.298443Z","shell.execute_reply.started":"2025-02-09T01:58:11.288464Z","shell.execute_reply":"2025-02-09T01:58:11.297086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_environments import make, evaluate\n\n# Create the game environment\nenv = make(\"connectx\")\n\n# Two random agents play one game round\nenv.run([MinMaxAB_agent, \"random\"]) #my_agent_Nab my_agent3\n\n# Show the game\nenv.render(mode=\"ipython\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T01:58:15.025234Z","iopub.execute_input":"2025-02-09T01:58:15.025584Z","iopub.status.idle":"2025-02-09T01:58:16.950425Z","shell.execute_reply.started":"2025-02-09T01:58:15.025563Z","shell.execute_reply":"2025-02-09T01:58:16.948639Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nimport numpy as np\nimport pandas as pd\nimport gym\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom kaggle_environments import make, evaluate\nfrom gym import spaces\n\nclass ConnectFourGym(gym.Env):\n    def __init__(self, agent2=\"random\"):\n        ks_env = make(\"connectx\", debug=True)\n        self.env = ks_env.train([None, agent2])\n        self.rows = ks_env.configuration.rows\n        self.columns = ks_env.configuration.columns\n        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n        self.action_space = spaces.Discrete(self.columns)\n        self.observation_space = spaces.Box(low=0, high=2, \n                                            shape=(1,self.rows,self.columns), dtype=int)\n        # Tuple corresponding to the min and max possible rewards\n        self.reward_range = (-10, 1)\n        # StableBaselines throws error if these are not defined\n        self.spec = None\n        self.metadata = None\n        \n    def reset(self):\n        self.obs = self.env.reset()\n        return np.array(self.obs['board']).reshape(1,self.rows,self.columns)\n        \n    def change_reward(self, old_reward, done):\n        if old_reward == 1: # The agent won the game\n            return 1\n        elif done: # The opponent won the game\n            return -1\n        else: # Reward 1/42\n            return 1/(self.rows*self.columns)\n            \n    def step(self, action):\n        # Check if agent's move is valid\n        is_valid = (self.obs['board'][int(action)] == 0)\n        if is_valid: # Play the move\n            self.obs, old_reward, done, _ = self.env.step(int(action))\n            reward = self.change_reward(old_reward, done)\n        else: # End the game and penalize agent\n            reward, done, _ = -10, True, {}\n        return np.array(self.obs['board']).reshape(1,self.rows,self.columns), reward, done, _\n    \n# Create ConnectFour environment \nenvTrain = ConnectFourGym(agent2=\"random\")\n#envTrain = ConnectFourGym(agent2=MinMaxAB_agent)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T02:01:52.571000Z","iopub.execute_input":"2025-02-09T02:01:52.571331Z","iopub.status.idle":"2025-02-09T02:01:52.635961Z","shell.execute_reply.started":"2025-02-09T02:01:52.571311Z","shell.execute_reply":"2025-02-09T02:01:52.634655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Old Structure\n\nimport torch as th\nimport torch.nn as nn\n\n#!pip install \"stable-baselines3\"\nfrom stable_baselines3 import PPO \nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n\n# Neural network for predicting action values\nclass CustomCNN(BaseFeaturesExtractor):\n    \n    def __init__(self, observation_space: gym.spaces.Box, features_dim: int=128):\n        super(CustomCNN, self).__init__(observation_space, features_dim)\n        # CxHxW images (channels first)\n        n_input_channels = observation_space.shape[0]\n        self.cnn = nn.Sequential(\n            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=0),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),\n            nn.ReLU(),\n            nn.Flatten(),\n        )\n\n        # Compute shape by doing one forward pass\n        with th.no_grad():\n            n_flatten = self.cnn(\n                th.as_tensor(observation_space.sample()[None]).float()\n            ).shape[1]\n\n        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n\n    def forward(self, observations: th.Tensor) -> th.Tensor:\n        return self.linear(self.cnn(observations))\n\npolicy_kwargs = dict(\n    features_extractor_class=CustomCNN,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T00:06:23.924751Z","iopub.execute_input":"2025-02-09T00:06:23.925101Z","iopub.status.idle":"2025-02-09T00:06:23.932644Z","shell.execute_reply.started":"2025-02-09T00:06:23.925073Z","shell.execute_reply":"2025-02-09T00:06:23.931657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## New Structure\n\nimport torch as th\nimport torch.nn as nn\nimport gym\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n\nclass CustomCNN(BaseFeaturesExtractor):\n    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 128):\n        super(CustomCNN, self).__init__(observation_space, features_dim)\n        \n        n_input_channels = observation_space.shape[0]\n        \n        self.cnn = nn.Sequential(\n            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Flatten(),\n        )\n\n        # Compute shape by doing a forward pass with a dummy tensor\n        with th.no_grad():\n            n_flatten = self.cnn(th.as_tensor(observation_space.sample()[None]).float()).shape[1]\n\n        # Multi-layer MLP head for better feature extraction\n        self.linear = nn.Sequential(\n            nn.Linear(n_flatten, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),  # Dropout for regularization\n            nn.Linear(256, features_dim),\n            nn.ReLU()\n        )\n\n    def forward(self, observations: th.Tensor) -> th.Tensor:\n        return self.linear(self.cnn(observations))\n\npolicy_kwargs = dict(\n    features_extractor_class=CustomCNN,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T02:02:04.973469Z","iopub.execute_input":"2025-02-09T02:02:04.973766Z","iopub.status.idle":"2025-02-09T02:02:04.981993Z","shell.execute_reply.started":"2025-02-09T02:02:04.973740Z","shell.execute_reply":"2025-02-09T02:02:04.980429Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from stable_baselines3.common.callbacks import BaseCallback\nimport time\nimport numpy as np\n\nclass RewardTrackingCallback(BaseCallback):\n    def __init__(self, check_freq=1000, verbose=1):\n        super(RewardTrackingCallback, self).__init__(verbose)\n        self.check_freq = check_freq\n        self.episode_rewards = []\n        self.episode_wins = 0  # Count wins\n        self.episode_losses = 0  # Count losses\n        self.episode_count = 0\n        self.start_time = time.time()\n    \n    def _on_step(self) -> bool:\n        # Check if episode is done\n        if \"episode\" in self.locals:\n            episode_reward = self.locals[\"episode\"][\"r\"]\n            self.episode_rewards.append(episode_reward)\n            self.episode_count += 1\n            \n            # Assume reward of 1 means a win, -1 means a loss\n            if episode_reward == 1:\n                self.episode_wins += 1\n            elif episode_reward == -1:\n                self.episode_losses += 1\n            \n            # Print reward stats every check_freq steps\n            if self.num_timesteps % self.check_freq == 0:\n                avg_reward = np.mean(self.episode_rewards[-10:])  # Average last 10 episodes\n                win_rate = (self.episode_wins / self.episode_count) * 100 if self.episode_count > 0 else 0\n                elapsed_time = time.time() - self.start_time\n                \n                print(f\"Timestep: {self.num_timesteps} | Avg Reward: {avg_reward:.2f} | Wins: {self.episode_wins} | Losses: {self.episode_losses} | Win Rate: {win_rate:.2f}% | Time: {elapsed_time:.2f}s\")\n\n        return True  # Continue training\n\n# Attach the callback\ncallback = RewardTrackingCallback(check_freq=1)\n\n# Create PPO model with GPU support\nmodel = PPO(\"CnnPolicy\", envTrain, policy_kwargs=policy_kwargs, verbose=1, device=device)\n\n# Train the agent\nmodel.learn(total_timesteps=50000, callback=callback)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T00:10:21.694870Z","iopub.execute_input":"2025-02-09T00:10:21.695203Z","iopub.status.idle":"2025-02-09T00:12:52.241193Z","shell.execute_reply.started":"2025-02-09T00:10:21.695178Z","shell.execute_reply":"2025-02-09T00:12:52.239233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from stable_baselines3.common.callbacks import BaseCallback\nimport time\n\nclass StepRewardTrackingCallback(BaseCallback):\n    def __init__(self, verbose=1):\n        super(StepRewardTrackingCallback, self).__init__(verbose)\n        self.start_time = time.time()\n\n    def _on_step(self) -> bool:\n        # Retrieve reward from locals\n        reward = self.locals[\"rewards\"]\n        done = self.locals[\"dones\"]\n\n        # Print step-wise reward details\n        elapsed_time = time.time() - self.start_time\n        print(f\"Timestep: {self.num_timesteps} | Reward: {reward} | Done: {done} | Time: {elapsed_time:.2f}s\")\n\n        return True  # Continue training\n\n# Attach the callback\ncallback = StepRewardTrackingCallback()\n\n# Create PPO model with GPU support\n#model = PPO(\"CnnPolicy\", envTrain, policy_kwargs=policy_kwargs, verbose=1, device=device)\n\n# Train the agent\n#model.learn(total_timesteps=50000, callback=callback)\n\npolicy_kwargs = dict(\n    features_extractor_class=CustomCNN,\n)\n        \n# Initialize agent\nmodel = PPO(\"CnnPolicy\", envTrain, policy_kwargs=policy_kwargs, verbose=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T02:03:27.880883Z","iopub.execute_input":"2025-02-09T02:03:27.881261Z","iopub.status.idle":"2025-02-09T02:03:28.133881Z","shell.execute_reply.started":"2025-02-09T02:03:27.881239Z","shell.execute_reply":"2025-02-09T02:03:28.132983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train agent\nmodel.learn(total_timesteps=6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T02:03:59.088230Z","iopub.execute_input":"2025-02-09T02:03:59.088593Z","iopub.status.idle":"2025-02-09T02:04:55.338687Z","shell.execute_reply.started":"2025-02-09T02:03:59.088571Z","shell.execute_reply":"2025-02-09T02:04:55.337602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\n# Start timer\nstart_time = time.time()\n\n# Use GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Create PPO model with GPU support\nmodel = PPO(\"CnnPolicy\", envTrain, policy_kwargs=policy_kwargs, verbose=1, device=device)\n\n# Train the agent\nmodel.learn(total_timesteps=50000, callback=callback)\n\n\n###################\n\n# Initialize agent\n#model = PPO(\"CnnPolicy\", env, policy_kwargs=policy_kwargs, verbose=0)\n\n# Train agent\n#model.learn(total_timesteps=1) #50000\n\n# Save the trained model\n#model.save(\"/kaggle/working/ppo_model\")\n\n# End timer\nend_time = time.time()\n\n# Print total training time\nprint(f\"Training completed in {end_time - start_time:.2f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T00:14:34.884395Z","iopub.execute_input":"2025-02-09T00:14:34.884784Z","iopub.status.idle":"2025-02-09T00:23:50.675746Z","shell.execute_reply.started":"2025-02-09T00:14:34.884752Z","shell.execute_reply":"2025-02-09T00:23:50.674727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create PPO model with GPU support\nmodel = PPO(\"CnnPolicy\", envTrain, policy_kwargs=policy_kwargs, verbose=1, device=device)\n\n# Train the agent\nmodel.learn(total_timesteps=50000, callback=callback)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T00:13:24.764913Z","iopub.execute_input":"2025-02-09T00:13:24.765269Z","iopub.status.idle":"2025-02-09T00:14:20.354271Z","shell.execute_reply.started":"2025-02-09T00:13:24.765239Z","shell.execute_reply":"2025-02-09T00:14:20.352942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"/kaggle/working/ppo_model_winMM\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T00:44:05.290187Z","iopub.execute_input":"2025-02-09T00:44:05.290625Z","iopub.status.idle":"2025-02-09T00:44:05.389754Z","shell.execute_reply.started":"2025-02-09T00:44:05.290591Z","shell.execute_reply":"2025-02-09T00:44:05.388956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def RL_agent(obs, config):\n    from stable_baselines3 import PPO\n    import random\n    import numpy as np\n\n   # model = PPO.load(\"/kaggle/input/ppo_modelpkl/pytorch/default/1/ppo_model.pkl\")\n    model = PPO.load(\"/kaggle/working/ppo_model_winMM\")\n    \n    # Use the best model to select a column\n    col, _ = model.predict(np.array(obs['board']).reshape(1, 6,7))\n    # Check if selected column is valid\n    is_valid = (obs['board'][int(col)] == 0)\n    # If not valid, select random move. \n    if is_valid:\n        return int(col)\n    else:\n        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T00:44:10.237617Z","iopub.execute_input":"2025-02-09T00:44:10.238015Z","iopub.status.idle":"2025-02-09T00:44:10.244197Z","shell.execute_reply.started":"2025-02-09T00:44:10.237984Z","shell.execute_reply":"2025-02-09T00:44:10.242966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_environments import make, evaluate\n\n# Create the game environment\nenv = make(\"connectx\")\n\n# Two random agents play one game round\n#env.run([RL_agent, MinMaxAB_agent])\nenv.run([RL_agent, 'random'])\n\n# Show the game\nenv.render(mode=\"ipython\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T00:44:49.945928Z","iopub.execute_input":"2025-02-09T00:44:49.946282Z","iopub.status.idle":"2025-02-09T00:44:51.284311Z","shell.execute_reply.started":"2025-02-09T00:44:49.946254Z","shell.execute_reply":"2025-02-09T00:44:51.283351Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndef get_win_percentages(agent1, agent2, n_rounds=100):\n    # Use default Connect Four setup\n    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n    # Agent 1 goes first (roughly) half the time          \n    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n    # Agent 2 goes first (roughly) half the time      \n    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))\n\nget_win_percentages(agent1=RL_agent, agent2='random', n_rounds=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T00:45:46.044087Z","iopub.execute_input":"2025-02-09T00:45:46.044450Z","iopub.status.idle":"2025-02-09T00:46:12.844092Z","shell.execute_reply.started":"2025-02-09T00:45:46.044419Z","shell.execute_reply":"2025-02-09T00:46:12.843102Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import inspect\nimport os\n\ndef write_agent_to_file(function, file):\n    with open(file, \"a\" if os.path.exists(file) else \"w\") as f:\n        f.write(inspect.getsource(function))\n        print(function, \"written to\", file)\n\nwrite_agent_to_file(RL_agent, \"submission.py\") #my_agent my_agent_Nab","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nfrom kaggle_environments import agent\nfrom kaggle_environments import utils\n\nout = sys.stdout\nsubmission = utils.read_file(\"/kaggle/working/submission.py\")\nagent = agent.get_last_callable(submission, path=submission)\nsys.stdout = out\n\nenv = make(\"connectx\", debug=True)\nenv.run([agent, agent])\nprint(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -R /kaggle/working","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h1>Submisstion Version<h1/>","metadata":{}},{"cell_type":"code","source":"def RL_agent(obs, config):\n    from stable_baselines3 import PPO\n    import random\n    import numpy as np\n\n    model = PPO.load(\"./ppo_model.pkl\")\n    \n    # Use the best model to select a column\n    col, _ = model.predict(np.array(obs['board']).reshape(1, 6,7))\n    # Check if selected column is valid\n    is_valid = (obs['board'][int(col)] == 0)\n    # If not valid, select random move. \n    if is_valid:\n        return int(col)\n    else:\n        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])\n\n\nimport inspect\nimport os\n\ndef write_agent_to_file(function, file):\n    with open(file, \"a\" if os.path.exists(file) else \"w\") as f:\n        f.write(inspect.getsource(function))\n        print(function, \"written to\", file)\n\nwrite_agent_to_file(RL_agent, \"submission.py\") #my_agent my_agent_Nab","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nThe submission way was referenced by  \n[NickMacd's PPO using SB3 Notebook](https://www.kaggle.com/code/nickmacd/ppo-using-sb3/notebook)  \nand  \n[Connect X Exercise: Deep RL Submission V1](https://www.kaggle.com/code/svendaj/connect-x-exercise-deep-rl-submission-v1/notebook).\n","metadata":{"execution":{"iopub.status.busy":"2025-02-08T23:11:17.210949Z","iopub.execute_input":"2025-02-08T23:11:17.211323Z","iopub.status.idle":"2025-02-08T23:11:17.218534Z","shell.execute_reply.started":"2025-02-08T23:11:17.211256Z","shell.execute_reply":"2025-02-08T23:11:17.216994Z"}}},{"cell_type":"code","source":"#Load a previous agent\nmodel = PPO.load(\"/kaggle/input/ppo_modelpkl/pytorch/default/1/ppo_model.pkl\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#save agent\nmodel.save('ppo_connectx')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T00:46:25.759785Z","iopub.execute_input":"2025-02-09T00:46:25.760134Z","iopub.status.idle":"2025-02-09T00:46:25.852942Z","shell.execute_reply.started":"2025-02-09T00:46:25.760107Z","shell.execute_reply":"2025-02-09T00:46:25.851744Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile main.py\nimport os, sys, random\nimport numpy as np\nfrom stable_baselines3 import PPO\nimport torch\n\ncwd = '/kaggle_simulations/agent/'\nif os.path.exists(cwd):\n    sys.path.append(cwd)\nelse:\n    cwd = ''\n\nmodel = None\n\ndef agent(obs, config):\n    global model\n    # load the trained model\n    if model == None:\n        model = PPO.load(cwd + \"ppo_connectx\")\n    \"\"\"\n    #reshape the board into the expected output    \n    board = torch.tensor(obs['board'], dtype=torch.float32)\n    mark = obs['mark']\n    board[(board !=mark) & (board != 0)] = 8\n    board[board==mark] = 4\n    board = board/8\n    board = torch.reshape(board, (6,7))\n    board = board.unsqueeze(dim=0)\n    \n    #predict the action\n    action, _ = model.predict(board, deterministic=True)\n    \n    #if valid, return the action, else choose a random action\n    if board[0][0][action] ==0:\n        return int(action)\n    return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])\n    \"\"\"\n    \n    # Use the best model to select a column\n    col, _ = model.predict(np.array(obs['board']).reshape(1, 6,7))\n    # Check if selected column is valid\n    is_valid = (obs['board'][int(col)] == 0)\n    # If not valid, select random move. \n    if is_valid:\n        return int(col)\n    else:\n        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T00:46:29.514213Z","iopub.execute_input":"2025-02-09T00:46:29.514532Z","iopub.status.idle":"2025-02-09T00:46:29.520514Z","shell.execute_reply.started":"2025-02-09T00:46:29.514506Z","shell.execute_reply":"2025-02-09T00:46:29.519730Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pack files used for submission\n!tar cvfz submission.tar.gz main.py ppo_connectx.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T00:46:31.250194Z","iopub.execute_input":"2025-02-09T00:46:31.250527Z","iopub.status.idle":"2025-02-09T00:46:32.505912Z","shell.execute_reply.started":"2025-02-09T00:46:31.250499Z","shell.execute_reply":"2025-02-09T00:46:32.504724Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nfrom kaggle_environments import utils, agent\n\nout = sys.stdout\nsubmission = utils.read_file(\"main.py\")\nagent = agent.get_last_callable(submission, path= submission)\nsys.stdout = out\n\nenv = make(\"connectx\", debug=True)\nenv.run([agent, MinMaxAB_agent])\nprint(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")\nenv.render(mode=\"ipython\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T00:46:35.090906Z","iopub.execute_input":"2025-02-09T00:46:35.091268Z","iopub.status.idle":"2025-02-09T00:46:38.721925Z","shell.execute_reply.started":"2025-02-09T00:46:35.091239Z","shell.execute_reply":"2025-02-09T00:46:38.721001Z"}},"outputs":[],"execution_count":null}]}